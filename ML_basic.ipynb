{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaeunjess/ML-basic/blob/main/ML_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZGCXVNWZhae"
      },
      "source": [
        "# 1주차 과제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZA6wxzbZhaf"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDgSH06FZhaf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnUEK7QtZhag"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_z2H8tyZhag"
      },
      "outputs": [],
      "source": [
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# Load the data and split it between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiPoiA8yZhag"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5e2JjtIZhag"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVGL8eZNZhah"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKuHMa0cZhah"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "epochs = 15\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETv94BdiZhah"
      },
      "source": [
        "## Evaluate the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LrYxleJZhah"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hvRDMhMbRL-"
      },
      "source": [
        "## tensor->torch 코드 변환"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9mjbSMNhgrX"
      },
      "source": [
        "### 최종 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-f4ffg_4hlGt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from PIL import Image\n",
        "\n",
        "# Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=32,\n",
        "                kernel_size=5,\n",
        "                stride=1), # 컨볼루션 레이어: 입력 채널 1, 출력 채널 32, 커널 크기 5x5, 스트라이드 1\n",
        "            nn.LeakyReLU(), # LeakyReLU 활성화 함수\n",
        "            nn.BatchNorm2d(32), # 배치 정규화\n",
        "            nn.MaxPool2d(kernel_size=2), # 맥스 풀링\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 5, 1),  # 컨볼루션 레이어: 입력 채널 32, 출력 채널 64, 커널 크기 5x5, 스트라이드 1\n",
        "            nn.ReLU(), # ReLU 활성화 함수\n",
        "            nn.BatchNorm2d(64), # 배치 정규화\n",
        "            nn.MaxPool2d(2), # 맥스 풀링\n",
        "        )\n",
        "        # fully connected layer, output 10 classes\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 128)  # 64개의 4x4 피처 맵을 128개의 출력 피처로 변환\n",
        "        self.fc2 = nn.Linear(128, 10 ) # 128개의 입력 피처를 10개의 클래스로 변\n",
        "        self.relu = nn.ReLU() # ReLU 활성화 함수\n",
        "        self.dropout = nn.Dropout(0.5) # 드롭아웃\n",
        "        self.softmax = nn.LogSoftmax(dim=1) # 소프트맥스 함수\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # 피처 맵을 1차원 벡터로 변환\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.softmax(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "'''class RandomInvert(object):  색상 반전class\n",
        "    def __call__(self, tensor):\n",
        "        if torch.rand(1) > 0.5:\n",
        "            return 1 - tensor\n",
        "        else:\n",
        "            return tensor\n",
        "\n",
        "\n",
        "transform = transforms.Compose([ 이미지 transform 실제 구동한 함수에서는 이미지 회전만 사용\n",
        "    transforms.RandomRotation(degrees=(-13, 13)),  # 무작위 회전\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2), #색상 변경\n",
        "    transforms.RandomApply([RandomInvert()], p=0.5), #색상 반전\n",
        "    transforms.ToTensor(),\n",
        "])'''\n",
        "\n",
        "# MNIST 불러오기\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "transform_training = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomRotation(degrees=(-13, 13)),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_training)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "# Instantiate the model\n",
        "model = CNN()\n",
        "print(model)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00108) # Adam 옵티마이저\n",
        "scheduler = StepLR(optimizer, step_size=4, gamma=0.5) # StepLR 스케줄러\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 25\n",
        "\n",
        "# 오답 이미지를 저장할 디렉토리 생성\n",
        "misclassified_dir = 'misclassified'\n",
        "os.makedirs(misclassified_dir, exist_ok=True)\n",
        "\n",
        "# 학습 및 테스트 루프\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "       # 그래디언트 초기화\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_func(outputs, labels)\n",
        "        # 역전파 및 가중치 업데이트\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # 손실 및 정확도 추적\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 에폭당 평균 손실과 정확도 계산\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.5f}, Accuracy: {:.3f}%'.format(epoch + 1, num_epochs, epoch_loss, epoch_accuracy))\n",
        "\n",
        "    # 학습 과정이 끝난 후 scheduler.step() 호출, 학습률 스케줄러로 학습률 조정\n",
        "    scheduler.step()\n",
        "\n",
        "     # 평가 모드로 설정\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "       # 테스트 데이터에 대한 평가\n",
        "        misclassified_count = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        test_loss = 0\n",
        "        for images, labels in test_loader:\n",
        "           # 모델 순방향 전파\n",
        "            outputs = model(images)\n",
        "            # 정확도 계산\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            # 손실 계산\n",
        "            test_loss += loss_func(outputs, labels)\n",
        "            # 오분류된 이미지 저장\n",
        "            misclassified_idx = (predicted != labels).nonzero()[:, 0]\n",
        "            for idx in misclassified_idx:\n",
        "                misclassified_image = images[idx].squeeze().numpy() * 255\n",
        "                misclassified_image = Image.fromarray(misclassified_image.astype('uint8'), mode='L')\n",
        "                misclassified_image.save(os.path.join(misclassified_dir, f'misclassified_{misclassified_count}.png'))\n",
        "                misclassified_count += 1\n",
        "\n",
        "\n",
        "    test_accuracy = 100 * correct / total\n",
        "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
        "    print('Test Loss: {:.5f}, Test Accuracy: {:.3f}%'.format(avg_test_loss, test_accuracy))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihdGjDG6b6xE"
      },
      "source": [
        "### 진한 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-TjE5KYJk9h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (1, 28, 28)\n",
        "\n",
        "# Load the data and split it between train and test sets\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Define the model\n",
        "class SimpleConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3))\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3))\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(64 * 5 * 5, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(torch.relu(self.conv1(x)))\n",
        "        x = self.pool2(torch.relu(self.conv2(x)))\n",
        "        x = self.dropout(self.flatten(x))\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = SimpleConvNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "    with torch.no_grad():\n",
        "      for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        test_loss += criterion(outputs, labels).item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    print(\"Test loss:\", test_loss / len(test_loader))\n",
        "    print(\"Test accuracy:\", 100 * correct / len(test_loader.dataset), \"%\")\n",
        "# Evaluate the trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS88UrI3b_-g"
      },
      "source": [
        "### 동진 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqEIgc7nHLST"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "class RandomInvert(object):\n",
        "    def __call__(self, tensor):\n",
        "        if torch.rand(1) > 0.5:\n",
        "            return 1 - tensor\n",
        "        else:\n",
        "            return tensor\n",
        "\n",
        "# Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=32,\n",
        "                kernel_size=5,\n",
        "                stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 5, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        # fully connected layer, output 10 classes\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
        "        self.fc2 = nn.Linear(128, 10 )\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.softmax(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# Load MNIST dataset and preprocess\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "transform_training = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomRotation(degrees=(-13, 13)),\n",
        "    #transforms.RandomApply([RandomInvert()], p=0.5),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform_training)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "# Instantiate the model\n",
        "model = CNN()\n",
        "print(model)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00108)\n",
        "scheduler = StepLR(optimizer, step_size=4, gamma=0.0002)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 25\n",
        "\n",
        "fir_adjust_epoch = 20\n",
        "fir_adjust_lr = 0.00004\n",
        "sec_adjust_epoch = 23\n",
        "sec_adjust_lr = 0.00002\n",
        "\n",
        "# 오답 이미지를 저장할 디렉토리 생성\n",
        "misclassified_dir = 'misclassified'\n",
        "os.makedirs(misclassified_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    if epoch >= fir_adjust_epoch and epoch <= sec_adjust_epoch:\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = fir_adjust_lr\n",
        "    elif epoch > sec_adjust_epoch:\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = sec_adjust_lr\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_func(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.5f}, Accuracy: {:.3f}%'.format(epoch + 1, num_epochs, epoch_loss, epoch_accuracy))\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        test_loss = 0\n",
        "        misclassified_count = 0\n",
        "\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            test_loss += loss_func(outputs, labels)\n",
        "            misclassified_idx = (predicted != labels).nonzero()[:, 0]\n",
        "            for idx in misclassified_idx:\n",
        "                misclassified_image = images[idx].squeeze().numpy() * 255\n",
        "                misclassified_image = Image.fromarray(misclassified_image.astype('uint8'), mode='L')\n",
        "                misclassified_image.save(os.path.join(misclassified_dir, f'misclassified_{misclassified_count}.png'))\n",
        "                misclassified_count += 1\n",
        "\n",
        "    test_accuracy = 100 * correct / total\n",
        "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
        "    print('Test Loss: {:.5f}, Test Accuracy: {:.3f}%'.format(avg_test_loss, test_accuracy))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIoFLa1HWpAz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "# Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=32,\n",
        "                kernel_size=5,\n",
        "                stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 5, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        # fully connected layer, output 10 classes\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
        "        self.fc2 = nn.Linear(128, 10, )\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.softmax(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# Load MNIST dataset and preprocess\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "transform_training = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomRotation(degrees=(-13, 13))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_training)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Instantiate the model\n",
        "model = CNN()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 30\n",
        "prev_loss = float('inf')  # 이전 손실 값 초기화\n",
        "patience = 5  # 조기 종료를 위한 참을성 값\n",
        "early_stop_counter = 0  # 조기 종료 카운터 초기화\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_func(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch + 1, num_epochs, epoch_loss, epoch_accuracy))\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        test_loss = 0\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            test_loss += loss_func(outputs, labels)\n",
        "\n",
        "    test_accuracy = 100 * correct / total\n",
        "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
        "    print('Test Loss: {:.4f}, Test Accuracy: {:.2f}%'.format(avg_test_loss, test_accuracy))\n",
        "\n",
        "    if avg_test_loss > prev_loss:\n",
        "        early_stop_counter += 1\n",
        "        if early_stop_counter >= patience:\n",
        "            print(\"Validation loss doesn't decrease anymore. Early stopping...\")\n",
        "            break\n",
        "    else:\n",
        "        early_stop_counter = 0\n",
        "        prev_loss = avg_test_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total}%')\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3REuC7Ukyr5"
      },
      "outputs": [],
      "source": [
        "## 정확도 15~20 epoch 사이 99.3 ~ 99.4??\n",
        "### dropout 이나 회전 각이나 step 사이즈 조정하기?\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomRotation(degrees=(-13, 13)),  # 무작위 회전\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# MNIST 데이터셋 로드\n",
        "train_data = datasets.MNIST(\n",
        "    root='data',\n",
        "    train=True,\n",
        "    transform=transform,\n",
        "    download=True,\n",
        ")\n",
        "test_data = datasets.MNIST(\n",
        "    root='data',\n",
        "    train=False,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=16,\n",
        "                kernel_size=5\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, 5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        '''\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(8, 16, 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        '''\n",
        "        # fully connected layer, output 10 classes\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512, 10),  # assuming input image size is 28x28\n",
        "            nn.Dropout(0.20)  # dropout with probability\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        #x = self.conv3(x)\n",
        "        # flatten the output of conv3 to (batch_size, 64)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = CNN()\n",
        "print(model)\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "print(loss_func)\n",
        "\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0011)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.0001)\n",
        "\n",
        "print(optimizer)\n",
        "\n",
        "num_epochs = 30\n",
        "prev_loss = float('inf')  # 이전 손실 값 초기화\n",
        "patience = 5  # 조기 종료를 위한 참을성 값\n",
        "early_stop_counter = 0  # 조기 종료 카운터 초기화\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        outputs = model(images)\n",
        "        loss = loss_func(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        test_loss = 0\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            test_loss += loss_func(outputs, labels)\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}, Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch + 1, num_epochs, loss.item(),\n",
        "                                                                                     avg_test_loss, accuracy))\n",
        "\n",
        "\n",
        "    if avg_test_loss > prev_loss:\n",
        "        early_stop_counter += 1\n",
        "        if early_stop_counter >= patience:\n",
        "            print(\"Validation loss doesn't decrease anymore. Early stopping...\")\n",
        "            break\n",
        "    else:\n",
        "        early_stop_counter = 0\n",
        "        prev_loss = avg_test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27yQ5IliYwhO"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomRotation(degrees=(-10, 10)),  # 무작위 회전\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# MNIST 데이터셋 로드\n",
        "train_data = datasets.MNIST(\n",
        "    root='data',\n",
        "    train=True,\n",
        "    transform=ToTensor(),\n",
        "    download=True,\n",
        ")\n",
        "test_data = datasets.MNIST(\n",
        "    root='data',\n",
        "    train=False,\n",
        "    transform=ToTensor(),\n",
        "    download=True,\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=32,\n",
        "                kernel_size=5,\n",
        "            stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 5, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        # fully connected layer, output 10 classes\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(4*4*64, 10),  # assuming input image size is 28x28\n",
        "            nn.Dropout(0.2)  # dropout with probability 0.1\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = CNN()\n",
        "print(model)\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "print(loss_func)\n",
        "\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "print(optimizer)\n",
        "\n",
        "num_epochs = 10\n",
        "prev_loss = float('inf')  # 이전 손실 값 초기화\n",
        "patience = 5  # 조기 종료를 위한 참을성 값\n",
        "early_stop_counter = 0  # 조기 종료 카운터 초기화\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        outputs = model(images)\n",
        "        loss = loss_func(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        test_loss = 0\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            test_loss += loss_func(outputs, labels)\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}, Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch + 1, num_epochs, loss.item(),\n",
        "                                                                                     avg_test_loss, accuracy))\n",
        "\n",
        "    # 검증 손실이 이전보다 증가한 경우 조기 종료를 검사합니다.\n",
        "    if avg_test_loss > prev_loss:\n",
        "        early_stop_counter += 1\n",
        "        if early_stop_counter >= patience:\n",
        "            print(\"Validation loss doesn't decrease anymore. Early stopping...\")\n",
        "            break\n",
        "    else:\n",
        "        early_stop_counter = 0\n",
        "        prev_loss = avg_test_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJZf-f3Q8lXn"
      },
      "source": [
        "99.5 이상 모델 및 파라미터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "affjg5j28g03"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=32,\n",
        "                kernel_size=5,\n",
        "                stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 5, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        # fully connected layer, output 10 classes\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
        "        self.fc2 = nn.Linear(128, 10, )\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.softmax(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# Load MNIST dataset and preprocess\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "transform_training = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomRotation(degrees=(-15, 15))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_training)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Instantiate the model\n",
        "model = CNN()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00108)\n",
        "scheduler = StepLR(optimizer, step_size=4, gamma=0.00020)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 25\n",
        "\n",
        "last_epoch_adjust_lr = 20  # 학습률을 조절할 마지막 epoch\n",
        "new_lr = 0.00003\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    if epoch >= last_epoch_adjust_lr:\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = new_lr\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_func(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.5f}, Accuracy: {:.3f}%'.format(epoch + 1, num_epochs, epoch_loss, epoch_accuracy))\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        test_loss = 0\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            test_loss += loss_func(outputs, labels)\n",
        "\n",
        "    test_accuracy = 100 * correct / total\n",
        "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
        "    print('Test Loss: {:.5f}, Test Accuracy: {:.3f}%'.format(avg_test_loss, test_accuracy))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-BCRZ7x-CPL"
      },
      "source": [
        "99.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5t9fChu98k9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=32,\n",
        "                kernel_size=5,\n",
        "                stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 5, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        # fully connected layer, output 10 classes\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
        "        self.fc2 = nn.Linear(128, 10, )\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.softmax(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# Load MNIST dataset and preprocess\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "transform_training = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomRotation(degrees=(-13, 13))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_training)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Instantiate the model\n",
        "model = CNN()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00108)\n",
        "scheduler = StepLR(optimizer, step_size=4, gamma=0.00020)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 25\n",
        "\n",
        "last_epoch_adjust_lr = 20  # 학습률을 조절할 마지막 epoch\n",
        "new_lr = 0.00003\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    if epoch >= last_epoch_adjust_lr:\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = new_lr\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_func(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.5f}, Accuracy: {:.3f}%'.format(epoch + 1, num_epochs, epoch_loss, epoch_accuracy))\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        test_loss = 0\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            test_loss += loss_func(outputs, labels)\n",
        "\n",
        "    test_accuracy = 100 * correct / total\n",
        "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
        "    print('Test Loss: {:.5f}, Test Accuracy: {:.3f}%'.format(avg_test_loss, test_accuracy))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A1_4vxLa21S"
      },
      "source": [
        "# 2주차 과제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usMMA5TFeOYR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jVjyVSvdSn7"
      },
      "source": [
        "## 동진 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7Nknanydfgq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# CIFAR-100 데이터셋 로드 및 전처리\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=False, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=False, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=0)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=0)\n",
        "\n",
        "# EfficientNet 모델 불러오기 (pretrained)\n",
        "model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=100)\n",
        "\n",
        "# GPU 사용 가능 여부 확인\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)\n",
        "model.to(device)\n",
        "\n",
        "# 손실 함수 및 옵티마이저 정의\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "# 모델 학습\n",
        "for epoch in range(30):  # 데이터셋을 여러 번 반복하여 학습\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:  # 매 2000 미니배치마다 손실 출력\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # 각 epoch마다 테스트 데이터셋을 사용하여 모델의 정확도 계산\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "        100 * correct / total))\n",
        "\n",
        "print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaEINLjAMNvK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.utils.data import random_split\n",
        "import wandb\n",
        "\n",
        "text_path = \"/home/piai/바탕화면/dongjin/training_init.txt\"\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be loggedx\n",
        "    project=\"CIFAR classifier\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"architecture\": \"EfficientNetV2-M\",\n",
        "    \"dataset\": \"CIFAR-100\",\n",
        "    \"epochs\": 50,\n",
        "    \"batch_size\" : 64\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "weights = models.EfficientNet_V2_M_Weights.DEFAULT\n",
        "#weights.transforms()?\n",
        "\n",
        "training_dataset = datasets.CIFAR100('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR100('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "dataset_length = len(training_dataset)\n",
        "\n",
        "train_ratio = 0.8\n",
        "train_size = int(train_ratio * dataset_length)\n",
        "val_size = dataset_length - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(training_dataset, [train_size, val_size])\n",
        "\n",
        "training_dataloader = DataLoader(training_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, drop_last=True)\n",
        "\n",
        "modified_EfficientNetV2M = models.efficientnet_v2_m(weights=None)\n",
        "# modified_EfficientNetV2M = models.efficientnet_v2_m(weights=models.EfficientNet_V2_M_Weights.IMAGENET1K_V1)\n",
        "\n",
        "'''\n",
        "for param in modified_EfficientNetV2M.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "modified_EfficientNetV2M.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.4, inplace=True),\n",
        "    nn.Linear(in_features=1280, out_features=1000),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(p=0.4, inplace=True),\n",
        "    nn.Linear(in_features=1000, out_features=100)\n",
        ")\n",
        "\n",
        "'''\n",
        "\n",
        "modified_model = modified_EfficientNetV2M.to(device)\n",
        "\n",
        "def training_batch(data, labels, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def test_batch(data, labels, model, criterion):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    return loss.item()\n",
        "\n",
        "def accuracy(data, labels, model):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    _, pred_labels = output.max(-1)\n",
        "    correct = (pred_labels == labels)\n",
        "    return correct.cpu().detach().numpy().tolist()\n",
        "\n",
        "model_to_train = modified_EfficientNetV2M.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model_to_train.parameters(), lr=0.01)\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42) ### 랜덤 시드를 적용\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "training_loss, test_loss = [], []\n",
        "training_accuracy, test_accuracy = [], []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    training_losses, test_losses = [], []\n",
        "    training_accuracies, test_accuracies = [], []\n",
        "\n",
        "    time1 = time.time()\n",
        "    for data, labels in training_dataloader:\n",
        "        trng_batch_loss = training_batch(data, labels, model_to_train, criterion, optimizer)\n",
        "        training_losses.append(trng_batch_loss)\n",
        "        trng_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        training_accuracies.extend(trng_batch_correct)\n",
        "    training_per_epoch_loss = np.array(training_losses).mean()\n",
        "    training_per_epoch_accuracy = np.mean(training_accuracies)\n",
        "    time2 = time.time()\n",
        "    print(\"Respective Execution time : \" + str(time2 - time1) + \"sec    \" + str(epoch+1) + \" epochs\")\n",
        "\n",
        "    for data, labels in val_dataloader:\n",
        "        val_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        test_accuracies.extend(val_batch_correct)\n",
        "\n",
        "\n",
        "    val_per_epoch_accuracy = np.mean(test_accuracies)\n",
        "\n",
        "    for data, labels in test_dataloader:\n",
        "        tst_batch_loss = test_batch(data, labels, model_to_train, criterion)\n",
        "        test_losses.append(tst_batch_loss)\n",
        "        tst_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        test_accuracies.extend(tst_batch_correct)\n",
        "    test_per_epoch_loss = np.array(test_losses).mean()\n",
        "    test_per_epoch_accuracy = np.mean(test_accuracies)\n",
        "\n",
        "    training_loss.append(training_per_epoch_loss)\n",
        "    training_accuracy.append(training_per_epoch_accuracy)\n",
        "    test_loss.append(test_per_epoch_loss)\n",
        "    test_accuracy.append(test_per_epoch_accuracy)\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}/{n_epochs}\\t| Training loss: {training_per_epoch_loss:.4f} | ', end='')\n",
        "    print(f'Training accuracy: {training_per_epoch_accuracy:.4f} | Validation accuracy: {val_per_epoch_accuracy:.4f} | ', end='')\n",
        "    print(f'Test loss: {test_per_epoch_loss:.4f} |  Test accuracy: {test_accuracy:.4f} |' + \"\\n\")\n",
        "\n",
        "\n",
        "    wandb.log({\n",
        "    \"epoch\": epoch + 1,\n",
        "    \"training_loss\": training_per_epoch_loss,\n",
        "    \"training_accuracy\": training_per_epoch_accuracy,\n",
        "    \"validation_accuracy\": val_per_epoch_accuracy,\n",
        "    \"test_loss\": test_per_epoch_loss,\n",
        "    \"test_accuracy\": test_accuracy\n",
        "    })\n",
        "    with open(text_path, \"w\") as file:\n",
        "        # 파일에 쓸 텍스트 작성\n",
        "        file.write(f'Epoch: {epoch + 1}/{n_epochs}\\t| Training loss: {training_loss[epoch]:.4f} | ')\n",
        "        file.write(f'Training accuracy: {training_accuracy[epoch]:.4f} | Validation accuracy: {val_per_epoch_accuracy:.4f} | ')\n",
        "        file.write(f'Test loss: {test_loss[epoch]:.4f} | Test accuracy: {test_accuracy:.4f} \\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcLDF2npBdnT"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import transforms, RandAugment\n",
        "import time\n",
        "import wandb\n",
        "import random\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "text_path = \"/home/piai/바탕화면/dongjin/complexed_2.txt\"\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be loggedx\n",
        "    project=\"CIFAR classifier\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"architecture\": \"Pretrain_EfficientNetV2-M_2\",\n",
        "    \"dataset\": \"CIFAR-100\",\n",
        "    \"epochs\": 50,\n",
        "    \"batch_size\" : 64\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# 1. 데이터셋 및 데이터로더 준비\n",
        "def get_dataloader(train, batch_size, image_size, augment=False, num_ops=2, magnitude=9):\n",
        "    if not augment:\n",
        "        transform_list = [\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "        ]\n",
        "        transform = transforms.Compose(transform_list)\n",
        "        dataset = CIFAR100(root='./data', train=train, download=True, transform=transform)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    else:\n",
        "        augment_list = [\n",
        "            RandAugment(num_ops, magnitude),  # RandAugment를 먼저 적용\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "        ]\n",
        "        transform = transforms.Compose(augment_list)\n",
        "        dataset = CIFAR100(root='./data', train=train, download=True, transform=transform)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "class StochasticDepth(nn.Module):\n",
        "    def __init__(self, p_start=1.0, p_end=0.5):\n",
        "        super(StochasticDepth, self).__init__()\n",
        "        self.p_start = p_start\n",
        "        self.p_end = p_end\n",
        "\n",
        "    def forward(self, x, i):\n",
        "        p = self.p_start - (self.p_start - self.p_end) * i / self.num_layers\n",
        "        if random.random() < p:\n",
        "            return x\n",
        "        else:\n",
        "            return torch.zeros_like(x)\n",
        "\n",
        "\n",
        "def get_model(dropout_rate=None):\n",
        "    depth_prob = 1.0  # 시작 확률\n",
        "    model = models.efficientnet_v2_m(weights=None)\n",
        "    num_layers = len(model.features)\n",
        "    for i, layer in enumerate(model.features):\n",
        "        if isinstance(layer, nn.Conv2d):  # Convolutional 레이어에만 적용\n",
        "            depth_prob = 1.0 - (1.0 - 0.5) * i / num_layers\n",
        "            layer = nn.Sequential(layer, StochasticDepth(depth_prob))\n",
        "            model.features[i] = layer\n",
        "    if dropout_rate is not None:\n",
        "        # EfficientNet 모델의 마지막 FC 레이어 전, Dropout 비율 수정\n",
        "        model.classifier[0] = torch.nn.Dropout(p=dropout_rate, inplace=False)\n",
        "    return model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "# 3. 모델 학습 및 평가\n",
        "## 3.1 모델 학습에 사용될 함수 정의\n",
        "def training_batch(data, labels, model, criterion, optimizer,):\n",
        "    model.train()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def test_batch(data, labels, model, criterion):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    return loss.item()\n",
        "\n",
        "def accuracy(data, labels, model):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    _, pred_labels = output.max(-1)\n",
        "    correct = (pred_labels == labels)\n",
        "    return correct.cpu().detach().numpy().tolist()\n",
        "\n",
        "## 3.2 Hyperparameter 설정\n",
        "model_to_train = get_model()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model_to_train.parameters(), lr=0.001, weight_decay=0.005)\n",
        "\n",
        "training_loss, test_loss = [], []\n",
        "training_accuracy, test_accuracy = [], []\n",
        "\n",
        "n_epochs = 50\n",
        "batch_size = 64\n",
        "image_size = 224\n",
        "\n",
        "scheduler = OneCycleLR(optimizer, max_lr=0.1, epochs=n_epochs, steps_per_epoch=10)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    if epoch % 20 == 0:\n",
        "        # 20 epoch마다 변경할 설정\n",
        "        image_size = 224 + (epoch // 20) * 32       # image size 증가\n",
        "        dropout_rate = 0.2 + (epoch // 20) * 0.05   # dropout 비율 증가\n",
        "        num_ops = 2 + (epoch // 20) * 2             # RandAugment 매개변수 증가\n",
        "        magnitude = 3 + (epoch // 20) * 2           # RandAugment 매개변수 증가\n",
        "        # mixup_alpha = 0 + (epoch // 20) * 1         # mixup alpha 증가\n",
        "\n",
        "        training_dataloader = get_dataloader(train=True, batch_size=batch_size, image_size=image_size, augment=True, num_ops=num_ops, magnitude=magnitude)\n",
        "        test_dataloader = get_dataloader(train=False, batch_size=batch_size, image_size=image_size)\n",
        "        model_to_train = get_model(dropout_rate=dropout_rate)\n",
        "        optimizer = optim.AdamW(model_to_train.parameters(), lr=0.01)\n",
        "\n",
        "    if epoch == 30:\n",
        "        torch.save(model_to_train, '/home/piai/바탕화면/dongjin/' + 'pretrained_model_2.pt')\n",
        "\n",
        "\n",
        "    training_losses, test_losses = [], []\n",
        "    training_accuracies, test_accuracies = [], []\n",
        "\n",
        "\n",
        "    # 현재 에포크의 시작 시간 측정\n",
        "    time1 = time.time()\n",
        "\n",
        "    # train 데이터 로더를 통해 배치 단위로 데이터 로딩 및 학습\n",
        "    for data, labels in training_dataloader:\n",
        "        trng_batch_loss = training_batch(data, labels, model_to_train, criterion, optimizer)\n",
        "        training_losses.append(trng_batch_loss)\n",
        "        trng_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        training_accuracies.extend(trng_batch_correct)\n",
        "\n",
        "    # 현재 에포크의 평균 학습 손실 및 정확도 계산\n",
        "    training_per_epoch_loss = np.array(training_losses).mean()\n",
        "    training_per_epoch_accuracy = np.mean(training_accuracies)\n",
        "\n",
        "    # 현재 에포크의 종료 시간 측정\n",
        "    time2 = time.time()\n",
        "    print(\"Respective Execution time : \" + str(time2 - time1) + \"   \" + str(epoch + 1) + \"epochs\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # test 데이터 로더를 통해 배치 단위로 데이터 로딩 및 평가\n",
        "    for data, labels in test_dataloader:\n",
        "        tst_batch_loss = test_batch(data, labels, model_to_train, criterion)\n",
        "        test_losses.append(tst_batch_loss)\n",
        "        tst_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        test_accuracies.extend(tst_batch_correct)\n",
        "    # 현재 에포크의 평균 테스트 손실 및 정확도 계산\n",
        "    test_per_epoch_loss = np.array(test_losses).mean()\n",
        "    test_per_epoch_accuracy = np.mean(test_accuracies)\n",
        "\n",
        "    # 계산된 평균 손실 및 정확도를 저장\n",
        "    training_loss.append(training_per_epoch_loss)\n",
        "    training_accuracy.append(training_per_epoch_accuracy)\n",
        "    test_loss.append(test_per_epoch_loss)\n",
        "    test_accuracy.append(test_per_epoch_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # 현재 에포크의 학습 및 테스트 결과 출력\n",
        "    print(f'Epoch: {epoch + 1}/{n_epochs}\\t| Training loss: {training_per_epoch_loss:.4f} | ', end='')\n",
        "    print(f'Training accuracy: {training_per_epoch_accuracy:.4f} |', end='')\n",
        "    print(f'Test loss: {test_per_epoch_loss:.4f} |  Test accuracy: {test_per_epoch_accuracy:.4f} |' + \"\\n\")\n",
        "\n",
        "    wandb.log({\n",
        "    \"epoch\": epoch + 1,\n",
        "    \"training_loss\": training_per_epoch_loss,\n",
        "    \"training_accuracy\": training_per_epoch_accuracy,\n",
        "    \"test_loss\": test_per_epoch_loss,\n",
        "    \"test_accuracy\": test_per_epoch_accuracy\n",
        "    })\n",
        "\n",
        "\n",
        "\n",
        "    with open(text_path, \"a\") as file:\n",
        "        # 파일에 쓸 텍스트 작성\n",
        "        file.write(f'Epoch: {epoch + 1}/{n_epochs}\\t| Training loss: {training_loss[epoch]:.4f} | ')\n",
        "        file.write(f'Training accuracy: {training_accuracy[epoch]:.4f} | ')\n",
        "        file.write(f'Test loss: {test_loss[epoch]:.4f} | Test accuracy: {test_per_epoch_accuracy:.4f} \\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIw_kmjzkWTX"
      },
      "outputs": [],
      "source": [
        "#with data-augmentation and scheduler\n",
        "# 88% at epoch 15??\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import time\n",
        "from torchvision.transforms import RandomHorizontalFlip, RandomCrop\n",
        "from torch.utils.data import random_split\n",
        "import wandb\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "import torch.nn.functional as F\n",
        "\n",
        "text_path = \"/home/piai/바탕화면/dongjin/training_init.txt\"\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "'''\n",
        "    RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
        "    RandomCrop(size=224, padding=4, padding_mode='reflect'),  # Random crop with padding\n",
        "    '''\n",
        "\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be loggedx\n",
        "    project=\"CIFAR classifier\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"architecture\": \"EfficientNetV2-M\",\n",
        "    \"dataset\": \"CIFAR-100\",\n",
        "    \"epochs\": 50,\n",
        "    \"batch_size\" : 64\n",
        "    }\n",
        ")\n",
        "\n",
        "class ModifiedEfficientNetV2M(nn.Module):\n",
        "    def __init__(self, model, drop_prob=0.2):\n",
        "        super(ModifiedEfficientNetV2M, self).__init__()\n",
        "        self.features = model\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for module in self.features.children():\n",
        "            if isinstance(module, nn.modules.Sequential):\n",
        "                for submodule in module.children():\n",
        "                    if isinstance(submodule, nn.modules.Module):\n",
        "                        out = submodule(out)\n",
        "                        if isinstance(submodule, nn.modules.conv.Conv2d):\n",
        "                            if np.random.rand() > self.drop_prob:\n",
        "                                out = F.dropout(out, p=0.2, training=True)\n",
        "            else:\n",
        "                out = module(out)\n",
        "                if isinstance(module, nn.modules.conv.Conv2d):\n",
        "                    if np.random.rand() > self.drop_prob:\n",
        "                        out = F.dropout(out, p=0.2, training=True)\n",
        "        return out\n",
        "\n",
        "\n",
        "#weights = models.EfficientNet_V2_M_Weights.DEFAULT\n",
        "#weights.transforms()?\n",
        "\n",
        "training_dataset = datasets.CIFAR100('./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR100('./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "dataset_length = len(training_dataset)\n",
        "\n",
        "train_ratio = 0.8\n",
        "train_size = int(train_ratio * dataset_length)\n",
        "val_size = dataset_length - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(training_dataset, [train_size, val_size])\n",
        "\n",
        "training_dataloader = DataLoader(training_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, drop_last=True)\n",
        "\n",
        "EfficientNetV2M = models.efficientnet_v2_m(weights=models.EfficientNet_V2_M_Weights.IMAGENET1K_V1)\n",
        "#modified_EfficientNetV2M= ModifiedEfficientNetV2M(EfficientNetV2M)\n",
        "# modified_EfficientNetV2M = models.efficientnet_v2_m(weights=models.EfficientNet_V2_M_Weights.IMAGENET1K_V1)\n",
        "\n",
        "\n",
        "\n",
        "modified_model = EfficientNetV2M.to(device)\n",
        "\n",
        "def training_batch(data, labels, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def test_batch(data, labels, model, criterion):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    return loss.item()\n",
        "\n",
        "def accuracy(data, labels, model):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    _, pred_labels = output.max(-1)\n",
        "    correct = (pred_labels == labels)\n",
        "    return correct.cpu().detach().numpy().tolist()\n",
        "\n",
        "model_to_train = EfficientNetV2M.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model_to_train.parameters(), lr=0.005)\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42) ### 랜덤 시드를 적용\n",
        "\n",
        "n_epochs = 29\n",
        "'''\n",
        "scheduler = OneCycleLR(optimizer, max_lr=0.01, epochs=n_epochs, steps_per_epoch=10)\n",
        "'''\n",
        "\n",
        "training_loss, test_loss = [], []\n",
        "training_accuracy, test_accuracy = [], []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    training_losses, test_losses = [], []\n",
        "    training_accuracies, test_accuracies = [], []\n",
        "\n",
        "    if epoch == 30:\n",
        "        torch.save(model_to_train, '/home/piai/바탕화면/dongjin/' + 'pretrained.pt')\n",
        "\n",
        "\n",
        "    time1 = time.time()\n",
        "    for data, labels in training_dataloader:\n",
        "        trng_batch_loss = training_batch(data, labels, model_to_train, criterion, optimizer)\n",
        "        training_losses.append(trng_batch_loss)\n",
        "        trng_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        training_accuracies.extend(trng_batch_correct)\n",
        "    training_per_epoch_loss = np.array(training_losses).mean()\n",
        "    training_per_epoch_accuracy = np.mean(training_accuracies)\n",
        "    time2 = time.time()\n",
        "    print(\"Respective Execution time : \" + str(time2 - time1) + \"sec    \" + str(epoch+1) + \" epochs\")\n",
        "\n",
        "    '''\n",
        "    scheduler.step()\n",
        "   '''\n",
        "    for data, labels in val_dataloader:\n",
        "        val_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        test_accuracies.extend(val_batch_correct)\n",
        "\n",
        "\n",
        "    val_per_epoch_accuracy = np.mean(test_accuracies)\n",
        "\n",
        "    for data, labels in test_dataloader:\n",
        "        tst_batch_loss = test_batch(data, labels, model_to_train, criterion)\n",
        "        test_losses.append(tst_batch_loss)\n",
        "        tst_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        test_accuracies.extend(tst_batch_correct)\n",
        "    test_per_epoch_loss = np.array(test_losses).mean()\n",
        "    test_per_epoch_accuracy = np.mean(test_accuracies)\n",
        "\n",
        "    training_loss.append(training_per_epoch_loss)\n",
        "    training_accuracy.append(training_per_epoch_accuracy)\n",
        "    test_loss.append(test_per_epoch_loss)\n",
        "    test_accuracy.append(test_per_epoch_accuracy)\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}/{n_epochs}\\t| Training loss: {training_per_epoch_loss:.4f} | ', end='')\n",
        "    print(f'Training accuracy: {training_per_epoch_accuracy:.4f} | Validation accuracy: {val_per_epoch_accuracy:.4f} | ', end='')\n",
        "    print(f'Test loss: {test_per_epoch_loss:.4f} |  Test accuracy: {test_per_epoch_accuracy:.4f} |' + \"\\n\")\n",
        "\n",
        "\n",
        "    wandb.log({\n",
        "    \"epoch\": epoch + 1,\n",
        "    \"training_loss\": training_per_epoch_loss,\n",
        "    \"training_accuracy\": training_per_epoch_accuracy,\n",
        "    \"validation_accuracy\": val_per_epoch_accuracy,\n",
        "    \"test_loss\": test_per_epoch_loss,\n",
        "    \"test_accuracy\": test_per_epoch_accuracy\n",
        "    })\n",
        "    with open(text_path, \"a\") as file:\n",
        "        # 파일에 쓸 텍스트 작성\n",
        "        file.write(f'Epoch: {epoch + 1}/{n_epochs}\\t| Training loss: {training_loss[epoch]:.4f} | ')\n",
        "        file.write(f'Training accuracy: {training_accuracy[epoch]:.4f} | Validation accuracy: {val_per_epoch_accuracy:.4f} | ')\n",
        "        file.write(f'Test loss: {test_loss[epoch]:.4f} | Test accuracy: {test_per_epoch_accuracy:.4f} \\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "2EE2LWwJhlA-",
        "outputId": "960a8fe7-052b-4508-8b08-f13fcbae28f9"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'wandb'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6f99930ad525>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomHorizontalFlip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomCrop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneCycleLR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wandb'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#with data-augmentation and scheduler\n",
        "# 88% at epoch 15??\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import time\n",
        "from torchvision.transforms import RandomHorizontalFlip, RandomCrop\n",
        "from torch.utils.data import random_split\n",
        "import wandb\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "text_path = \"/home/piai/바탕화면/dongjin/training.txt\"\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224), interpolation=Image.BILINEAR),\n",
        "    transforms.ToTensor(),\n",
        "    RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
        "    RandomCrop(size=224, padding=4, padding_mode='reflect'),  # Random crop with padding\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "'''\n",
        "cutmix?\n",
        "    '''\n",
        "\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224), interpolation=Image.BILINEAR),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be loggedx\n",
        "    project=\"CIFAR classifier\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"learning_rate\": 0.0005,\n",
        "    \"architecture\": \"Bilinear_EfficientNetV2-s - step\",\n",
        "    \"dataset\": \"CIFAR-100\",\n",
        "    \"epochs\": 25,\n",
        "    \"batch_size\" : 128\n",
        "    }\n",
        ")\n",
        "\n",
        "class StochasticDepth(nn.Module):\n",
        "    def __init__(self, p_start=1.0, p_end=0.2):\n",
        "        super(StochasticDepth, self).__init__()\n",
        "        self.p_start = p_start\n",
        "        self.p_end = p_end\n",
        "\n",
        "    def forward(self, x):\n",
        "      if self.training:\n",
        "        p = self.p_start - (self.p_start - self.p_end) * random.random()\n",
        "        if random.random() < p:\n",
        "            return x\n",
        "        else:\n",
        "            return torch.zeros_like(x)\n",
        "      else:\n",
        "        return x\n",
        "\n",
        "def get_model(dropout_rate=None):\n",
        "    model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "    for i, layer in enumerate(model.features):\n",
        "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "            depth_prob = 1.0 - (0.5 / (len(model.features) - 1)) * i\n",
        "            layer = nn.Sequential(layer, StochasticDepth(depth_prob))\n",
        "            model.features[i] = layer\n",
        "\n",
        "    if dropout_rate is not None:\n",
        "        model.classifier[0] = nn.Dropout(p=dropout_rate, inplace=False)\n",
        "\n",
        "    return model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "#weights = models.EfficientNet_V2_M_Weights.DEFAULT\n",
        "#weights.transforms()?\n",
        "\n",
        "training_dataset = datasets.CIFAR100('./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR100('./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "dataset_length = len(training_dataset)\n",
        "\n",
        "train_ratio = 0.8\n",
        "train_size = int(train_ratio * dataset_length)\n",
        "val_size = dataset_length - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(training_dataset, [train_size, val_size])\n",
        "\n",
        "training_dataloader = DataLoader(training_dataset, batch_size=128, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "#modified_EfficientNetV2M= ModifiedEfficientNetV2M(EfficientNetV2M)\n",
        "# EfficientNetV2M = models.efficientnet_v2_m(weights=models.EfficientNet_V2_M_Weights.IMAGENET1K_V1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def training_batch(data, labels, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def test_batch(data, labels, model, criterion):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    return loss.item()\n",
        "\n",
        "def accuracy(data, labels, model):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    _, pred_labels = output.max(-1)\n",
        "    correct = (pred_labels == labels)\n",
        "    return correct.cpu().detach().numpy().tolist()\n",
        "\n",
        "#model_to_train = get_model()\n",
        "\n",
        "efficientnet = models.efficientnet_v2_s(weights = models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "model_to_train = efficientnet.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model_to_train.parameters(), lr=0.0005, weight_decay= 0.005)\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42) ### 랜덤 시드를 적용\n",
        "\n",
        "n_epochs = 25\n",
        "steps_epoch = len(training_dataloader)\n",
        "scheduler = OneCycleLR(optimizer, max_lr=0.01, epochs=n_epochs, steps_per_epoch=steps_epoch)\n",
        "\n",
        "\n",
        "training_loss, test_loss = [], []\n",
        "training_accuracy, test_accuracy = [], []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    training_losses, test_losses = [], []\n",
        "    training_accuracies, test_accuracies = [], []\n",
        "\n",
        "\n",
        "    time1 = time.time()\n",
        "    for data, labels in training_dataloader:\n",
        "        trng_batch_loss = training_batch(data, labels, model_to_train, criterion, optimizer)\n",
        "        training_losses.append(trng_batch_loss)\n",
        "        trng_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        training_accuracies.extend(trng_batch_correct)\n",
        "    training_per_epoch_loss = np.array(training_losses).mean()\n",
        "    training_per_epoch_accuracy = np.mean(training_accuracies)\n",
        "    time2 = time.time()\n",
        "    print(\"Respective Execution time : \" + str(time2 - time1) + \"sec    \" + str(epoch+1) + \" epochs\")\n",
        "\n",
        "    learning_rate = optimizer.param_groups[0]['lr']\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "    for data, labels in val_dataloader:\n",
        "        val_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        test_accuracies.extend(val_batch_correct)\n",
        "\n",
        "\n",
        "    val_per_epoch_accuracy = np.mean(test_accuracies)\n",
        "\n",
        "\n",
        "    for data, labels in test_dataloader:\n",
        "        tst_batch_loss = test_batch(data, labels, model_to_train, criterion)\n",
        "        test_losses.append(tst_batch_loss)\n",
        "        tst_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        test_accuracies.extend(tst_batch_correct)\n",
        "    test_per_epoch_loss = np.array(test_losses).mean()\n",
        "    test_per_epoch_accuracy = np.mean(test_accuracies)\n",
        "\n",
        "    training_loss.append(training_per_epoch_loss)\n",
        "    training_accuracy.append(training_per_epoch_accuracy)\n",
        "    test_loss.append(test_per_epoch_loss)\n",
        "    test_accuracy.append(test_per_epoch_accuracy)\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}/{n_epochs}\\t| Training loss: {training_per_epoch_loss:.4f} | ', end='')\n",
        "    print(f'Training accuracy: {training_per_epoch_accuracy:.4f} | Validation accuracy: {val_per_epoch_accuracy:.4f} ', end='')\n",
        "    print(f'Test loss: {test_per_epoch_loss:.4f} |  Test accuracy: {test_per_epoch_accuracy:.4f} |' + \"\\n\")\n",
        "    print(f'learning rate: {learning_rate:.6f}')\n",
        "\n",
        "    wandb.log({\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"training_loss\": training_per_epoch_loss,\n",
        "    \"training_accuracy\": training_per_epoch_accuracy,\n",
        "    \"test_loss\": test_per_epoch_loss,\n",
        "    \"test_accuracy\": test_per_epoch_accuracy,\n",
        "    \"validation_accuracy\" : val_per_epoch_accuracy\n",
        "    })\n",
        "\n",
        "\n",
        "    with open(text_path, \"a\") as file:\n",
        "        # 파일에 쓸 텍스트 작성\n",
        "        file.write(f'Epoch: {epoch + 1}/{n_epochs}\\t| Training loss: {training_loss[epoch]:.4f} | ')\n",
        "        file.write(f'Training accuracy: {training_accuracy[epoch]:.4f} | Validation accuracy: {val_per_epoch_accuracy:.4f} ')\n",
        "        file.write(f'Test loss: {test_loss[epoch]:.4f} | Test accuracy: {test_per_epoch_accuracy:.4f} \\n')\n",
        "        file.write(f'learning rate: {learning_rate:.6f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PquariLVM9Mr"
      },
      "outputs": [],
      "source": [
        "# final ?\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import time\n",
        "from torchvision.transforms import RandomHorizontalFlip, RandomCrop\n",
        "from torch.utils.data import random_split\n",
        "import wandb\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "text_path = \"/home/piai/바탕화면/dongjin/training_init.txt\"\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224), interpolation=Image.BILINEAR),\n",
        "    transforms.ToTensor(),\n",
        "    RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
        "    RandomCrop(size=224, padding=4, padding_mode='reflect'),  # Random crop with padding\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "'''\n",
        "cutmix?\n",
        "    '''\n",
        "\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224), interpolation=Image.BILINEAR),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be loggedx\n",
        "    project=\"CIFAR classifier\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"architecture\": \"Bilinear_EfficientNetV2-s\",\n",
        "    \"dataset\": \"CIFAR-100\",\n",
        "    \"epochs\": 25,\n",
        "    \"batch_size\" : 128\n",
        "    }\n",
        ")\n",
        "\n",
        "class StochasticDepth(nn.Module):\n",
        "    def __init__(self, p_start=1.0, p_end=0.2):\n",
        "        super(StochasticDepth, self).__init__()\n",
        "        self.p_start = p_start\n",
        "        self.p_end = p_end\n",
        "\n",
        "    def forward(self, x):\n",
        "      if self.training:\n",
        "        p = self.p_start - (self.p_start - self.p_end) * random.random()\n",
        "        if random.random() < p:\n",
        "            return x\n",
        "        else:\n",
        "            return torch.zeros_like(x)\n",
        "      else:\n",
        "        return x\n",
        "\n",
        "def get_model(dropout_rate=None):\n",
        "    model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "    for i, layer in enumerate(model.features):\n",
        "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "            depth_prob = 1.0 - (0.5 / (len(model.features) - 1)) * i\n",
        "            layer = nn.Sequential(layer, StochasticDepth(depth_prob))\n",
        "            model.features[i] = layer\n",
        "\n",
        "    if dropout_rate is not None:\n",
        "        model.classifier[0] = nn.Dropout(p=dropout_rate, inplace=False)\n",
        "\n",
        "    return model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "#weights = models.EfficientNet_V2_M_Weights.DEFAULT\n",
        "#weights.transforms()?\n",
        "\n",
        "training_dataset = datasets.CIFAR100('./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR100('./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "dataset_length = len(training_dataset)\n",
        "\n",
        "train_ratio = 0.8\n",
        "train_size = int(train_ratio * dataset_length)\n",
        "val_size = dataset_length - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(training_dataset, [train_size, val_size])\n",
        "\n",
        "training_dataloader = DataLoader(training_dataset, batch_size=128, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "#modified_EfficientNetV2M= ModifiedEfficientNetV2M(EfficientNetV2M)\n",
        "# EfficientNetV2M = models.efficientnet_v2_m(weights=models.EfficientNet_V2_M_Weights.IMAGENET1K_V1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def training_batch(data, labels, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def test_batch(data, labels, model, criterion):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    return loss.item()\n",
        "\n",
        "def accuracy(data, labels, model):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    _, pred_labels = output.max(-1)\n",
        "    correct = (pred_labels == labels)\n",
        "    return correct.cpu().detach().numpy().tolist()\n",
        "\n",
        "#model_to_train = get_model()\n",
        "\n",
        "efficientnet = models.efficientnet_v2_s(weights = models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "model_to_train = efficientnet.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model_to_train.parameters(), lr=0.0005, weight_decay= 0.005)\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42) ### 랜덤 시드를 적용\n",
        "\n",
        "n_epochs = 25\n",
        "steps_epoch = len(training_dataloader)\n",
        "\n",
        "scheduler = OneCycleLR(optimizer, max_lr=0.01, epochs=n_epochs, steps_per_epoch=steps_epoch)\n",
        "\n",
        "\n",
        "training_loss, test_loss = [], []\n",
        "training_accuracy, test_accuracy = [], []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    training_losses, test_losses = [], []\n",
        "    training_accuracies, test_accuracies = [], []\n",
        "\n",
        "\n",
        "    time1 = time.time()\n",
        "    for data, labels in training_dataloader:\n",
        "        trng_batch_loss = training_batch(data, labels, model_to_train, criterion, optimizer)\n",
        "        training_losses.append(trng_batch_loss)\n",
        "        trng_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        training_accuracies.extend(trng_batch_correct)\n",
        "    training_per_epoch_loss = np.array(training_losses).mean()\n",
        "    training_per_epoch_accuracy = np.mean(training_accuracies)\n",
        "    time2 = time.time()\n",
        "    print(\"Respective Execution time : \" + str(time2 - time1) + \"sec    \" + str(epoch+1) + \" epochs\")\n",
        "\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    for data, labels in val_dataloader:\n",
        "        val_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        test_accuracies.extend(val_batch_correct)\n",
        "\n",
        "\n",
        "    val_per_epoch_accuracy = np.mean(test_accuracies)\n",
        "\n",
        "    for data, labels in test_dataloader:\n",
        "        tst_batch_loss = test_batch(data, labels, model_to_train, criterion)\n",
        "        test_losses.append(tst_batch_loss)\n",
        "        tst_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        test_accuracies.extend(tst_batch_correct)\n",
        "    test_per_epoch_loss = np.array(test_losses).mean()\n",
        "    test_per_epoch_accuracy = np.mean(test_accuracies)\n",
        "\n",
        "    training_loss.append(training_per_epoch_loss)\n",
        "    training_accuracy.append(training_per_epoch_accuracy)\n",
        "    test_loss.append(test_per_epoch_loss)\n",
        "    test_accuracy.append(test_per_epoch_accuracy)\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}/{n_epochs}\\t| Training loss: {training_per_epoch_loss:.4f} | ', end='')\n",
        "    print(f'Training accuracy: {training_per_epoch_accuracy:.4f} | Validation accuracy: {val_per_epoch_accuracy:.4f} | ', end='')\n",
        "    print(f'Test loss: {test_per_epoch_loss:.4f} |  Test accuracy: {test_per_epoch_accuracy:.4f} |' + \"\\n\")\n",
        "\n",
        "\n",
        "    wandb.log({\n",
        "    \"epoch\": epoch + 1,\n",
        "    \"training_loss\": training_per_epoch_loss,\n",
        "    \"training_accuracy\": training_per_epoch_accuracy,\n",
        "    \"validation_accuracy\": val_per_epoch_accuracy,\n",
        "    \"test_loss\": test_per_epoch_loss,\n",
        "    \"test_accuracy\": test_per_epoch_accuracy\n",
        "    })\n",
        "\n",
        "\n",
        "    with open(text_path, \"a\") as file:\n",
        "        # 파일에 쓸 텍스트 작성\n",
        "        file.write(f'Epoch: {epoch + 1}/{n_epochs}\\t| Training loss: {training_loss[epoch]:.4f} | ')\n",
        "        file.write(f'Training accuracy: {training_accuracy[epoch]:.4f} | Validation accuracy: {val_per_epoch_accuracy:.4f} | ')\n",
        "        file.write(f'Test loss: {test_loss[epoch]:.4f} | Test accuracy: {test_per_epoch_accuracy:.4f} \\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTJ-Ul7Lkchc"
      },
      "outputs": [],
      "source": [
        "# cutmix 적용\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import time\n",
        "from torchvision.transforms import RandomHorizontalFlip, RandomCrop\n",
        "from torch.utils.data import random_split\n",
        "import wandb\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "text_path = \"/home/piai/바탕화면/dongjin/training.txt\"\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224), interpolation=Image.BILINEAR),\n",
        "    transforms.ToTensor(),\n",
        "    RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
        "    RandomCrop(size=224, padding=4, padding_mode='reflect'),  # Random crop with padding\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "def cutmix_data(images, labels, alpha=1.0):\n",
        "    indices = torch.randperm(images.size(0))\n",
        "    shuffled_images = images[indices]\n",
        "    shuffled_labels = labels[indices]\n",
        "\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    rand_index = torch.randperm(images.size()[0])\n",
        "\n",
        "    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n",
        "    images[:, :, bbx1:bbx2, bby1:bby2] = images[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "    # Adjust lambda to exactly match pixel ratio\n",
        "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.size()[-1] * images.size()[-2]))\n",
        "    return images, labels, shuffled_labels, lam\n",
        "\n",
        "# 이미지를 자르고 새로운 이미지를 생성하기 위한 함수\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "\n",
        "    # uniform\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224), interpolation=Image.BILINEAR),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be loggedx\n",
        "    project=\"CIFAR classifier\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"learning_rate\": 0.0005,\n",
        "    \"architecture\": \"Bilinear_EfficientNetV2-s - step\",\n",
        "    \"dataset\": \"CIFAR-100\",\n",
        "    \"epochs\": 25,\n",
        "    \"batch_size\" : 128\n",
        "    }\n",
        ")\n",
        "\n",
        "'''\n",
        "class StochasticDepth(nn.Module):\n",
        "    def __init__(self, p_start=1.0, p_end=0.2):\n",
        "        super(StochasticDepth, self).__init__()\n",
        "        self.p_start = p_start\n",
        "        self.p_end = p_end\n",
        "\n",
        "    def forward(self, x):\n",
        "      if self.training:\n",
        "        p = self.p_start - (self.p_start - self.p_end) * random.random()\n",
        "        if random.random() < p:\n",
        "            return x\n",
        "        else:\n",
        "            return torch.zeros_like(x)\n",
        "      else:\n",
        "        return x\n",
        "\n",
        "def get_model(dropout_rate=None):\n",
        "    model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "    for i, layer in enumerate(model.features):\n",
        "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "            depth_prob = 1.0 - (0.5 / (len(model.features) - 1)) * i\n",
        "            layer = nn.Sequential(layer, StochasticDepth(depth_prob))\n",
        "            model.features[i] = layer\n",
        "\n",
        "    if dropout_rate is not None:\n",
        "        model.classifier[0] = nn.Dropout(p=dropout_rate, inplace=False)\n",
        "\n",
        "    return model.to(device)\n",
        "'''\n",
        "\n",
        "\n",
        "#weights = models.EfficientNet_V2_M_Weights.DEFAULT\n",
        "#weights.transforms()?\n",
        "\n",
        "training_dataset = datasets.CIFAR100('./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR100('./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "dataset_length = len(training_dataset)\n",
        "\n",
        "train_ratio = 0.8\n",
        "train_size = int(train_ratio * dataset_length)\n",
        "val_size = dataset_length - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(training_dataset, [train_size, val_size])\n",
        "\n",
        "training_dataloader = DataLoader(training_dataset, batch_size=128, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "#modified_EfficientNetV2M= ModifiedEfficientNetV2M(EfficientNetV2M)\n",
        "# EfficientNetV2M = models.efficientnet_v2_m(weights=models.EfficientNet_V2_M_Weights.IMAGENET1K_V1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def training_batch(train_loader, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    beta = 1.0\n",
        "    cutmix_prob = 0.5\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for batch_idx, (input, target) in enumerate(train_loader):\n",
        "        input = input.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        r = np.random.rand(1)\n",
        "\n",
        "        if beta > int(0.0) and r < cutmix_prob:\n",
        "            lam = np.random.beta(beta, beta)\n",
        "            rand_index = torch.randperm(input.size()[0]).cuda()\n",
        "            target_a = target # target이 label\n",
        "            target_b = target[rand_index]\n",
        "            bbx1, bby1, bbx2, bby2 = rand_bbox(input.size(), lam)\n",
        "            input[:, :, bbx1:bbx2, bby1:bby2] = input[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "                # adjust lambda to exactly match pixel ratio\n",
        "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (input.size()[-1] * input.size()[-2]))\n",
        "                # compute output\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target_a) * lam + criterion(output, target_b) * (1. - lam)\n",
        "        else:\n",
        "            # compute output\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        training_losses.append(loss.item())\n",
        "\n",
        "    for data, labels in train_loader:\n",
        "        training_accuracies.extend(accuracy(data, labels, model))\n",
        "    '''\n",
        "        trng_batch_loss = training_batch(training_dataloader, model_to_train, criterion, optimizer)\n",
        "        training_losses.append(trng_batch_loss)\n",
        "        trng_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        training_accuracies.extend(trng_batch_correct)\n",
        "    '''\n",
        "\n",
        "def test_batch(data, labels, model, criterion):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    return loss.item()\n",
        "\n",
        "def accuracy(data, labels, model):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    _, pred_labels = output.max(-1)\n",
        "    correct = (pred_labels == labels)\n",
        "    return correct.cpu().detach().numpy().tolist()\n",
        "\n",
        "#model_to_train = get_model()\n",
        "\n",
        "efficientnet = models.efficientnet_v2_s(weights = models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "model_to_train = efficientnet.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model_to_train.parameters(), lr=0.0004, weight_decay= 0.005)\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42) ### 랜덤 시드를 적용\n",
        "\n",
        "n_epochs = 25\n",
        "steps_epoch = len(training_dataloader)\n",
        "#scheduler = OneCycleLR(optimizer, max_lr=0.01, epochs=5, steps_per_epoch=5)\n",
        "\n",
        "\n",
        "\n",
        "training_loss, test_loss = [], []\n",
        "training_accuracy, test_accuracy = [], []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    training_losses, test_losses = [], []\n",
        "    training_accuracies, test_accuracies = [], []\n",
        "\n",
        "    time1 = time.time()\n",
        "    training_batch(training_dataloader, model_to_train, criterion, optimizer)\n",
        "    '''\n",
        "        trng_batch_loss = training_batch(training_dataloader, model_to_train, criterion, optimizer)\n",
        "        training_losses.append(trng_batch_loss)\n",
        "        trng_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        training_accuracies.extend(trng_batch_correct)\n",
        "    '''\n",
        "    training_per_epoch_loss = np.array(training_losses).mean()\n",
        "    training_per_epoch_accuracy = np.mean(training_accuracies)\n",
        "    time2 = time.time()\n",
        "    print(\"Respective Execution time : \" + str(time2 - time1) + \"sec    \" + str(epoch+1) + \" epochs\")\n",
        "\n",
        "    learning_rate = optimizer.param_groups[0]['lr']\n",
        "    #scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "    for data, labels in val_dataloader:\n",
        "        val_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        test_accuracies.extend(val_batch_correct)\n",
        "    val_per_epoch_accuracy = np.mean(test_accuracies)\n",
        "\n",
        "\n",
        "    for data, labels in test_dataloader:\n",
        "        tst_batch_loss = test_batch(data, labels, model_to_train, criterion)\n",
        "        test_losses.append(tst_batch_loss)\n",
        "        tst_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        test_accuracies.extend(tst_batch_correct)\n",
        "    test_per_epoch_loss = np.array(test_losses).mean()\n",
        "    test_per_epoch_accuracy = np.mean(test_accuracies)\n",
        "\n",
        "    training_loss.append(training_per_epoch_loss)\n",
        "    training_accuracy.append(training_per_epoch_accuracy)\n",
        "    test_loss.append(test_per_epoch_loss)\n",
        "    test_accuracy.append(test_per_epoch_accuracy)\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}/{n_epochs}\\t| Training loss: {training_per_epoch_loss:.4f} | ', end='')\n",
        "    print(f'Training accuracy: {training_per_epoch_accuracy:.4f} | Validation accuracy: {val_per_epoch_accuracy:.4f} ', end='')\n",
        "    print(f'Test loss: {test_per_epoch_loss:.4f} |  Test accuracy: {test_per_epoch_accuracy:.4f} |')\n",
        "    print(f'learning rate: {learning_rate:.6f}' + \"\\n\")\n",
        "\n",
        "    wandb.log({\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"training_loss\": training_per_epoch_loss,\n",
        "    \"training_accuracy\": training_per_epoch_accuracy,\n",
        "    \"test_loss\": test_per_epoch_loss,\n",
        "    \"test_accuracy\": test_per_epoch_accuracy,\n",
        "    \"validation_accuracy\" : val_per_epoch_accuracy\n",
        "    })\n",
        "\n",
        "\n",
        "    with open(text_path, \"a\") as file:\n",
        "        # 파일에 쓸 텍스트 작성\n",
        "        file.write(f'Epoch: {epoch + 1}/{n_epochs}\\t| Training loss: {training_loss[epoch]:.4f} | ')\n",
        "        file.write(f'Training accuracy: {training_accuracy[epoch]:.4f} | Validation accuracy: {val_per_epoch_accuracy:.4f} ')\n",
        "        file.write(f'Test loss: {test_loss[epoch]:.4f} | Test accuracy: {test_per_epoch_accuracy:.4f} \\n')\n",
        "        file.write(f'learning rate: {learning_rate:.6f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYUjcmUydVh3"
      },
      "source": [
        "## 진한 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnW8nFVk39VJ",
        "outputId": "a1d62840-e106-4110-c02b-c3ab37f2db58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1\n",
            "Supermodel Training Loss: 2.7431, Accuracy: 15.68%\n",
            "Supermodel Test Accuracy: 20.00%\n",
            "Submodel Training Loss: 3.8411, Accuracy: 11.13%\n",
            "Submodel Training Loss: 3.8921, Accuracy: 10.33%\n",
            "Submodel Training Loss: 3.7665, Accuracy: 12.48%\n",
            "Submodel Training Loss: 3.8373, Accuracy: 10.96%\n",
            "Submodel Training Loss: 3.8565, Accuracy: 10.78%\n",
            "Submodel Test Accuracy: 18.81%\n",
            "Epoch 2\n",
            "Supermodel Training Loss: 2.5568, Accuracy: 22.07%\n",
            "Supermodel Test Accuracy: 26.25%\n",
            "Submodel Training Loss: 3.3707, Accuracy: 18.46%\n",
            "Submodel Training Loss: 3.3727, Accuracy: 18.47%\n",
            "Submodel Training Loss: 3.2917, Accuracy: 20.18%\n",
            "Submodel Training Loss: 3.3453, Accuracy: 18.65%\n",
            "Submodel Training Loss: 3.3682, Accuracy: 18.40%\n",
            "Submodel Test Accuracy: 23.93%\n",
            "Epoch 3\n",
            "Supermodel Training Loss: 2.4484, Accuracy: 25.34%\n",
            "Supermodel Test Accuracy: 23.75%\n",
            "Submodel Training Loss: 3.1848, Accuracy: 21.96%\n",
            "Submodel Training Loss: 3.1567, Accuracy: 22.58%\n",
            "Submodel Training Loss: 3.0986, Accuracy: 23.74%\n",
            "Submodel Training Loss: 3.1554, Accuracy: 22.14%\n",
            "Submodel Training Loss: 3.1871, Accuracy: 21.56%\n",
            "Submodel Test Accuracy: 26.50%\n",
            "Epoch 4\n",
            "Supermodel Training Loss: 2.3717, Accuracy: 27.78%\n",
            "Supermodel Test Accuracy: 26.25%\n",
            "Submodel Training Loss: 3.0686, Accuracy: 23.83%\n",
            "Submodel Training Loss: 3.0145, Accuracy: 25.11%\n",
            "Submodel Training Loss: 2.9609, Accuracy: 26.16%\n",
            "Submodel Training Loss: 3.0334, Accuracy: 24.44%\n",
            "Submodel Training Loss: 3.0545, Accuracy: 24.04%\n",
            "Submodel Test Accuracy: 29.27%\n",
            "Epoch 5\n",
            "Supermodel Training Loss: 2.3089, Accuracy: 29.63%\n",
            "Supermodel Test Accuracy: 31.25%\n",
            "Submodel Training Loss: 2.9714, Accuracy: 26.10%\n",
            "Submodel Training Loss: 2.9016, Accuracy: 27.12%\n",
            "Submodel Training Loss: 2.8611, Accuracy: 28.03%\n",
            "Submodel Training Loss: 2.9495, Accuracy: 26.00%\n",
            "Submodel Training Loss: 2.9618, Accuracy: 25.79%\n",
            "Submodel Test Accuracy: 30.31%\n",
            "Epoch 6\n",
            "Supermodel Training Loss: 2.2608, Accuracy: 30.94%\n",
            "Supermodel Test Accuracy: 27.50%\n",
            "Submodel Training Loss: 2.8972, Accuracy: 27.55%\n",
            "Submodel Training Loss: 2.8285, Accuracy: 28.73%\n",
            "Submodel Training Loss: 2.7843, Accuracy: 29.76%\n",
            "Submodel Training Loss: 2.8793, Accuracy: 27.52%\n",
            "Submodel Training Loss: 2.8869, Accuracy: 27.21%\n",
            "Submodel Test Accuracy: 31.03%\n",
            "Epoch 7\n",
            "Supermodel Training Loss: 2.2168, Accuracy: 32.39%\n",
            "Supermodel Test Accuracy: 27.50%\n",
            "Submodel Training Loss: 2.8486, Accuracy: 28.60%\n",
            "Submodel Training Loss: 2.7662, Accuracy: 30.07%\n",
            "Submodel Training Loss: 2.7170, Accuracy: 30.76%\n",
            "Submodel Training Loss: 2.8212, Accuracy: 28.80%\n",
            "Submodel Training Loss: 2.8248, Accuracy: 28.55%\n",
            "Submodel Test Accuracy: 32.48%\n",
            "Epoch 8\n",
            "Supermodel Training Loss: 2.1753, Accuracy: 33.60%\n",
            "Supermodel Test Accuracy: 30.00%\n",
            "Submodel Training Loss: 2.7936, Accuracy: 29.29%\n",
            "Submodel Training Loss: 2.7159, Accuracy: 31.18%\n",
            "Submodel Training Loss: 2.6637, Accuracy: 32.33%\n",
            "Submodel Training Loss: 2.7694, Accuracy: 29.64%\n",
            "Submodel Training Loss: 2.7733, Accuracy: 29.55%\n",
            "Submodel Test Accuracy: 33.85%\n",
            "Epoch 9\n",
            "Supermodel Training Loss: 2.1516, Accuracy: 34.49%\n",
            "Supermodel Test Accuracy: 33.75%\n",
            "Submodel Training Loss: 2.7445, Accuracy: 30.47%\n",
            "Submodel Training Loss: 2.6706, Accuracy: 31.93%\n",
            "Submodel Training Loss: 2.6255, Accuracy: 32.79%\n",
            "Submodel Training Loss: 2.7262, Accuracy: 30.40%\n",
            "Submodel Training Loss: 2.7284, Accuracy: 30.42%\n",
            "Submodel Test Accuracy: 33.76%\n",
            "Epoch 10\n",
            "Supermodel Training Loss: 2.1227, Accuracy: 35.13%\n",
            "Supermodel Test Accuracy: 33.75%\n",
            "Submodel Training Loss: 2.7052, Accuracy: 31.39%\n",
            "Submodel Training Loss: 2.6343, Accuracy: 32.66%\n",
            "Submodel Training Loss: 2.5907, Accuracy: 33.60%\n",
            "Submodel Training Loss: 2.6931, Accuracy: 31.19%\n",
            "Submodel Training Loss: 2.6958, Accuracy: 30.83%\n",
            "Submodel Test Accuracy: 35.97%\n",
            "Submodel Training Loss after epoch 1:  2.639\n",
            "Submodel Training Loss after epoch 1:  2.581\n",
            "Submodel Training Loss after epoch 1:  2.515\n",
            "Submodel Training Loss after epoch 1:  2.646\n",
            "Submodel Training Loss after epoch 1:  2.661\n",
            "ENSENBLE Test Accuracy: 38.84\n",
            "ENSENBLE Test Accuracy: 38.84%\n"
          ]
        }
      ],
      "source": [
        "#서로 다른 augment 2개씩 적용 5개 model voting// super class 1개 + cifar100 1개\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR100\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as model꿈\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import transforms, RandAugment\n",
        "import time\n",
        "import random\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "input_size = 32 * 32 * 3  # CIFAR-10 이미지 크기: 32x32x3\n",
        "hidden_size = 256\n",
        "num_classes = 20\n",
        "num_epochs = 10\n",
        "batch_size = 128\n",
        "learning_rate = 0.001\n",
        "\n",
        "# 다양한 데이터 증강을 위한 변환 정의\n",
        "transform1 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomRotation(degrees=(-10, 10)),  # 이미지를 -10도에서 10도 사이에서 무작위로 회전\n",
        "    transforms.ColorJitter(brightness=0.5),  # 이미지 밝기를 0.5만큼 무작위로 조절\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform2 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.ColorJitter(brightness=0.5),  # 이미지 밝기를 0.5만큼 무작위로 조절\n",
        "    transforms.ColorJitter(saturation=0.5),  # 이미지 채도를 0.5만큼 무작위로 조절\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform3 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.ColorJitter(saturation=0.5),  # 이미지 채도를 0.5만큼 무작위로 조절\n",
        "    transforms.ColorJitter(contrast=0.2),  # 이미지 대비를 무작위로 조절\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform4 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ColorJitter(contrast=0.2),  # 이미지 대비를 무작위로 조절\n",
        "    transforms.RandomVerticalFlip(),  # 이미지를 수직으로 무작위로 뒤집기\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform5 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomVerticalFlip(),  # 이미지를 수직으로 무작위로 뒤집기\n",
        "    transforms.RandomRotation(degrees=(-10, 10)),  # 이미지를 -10도에서 10도 사이에서 무작위로 회전\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "transform6 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "\n",
        "# Load CIFAR-100 dataset with different transforms for each model\n",
        "train_datasets = [datasets.CIFAR100(root='./data', train=True, download=True, transform=transform) for transform in [transform1, transform2, transform3, transform4, transform5]]\n",
        "test_datasets = [datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)]\n",
        "\n",
        "super_train_datasets = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform6)\n",
        "super_test_datasets = datasets.CIFAR100(root='./data', train=True, download=True, transform=test_transform)\n",
        "\n",
        "train_loaders = [DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True) for train_dataset in train_datasets]\n",
        "test_loaders = [DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) for test_dataset in test_datasets]\n",
        "\n",
        "superclass_trainloader = DataLoader(dataset=super_train_datasets, batch_size=batch_size, shuffle=True)\n",
        "superclass_testloader = DataLoader(dataset=super_test_datasets, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "'''\n",
        "train_dataset=datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset=datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "'''\n",
        "\n",
        "'''train_loader=DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader=DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "'''\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(model, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=3,\n",
        "            out_channels=8,\n",
        "            kernel_size=3,\n",
        "            padding=1)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=8,\n",
        "            out_channels=16,\n",
        "            kernel_size=3,\n",
        "            padding=1)\n",
        "        self.pool = nn.MaxPool2d(\n",
        "            kernel_size=2,\n",
        "            stride=2\n",
        "        )\n",
        "        self.fc1 = nn.Linear(8 * 8 * 16, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 100)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(8)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(16)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = torch.relu(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = x.view(-1, 8 * 8 * 16)\n",
        "        x = self.fc1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class model2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(model2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=3,\n",
        "            out_channels=8,\n",
        "            kernel_size=3,\n",
        "            padding=1)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=8,\n",
        "            out_channels=16,\n",
        "            kernel_size=3,\n",
        "            padding=1)\n",
        "        self.pool = nn.MaxPool2d(\n",
        "            kernel_size=2,\n",
        "            stride=2\n",
        "        )\n",
        "        self.fc1 = nn.Linear(8 * 8 * 16, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 20)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(8)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(16)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = torch.relu(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = x.view(-1, 8 * 8 * 16)\n",
        "        x = self.fc1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "submodels = [model().to(device) for _ in range(5)]\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizers = [optim.Adam(submodel.parameters(), lr=learning_rate) for submodel in submodels]\n",
        "\n",
        "supermodel = model2().to(device)\n",
        "super_criterion = nn.CrossEntropyLoss()\n",
        "super_optimizer = optim.Adam(supermodel.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}\")\n",
        "    #1개의 superclass submodel 훈련\n",
        "    supermodel.train()\n",
        "    running_loss=0.\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, data in enumerate(superclass_trainloader, 0):\n",
        "      inputs, labels = data\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      super_optimizer.zero_grad()\n",
        "      outputs = supermodel(inputs)\n",
        "      loss = super_criterion(outputs, labels//5)\n",
        "      loss.backward()\n",
        "      super_optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels//5).sum().item()\n",
        "    print(f\"Supermodel Training Loss: {running_loss / len(superclass_trainloader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, data in enumerate(superclass_testloader, 0):\n",
        "        supermodel.eval()\n",
        "        correct=0\n",
        "        total=0\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = supermodel(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels//5).sum().item()\n",
        "    print(f\"Supermodel Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "    #5개의 cifar-100 submodel 훈련\n",
        "    for submodel, train_loader, optimizer in zip(submodels, train_loaders, optimizers):\n",
        "        submodel.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = submodel(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f\"Submodel Training Loss: {running_loss / len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "    for submodel, test_loader in zip(submodels, test_loaders):\n",
        "      submodel.eval()\n",
        "      correct = 0\n",
        "      total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "          outputs = submodel(inputs)\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "    print(f\"Submodel Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# Create ensemble model using soft voting\n",
        "ensemble_model = VotingClassifier(estimators=[('model'+str(i), submodel) for i, submodel in enumerate(submodels)], voting='soft')\n",
        "\n",
        "# Fit ensemble model\n",
        "for submodel, train_loader in zip(submodels, train_loaders):\n",
        "    for epoch in range(1):\n",
        "        submodel.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = submodel(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Submodel Training Loss after epoch {epoch + 1}: {running_loss / len(train_loader): .3f}\")\n",
        "\n",
        "# Evaluate ensemble model\n",
        "with torch.no_grad():\n",
        "    for test_loader in test_loaders:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_outputs = []\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs_list = [submodel(inputs) for submodel in submodels]\n",
        "            superclass = predicted//5\n",
        "            super_output = supermodel(inputs)\n",
        "            _, super_predicted = torch.max(super_output, 1)\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs_super_list = [outputs // 20 for outputs in outputs_list]\n",
        "            ensemble_outputs = torch.stack(outputs_list, dim=0).mean(dim=0)  # 각 모델의 예측 확률을 평균하여 앙상블 예측 생성\n",
        "            _, predicted = torch.max(ensemble_outputs, 1)\n",
        "            for i in range(batch_size):\n",
        "              if i < ensemble_outputs.shape[0]:\n",
        "                if (super_predicted[i]) != predicted[i]:\n",
        "                 ensemble_outputs[i][predicted[i]]=0\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        print(f'ENSENBLE Test Accuracy: {100* correct / total:.2f}')\n",
        "        ''' _, predicted = torch.max(outputs, 1)\n",
        "            if (predicted[0]//5) == (super_predicted[0]):\n",
        "              print(f\"predicted: {predicted}\")\n",
        "              predicted=predicted\n",
        "            else:\n",
        "              outputs_list = [x for x in outputs_list if x[0]//5 == super_predicted[0]]\n",
        "              outputs = torch.stack(outputs_list, dim=0).mean(dim=0)\n",
        "              _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()'''\n",
        "        print(f'ENSENBLE Test Accuracy: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8bNhw_0YxQ-",
        "outputId": "5cb6a2d3-293a-4eb4-8ba8-8cb679bafc48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1\n",
            "Submodel Training Loss: 4.1362, Accuracy: 6.95%\n",
            "Submodel Training Loss: 4.1843, Accuracy: 6.18%\n",
            "Submodel Training Loss: 4.1561, Accuracy: 7.09%\n",
            "Submodel Training Loss: 4.1180, Accuracy: 7.26%\n",
            "Submodel Training Loss: 4.2417, Accuracy: 5.56%\n",
            "Submodel Test Accuracy: 8.84%\n",
            "Epoch 2\n",
            "Submodel Training Loss: 3.6981, Accuracy: 12.53%\n",
            "Submodel Training Loss: 3.7359, Accuracy: 11.91%\n",
            "Submodel Training Loss: 3.7373, Accuracy: 12.17%\n",
            "Submodel Training Loss: 3.6666, Accuracy: 12.99%\n",
            "Submodel Training Loss: 3.7973, Accuracy: 10.95%\n",
            "Submodel Test Accuracy: 11.93%\n",
            "Epoch 3\n",
            "Submodel Training Loss: 3.5336, Accuracy: 15.09%\n",
            "Submodel Training Loss: 3.5590, Accuracy: 14.81%\n",
            "Submodel Training Loss: 3.5758, Accuracy: 15.03%\n",
            "Submodel Training Loss: 3.4704, Accuracy: 16.16%\n",
            "Submodel Training Loss: 3.6030, Accuracy: 13.77%\n",
            "Submodel Test Accuracy: 13.81%\n",
            "Epoch 4\n",
            "Submodel Training Loss: 3.4208, Accuracy: 17.07%\n",
            "Submodel Training Loss: 3.4326, Accuracy: 16.84%\n",
            "Submodel Training Loss: 3.4763, Accuracy: 16.28%\n",
            "Submodel Training Loss: 3.3379, Accuracy: 18.35%\n",
            "Submodel Training Loss: 3.4748, Accuracy: 16.03%\n",
            "Submodel Test Accuracy: 16.81%\n",
            "Epoch 5\n",
            "Submodel Training Loss: 3.3440, Accuracy: 18.15%\n",
            "Submodel Training Loss: 3.3526, Accuracy: 18.27%\n",
            "Submodel Training Loss: 3.3966, Accuracy: 17.77%\n",
            "Submodel Training Loss: 3.2551, Accuracy: 19.81%\n",
            "Submodel Training Loss: 3.3889, Accuracy: 17.37%\n",
            "Submodel Test Accuracy: 17.55%\n",
            "Submodel Training Loss after epoch 1:  3.297\n",
            "Submodel Training Loss after epoch 1:  3.326\n",
            "Submodel Training Loss after epoch 1:  3.346\n",
            "Submodel Training Loss after epoch 1:  3.219\n",
            "Submodel Training Loss after epoch 1:  3.324\n",
            "ENSENBLE Test Accuracy: 24.98%\n",
            "ENSENBLE Test Accuracy: 22.57%\n",
            "ENSENBLE Test Accuracy: 25.09%\n",
            "ENSENBLE Test Accuracy: 26.30%\n",
            "ENSENBLE Test Accuracy: 21.34%\n"
          ]
        }
      ],
      "source": [
        "#서로 다른 augment적용 5개 model voting\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR100\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import transforms, RandAugment\n",
        "import time\n",
        "import random\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# 다양한 데이터 증강을 위한 변환 정의\n",
        "transform1 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomRotation(degrees=(-10, 10)),  # 이미지를 -10도에서 10도 사이에서 무작위로 회전\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform2 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.ColorJitter(brightness=0.5),  # 이미지 밝기를 0.5만큼 무작위로 조절\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform3 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.ColorJitter(saturation=0.5),  # 이미지 채도를 0.5만큼 무작위로 조절\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform4 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform5 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomVerticalFlip(),  # 이미지를 수직으로 무작위로 뒤집기\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-100 dataset with different transforms for each model\n",
        "train_datasets = [datasets.CIFAR100(root='./data', train=True, download=True, transform=transform) for transform in [transform1, transform2, transform3, transform4, transform5]]\n",
        "test_datasets = [datasets.CIFAR100(root='./data', train=False, download=True, transform=transform) for transform in [transform1, transform2, transform3, transform4, transform5]]\n",
        "\n",
        "train_loaders = [DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True) for train_dataset in train_datasets]\n",
        "test_loaders = [DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) for test_dataset in test_datasets]\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(model, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=3,\n",
        "            out_channels=8,\n",
        "            kernel_size=3,\n",
        "            padding=1)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=8,\n",
        "            out_channels=16,\n",
        "            kernel_size=3,\n",
        "            padding=1)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            in_channels=16,\n",
        "            out_channels=32,\n",
        "            kernel_size=3,\n",
        "            padding=1)\n",
        "        self.pool = nn.MaxPool2d(\n",
        "            kernel_size=2,\n",
        "            stride=2\n",
        "        )\n",
        "        self.fc1 = nn.Linear(4 * 4 * 32, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 100)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(8)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(16)\n",
        "        self.bn3 = torch.nn.BatchNorm2d(32)\n",
        "        self.bn4 = torch.nn.BatchNorm1d(256)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = torch.relu(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv3(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = x.view(-1, 4 * 4 * 32)\n",
        "        if self.training:\n",
        "          x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn4(x)\n",
        "        x = torch.relu(x)\n",
        "        if self.training:\n",
        "          x=self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn4(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "submodels = [model().to(device) for _ in range(5)]\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizers = [optim.Adam(submodel.parameters(), lr=learning_rate) for submodel in submodels]\n",
        "\n",
        "for epoch in range(5):\n",
        "    print(f\"Epoch {epoch + 1}\")\n",
        "    for submodel, train_loader, optimizer in zip(submodels, train_loaders, optimizers):\n",
        "        submodel.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = submodel(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f\"Submodel Training Loss: {running_loss / len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "    for submodel, test_loader in zip(submodels, test_loaders):\n",
        "      submodel.eval()\n",
        "      correct = 0\n",
        "      total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "          outputs = submodel(inputs)\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "    print(f\"Submodel Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# Create ensemble model using soft voting\n",
        "ensemble_model = VotingClassifier(estimators=[('model'+str(i), submodel) for i, submodel in enumerate(submodels)], voting='soft')\n",
        "\n",
        "# Fit ensemble model\n",
        "for submodel, train_loader in zip(submodels, train_loaders):\n",
        "    for epoch in range(1):\n",
        "        submodel.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = submodel(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Submodel Training Loss after epoch {epoch + 1}: {running_loss / len(train_loader): .3f}\")\n",
        "\n",
        "# Evaluate ensemble model\n",
        "with torch.no_grad():\n",
        "    for test_loader in test_loaders:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_outputs = []\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs_list = [submodel(inputs) for submodel in submodels]\n",
        "            outputs = torch.stack(outputs_list, dim=0).mean(dim=0)  # 각 모델의 예측 확률을 평균하여 앙상블 예측 생성\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        print(f'ENSENBLE Test Accuracy: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNGvv0zAKRgN",
        "outputId": "ea0e96ac-2974-410b-ee83-8f95b509d1ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import transforms, RandAugment\n",
        "import time\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "\n",
        "# 1. 데이터셋 및 데이터로더 준비\n",
        "def get_dataloader(train, batch_size, image_size, augment=False, num_ops=2, magnitude=9):\n",
        "  if not augment:\n",
        "    transform_list = [\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "    ]\n",
        "    transform = transforms.Compose(transform_list)\n",
        "    dataset = CIFAR100(root='./data', train=train, download=True, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  else:\n",
        "    augment_list = [\n",
        "        RandAugment(num_ops, magnitude),  # RandAugment를 먼저 적용\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "        ]\n",
        "    transform = transforms.Compose(augment_list)\n",
        "    dataset = CIFAR100(root='./data', train=train, download=True, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "# 2. 모델 준비\n",
        "def get_model(dropout_rate=None):\n",
        "    model = models.efficientnet_v2_m(weights=models.EfficientNet_V2_M_Weights.IMAGENET1K_V1)\n",
        "    if dropout_rate is not None:\n",
        "        # EfficientNet 모델의 마지막 FC 레이어 전, Dropout 비율 수정\n",
        "        model.classifier[0] = torch.nn.Dropout(p=dropout_rate, inplace=False)\n",
        "    return model.to(device)\n",
        "\n",
        "# 3. 모델 학습 및 평가\n",
        "## 3.1 모델 학습에 사용될 함수 정의\n",
        "def training_batch(data, labels, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def test_batch(data, labels, model, criterion):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    return loss.item()\n",
        "\n",
        "def accuracy(data, labels, model):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    _, pred_labels = output.max(-1)\n",
        "    correct = (pred_labels == labels)\n",
        "    return correct.cpu().detach().numpy().tolist()\n",
        "\n",
        "## 3.2 Hyperparameter 설정\n",
        "model_to_train = get_model()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_to_train.parameters(), lr=0.01)\n",
        "\n",
        "training_loss, test_loss = [], []\n",
        "training_accuracy, test_accuracy = [], []\n",
        "\n",
        "n_epochs = 100\n",
        "batch_size = 64\n",
        "image_size = 224\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    if epoch % 20 == 0:\n",
        "        # 20 epoch마다 변경할 설정\n",
        "        image_size = 224 + (epoch // 20) * 32       # image size 증가\n",
        "        dropout_rate = 0.2 + (epoch // 20) * 0.05   # dropout 비율 증가\n",
        "        num_ops = 2 + (epoch // 20) * 2             # RandAugment 매개변수 증가\n",
        "        magnitude = 3 + (epoch // 20) * 2           # RandAugment 매개변수 증가\n",
        "        # mixup_alpha = 0 + (epoch // 20) * 1         # mixup alpha 증가\n",
        "\n",
        "        training_dataloader = get_dataloader(train=True, batch_size=batch_size, image_size=image_size, augment=True, num_ops=num_ops, magnitude=magnitude)\n",
        "        test_dataloader = get_dataloader(train=False, batch_size=batch_size, image_size=image_size)\n",
        "        model_to_train = get_model(dropout_rate=dropout_rate)\n",
        "        optimizer = optim.Adam(model_to_train.parameters(), lr=0.01)\n",
        "\n",
        "    training_losses, test_losses = [], []\n",
        "    training_accuracies, test_accuracies = [], []\n",
        "\n",
        "\n",
        "    # 현재 에포크의 시작 시간 측정\n",
        "    time1 = time.time()\n",
        "\n",
        "    # train 데이터 로더를 통해 배치 단위로 데이터 로딩 및 학습\n",
        "    for data, labels in training_dataloader:\n",
        "        trng_batch_loss = training_batch(data, labels, model_to_train, criterion, optimizer)\n",
        "        training_losses.append(trng_batch_loss)\n",
        "        trng_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        training_accuracies.extend(trng_batch_correct)\n",
        "\n",
        "    # 현재 에포크의 평균 학습 손실 및 정확도 계산\n",
        "    training_per_epoch_loss = np.array(training_losses).mean()\n",
        "    training_per_epoch_accuracy = np.mean(training_accuracies)\n",
        "\n",
        "    # 현재 에포크의 종료 시간 측정\n",
        "    time2 = time.time()\n",
        "    print(\"Respective Execution time : \" + str(time2 - time1) + \"   \" + str(epoch) + \"epochs\")\n",
        "\n",
        "\n",
        "    # test 데이터 로더를 통해 배치 단위로 데이터 로딩 및 평가\n",
        "    for data, labels in test_dataloader:\n",
        "        tst_batch_loss = test_batch(data, labels, model_to_train, criterion)\n",
        "        test_losses.append(tst_batch_loss)\n",
        "        tst_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        test_accuracies.extend(tst_batch_correct)\n",
        "    # 현재 에포크의 평균 테스트 손실 및 정확도 계산\n",
        "    test_per_epoch_loss = np.array(test_losses).mean()\n",
        "    test_per_epoch_accuracy = np.mean(test_accuracies)\n",
        "\n",
        "    # 계산된 평균 손실 및 정확도를 저장\n",
        "    training_loss.append(training_per_epoch_loss)\n",
        "    training_accuracy.append(training_per_epoch_accuracy)\n",
        "    test_loss.append(test_per_epoch_loss)\n",
        "    test_accuracy.append(test_per_epoch_accuracy)\n",
        "\n",
        "    # 현재 에포크의 학습 및 테스트 결과 출력\n",
        "    print(f'Epoch: {epoch + 1}/{n_epochs}\\t| Training loss: {training_per_epoch_loss:.4f} | ', end='')\n",
        "    print(f'Training accuracy: {training_per_epoch_accuracy:.4f} | Test loss: {test_per_epoch_loss:.4f} | ', end='')\n",
        "    print(f'Test accuracy: {test_per_epoch_accuracy:.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZb-ng_nbJgD",
        "outputId": "68c89693-6218-4ddc-ab6e-d2b8f061e78d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "#세션 다운 해결될까\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import random\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# 1. 데이터셋 및 데이터로더 준비\n",
        "def get_dataloader(train, batch_size, image_size, augment=False, num_ops=2, magnitude=9):\n",
        "    if not augment:\n",
        "        transform_list = [\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "        ]\n",
        "    else:\n",
        "        augment_list = [\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(image_size, padding=4),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "        ]\n",
        "        transform_list = augment_list\n",
        "\n",
        "    transform = transforms.Compose(transform_list)\n",
        "    dataset = CIFAR100(root='./data', train=train, download=True, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    return dataloader\n",
        "\n",
        "# 2. 모델 준비\n",
        "class StochasticDepth(nn.Module):\n",
        "    def __init__(self, p_start=1.0, p_end=0.5):\n",
        "        super(StochasticDepth, self).__init__()\n",
        "        self.p_start = p_start\n",
        "        self.p_end = p_end\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.p_start - (self.p_start - self.p_end) * random.random()\n",
        "        if random.random() < p:\n",
        "            return x\n",
        "        else:\n",
        "            return torch.zeros_like(x)\n",
        "\n",
        "def get_model(dropout_rate=None):\n",
        "    model = models.efficientnet_v2_m(weights=models.EfficientNet_V2_M_Weights.IMAGENET1K_V1)\n",
        "    for i, layer in enumerate(model.features):\n",
        "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "            depth_prob = 1.0 - (0.5 / (len(model.features) - 1)) * i\n",
        "            layer = nn.Sequential(layer, StochasticDepth(depth_prob))\n",
        "            model.features[i] = layer\n",
        "\n",
        "    if dropout_rate is not None:\n",
        "        model.classifier[0] = nn.Dropout(p=dropout_rate, inplace=False)\n",
        "\n",
        "    return model.to(device)\n",
        "\n",
        "# 3. 모델 학습 및 평가\n",
        "def training_batch(data, labels, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def test_batch(data, labels, model, criterion):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    return loss.item()\n",
        "\n",
        "def accuracy(data, labels, model):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    _, pred_labels = output.max(-1)\n",
        "    correct = (pred_labels == labels)\n",
        "    return correct.cpu().detach().numpy().tolist()\n",
        "\n",
        "# Hyperparameters\n",
        "n_epochs = 100\n",
        "batch_size = 64\n",
        "image_size = 224\n",
        "\n",
        "# Model, criterion, optimizer\n",
        "model_to_train = get_model()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_to_train.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(n_epochs):\n",
        "    if epoch % 20 == 0:\n",
        "        # Data preparation\n",
        "        image_size = 224 + (epoch // 20) * 32\n",
        "        training_dataloader = get_dataloader(train=True, batch_size=batch_size, image_size=image_size, augment=True)\n",
        "        test_dataloader = get_dataloader(train=False, batch_size=batch_size, image_size=image_size)\n",
        "        model_to_train = get_model()\n",
        "        optimizer = optim.Adam(model_to_train.parameters(), lr=0.01)\n",
        "\n",
        "    training_losses, test_losses = [], []\n",
        "    training_accuracies, test_accuracies = [], []\n",
        "\n",
        "    time1 = time.time()\n",
        "    for data, labels in training_dataloader:\n",
        "        trng_batch_loss = training_batch(data, labels, model_to_train, criterion, optimizer)\n",
        "        training_losses.append(trng_batch_loss)\n",
        "        trng_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        training_accuracies.extend(trng_batch_correct)\n",
        "\n",
        "    training_per_epoch_loss = np.array(training_losses).mean()\n",
        "    training_per_epoch_accuracy = np.mean(training_accuracies)\n",
        "\n",
        "    time2 = time.time()\n",
        "    print(\"Respective Execution time : \" + str(time2 - time1) + \"   \" + str(epoch) + \"epochs\")\n",
        "\n",
        "    for data, labels in test_dataloader:\n",
        "        tst_batch_loss = test_batch(data, labels, model_to_train, criterion)\n",
        "        test_losses.append(tst_batch_loss)\n",
        "        tst_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        test_accuracies.extend(tst_batch_correct)\n",
        "\n",
        "    test_per_epoch_loss = np.array(test_losses).mean()\n",
        "    test_per_epoch_accuracy = np.mean(test_accuracies)\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}/{n_epochs}\\t| Training loss: {training_per_epoch_loss:.4f} | ', end='')\n",
        "    print(f'Training accuracy: {training_per_epoch_accuracy:.4f} | Test loss: {test_per_epoch_loss:.4f} | ', end='')\n",
        "    print(f'Test accuracy: {test_per_epoch_accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk6Sj_n_4vqr",
        "outputId": "86875918-afd1-45b8-c454-26518eb5760a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "#+stochastic dpeth\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import transforms, RandAugment\n",
        "import time\n",
        "import random\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "\n",
        "# 1. 데이터셋 및 데이터로더 준비\n",
        "def get_dataloader(train, batch_size, image_size, augment=False, num_ops=2, magnitude=9):\n",
        "  if not augment:\n",
        "    transform_list = [\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "    ]\n",
        "    transform = transforms.Compose(transform_list)\n",
        "    dataset = CIFAR100(root='./data', train=train, download=True, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  else:\n",
        "    augment_list = [\n",
        "        RandAugment(num_ops, magnitude),  # RandAugment를 먼저 적용\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "        ]\n",
        "    transform = transforms.Compose(augment_list)\n",
        "    dataset = CIFAR100(root='./data', train=train, download=True, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "#Stochastic Depth구현\n",
        "class StochasticDepth(nn.Module):\n",
        "    def __init__(self, p_start=1.0, p_end=0.5):\n",
        "        super(StochasticDepth, self).__init__()\n",
        "        self.p_start = p_start\n",
        "        self.p_end = p_end\n",
        "\n",
        "    def forward(self, x, num_layers):\n",
        "        p = self.p_start - (self.p_start - self.p_end) * i / num_layers\n",
        "        if random.random() < p:\n",
        "            return x\n",
        "        else:\n",
        "            return torch.zeros_like(x)\n",
        "\n",
        "# 2. 모델 준비\n",
        "def get_model(dropout_rate=None):\n",
        "    depth_prob = 1.0  # 시작 확률\n",
        "    num_layers = 0\n",
        "    model = models.efficientnet_v2_m(weights=models.EfficientNet_V2_M_Weights.IMAGENET1K_V1)\n",
        "    num_layers = len(model.features)\n",
        "    for i, layer in enumerate(model.features):\n",
        "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "            depth_prob =1.0 - (0.5) / (layer - 1) * i  # 각 레이어에 대한 확률 계산\n",
        "            print(depth_prob)\n",
        "            layer = nn.Sequential(layer, StochasticDepth(depth_prob, num_layers))\n",
        "            model.features[i] = layer\n",
        "    if dropout_rate is not None:\n",
        "        # EfficientNet 모델의 마지막 FC 레이어 전, Dropout 비율 수정\n",
        "        model.classifier[0] = torch.nn.Dropout(p=dropout_rate, inplace=False)\n",
        "    return model.to(device)\n",
        "\n",
        "# 3. 모델 학습 및 평가\n",
        "## 3.1 모델 학습에 사용될 함수 정의\n",
        "def training_batch(data, labels, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def test_batch(data, labels, model, criterion):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    return loss.item()\n",
        "\n",
        "def accuracy(data, labels, model):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    _, pred_labels = output.max(-1)\n",
        "    correct = (pred_labels == labels)\n",
        "    return correct.cpu().detach().numpy().tolist()\n",
        "\n",
        "## 3.2 Hyperparameter 설정\n",
        "model_to_train = get_model()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_to_train.parameters(), lr=0.01)\n",
        "\n",
        "training_loss, test_loss = [], []\n",
        "training_accuracy, test_accuracy = [], []\n",
        "\n",
        "n_epochs = 100\n",
        "batch_size = 64\n",
        "image_size = 224\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    if epoch % 20 == 0:\n",
        "        # 20 epoch마다 변경할 설정\n",
        "        image_size = 224 + (epoch // 20) * 32       # image size 증가\n",
        "        dropout_rate = 0.2 + (epoch // 20) * 0.05   # dropout 비율 증가\n",
        "        num_ops = 2 + (epoch // 20) * 2             # RandAugment 매개변수 증가\n",
        "        magnitude = 3 + (epoch // 20) * 2           # RandAugment 매개변수 증가\n",
        "        # mixup_alpha = 0 + (epoch // 20) * 1         # mixup alpha 증가\n",
        "\n",
        "        training_dataloader = get_dataloader(train=True, batch_size=batch_size, image_size=image_size, augment=True, num_ops=num_ops, magnitude=magnitude)\n",
        "        test_dataloader = get_dataloader(train=False, batch_size=batch_size, image_size=image_size)\n",
        "        model_to_train = get_model(dropout_rate=dropout_rate)\n",
        "        optimizer = optim.Adam(model_to_train.parameters(), lr=0.01)\n",
        "\n",
        "    training_losses, test_losses = [], []\n",
        "    training_accuracies, test_accuracies = [], []\n",
        "\n",
        "\n",
        "    # 현재 에포크의 시작 시간 측정\n",
        "    time1 = time.time()\n",
        "\n",
        "    # train 데이터 로더를 통해 배치 단위로 데이터 로딩 및 학습\n",
        "    for data, labels in training_dataloader:\n",
        "        trng_batch_loss = training_batch(data, labels, model_to_train, criterion, optimizer)\n",
        "        training_losses.append(trng_batch_loss)\n",
        "        trng_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        training_accuracies.extend(trng_batch_correct)\n",
        "\n",
        "    # 현재 에포크의 평균 학습 손실 및 정확도 계산\n",
        "    training_per_epoch_loss = np.array(training_losses).mean()\n",
        "    training_per_epoch_accuracy = np.mean(training_accuracies)\n",
        "\n",
        "    # 현재 에포크의 종료 시간 측정\n",
        "    time2 = time.time()\n",
        "    print(\"Respective Execution time : \" + str(time2 - time1) + \"   \" + str(epoch) + \"epochs\")\n",
        "\n",
        "\n",
        "    # test 데이터 로더를 통해 배치 단위로 데이터 로딩 및 평가\n",
        "    for data, labels in test_dataloader:\n",
        "        tst_batch_loss = test_batch(data, labels, model_to_train, criterion)\n",
        "        test_losses.append(tst_batch_loss)\n",
        "        tst_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        test_accuracies.extend(tst_batch_correct)\n",
        "    # 현재 에포크의 평균 테스트 손실 및 정확도 계산\n",
        "    test_per_epoch_loss = np.array(test_losses).mean()\n",
        "    test_per_epoch_accuracy = np.mean(test_accuracies)\n",
        "\n",
        "    # 계산된 평균 손실 및 정확도를 저장\n",
        "    training_loss.append(training_per_epoch_loss)\n",
        "    training_accuracy.append(training_per_epoch_accuracy)\n",
        "    test_loss.append(test_per_epoch_loss)\n",
        "    test_accuracy.append(test_per_epoch_accuracy)\n",
        "\n",
        "    # 현재 에포크의 학습 및 테스트 결과 출력\n",
        "    print(f'Epoch: {epoch + 1}/{n_epochs}\\t| Training loss: {training_per_epoch_loss:.4f} | ', end='')\n",
        "    print(f'Training accuracy: {training_per_epoch_accuracy:.4f} | Test loss: {test_per_epoch_loss:.4f} | ', end='')\n",
        "    print(f'Test accuracy: {test_per_epoch_accuracy:.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXc532igzPh2"
      },
      "source": [
        "# 3주차 과제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRcv5YK6ZaGK"
      },
      "source": [
        "**MULTI TASK LEARNING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swt5fu_tZjmp"
      },
      "source": [
        "## 동진 코드 Data Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGWrlsBQZv30"
      },
      "source": [
        "DATA LOADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP9_kMC7Zf6r",
        "outputId": "0db0e36a-acf4-45cd-afac-d6213bc0e0bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.4.0-py3-none-any.whl (868 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Collecting pretty-errors==1.2.25 (from torchmetrics)\n",
            "  Downloading pretty_errors-1.2.25-py3-none-any.whl (17 kB)\n",
            "Collecting colorama (from pretty-errors==1.2.25->torchmetrics)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, colorama, pretty-errors, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "Successfully installed colorama-0.4.6 lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pretty-errors-1.2.25 torchmetrics-1.4.0\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwumjwAyZ5tQ"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/Splitted_OfficeHomeDataset.zip'\n",
        "\n",
        "extract_path = '/content'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZtmxyq-Z8RW"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import RandomHorizontalFlip, RandomCrop, RandomRotation\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "from torchmetrics.classification import MulticlassAveragePrecision\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.preprocessing import label_binarize, LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byjfvfy2Z9tj"
      },
      "outputs": [],
      "source": [
        "dataset_dict = {\n",
        "    'style_id': {\"Art\": 0, \"Clipart\": 1, \"Product\": 2, \"Real_World\": 3},\n",
        "    'item_id': {\"Alarm_Clock\": 0, \"Backpack\": 1, \"Batteries\": 2, \"Bed\": 3, \"Bike\": 4, \"Bottle\": 5, \"Bucket\": 6, \"Calculator\": 7, \"Calendar\": 8, \"Candles\": 9, \"Chair\": 10, \"Clipboards\": 11, \"Computer\": 12, \"Couch\": 13, \"Curtains\": 14, \"Desk_Lamp\": 15, \"Drill\": 16, \"Eraser\": 17, \"Exit_Sign\": 18, \"Fan\": 19, \"File_Cabinet\": 20, \"Flipflops\": 21, \"Flowers\": 22, \"Folder\": 23, \"Fork\": 24, \"Glasses\": 25, \"Hammer\": 26, \"Helmet\": 27, \"Kettle\": 28, \"Keyboard\": 29, \"Knives\": 30, \"Lamp_Shade\": 31, \"Laptop\": 32, \"Marker\": 33, \"Monitor\": 34, \"Mop\": 35, \"Mouse\": 36, \"Mug\": 37, \"Notebook\": 38, \"Oven\": 39, \"Pan\": 40, \"Paper_Clip\": 41, \"Pen\": 42, \"Pencil\": 43, \"Postit_Notes\": 44, \"Printer\": 45, \"Push_Pin\": 46, \"Radio\": 47, \"Refrigerator\": 48, \"Ruler\": 49, \"Scissors\": 50, \"Screwdriver\": 51, \"Shelf\": 52, \"Sink\": 53, \"Sneakers\": 54, \"Soda\": 55, \"Speaker\": 56, \"Spoon\": 57, \"Table\": 58, \"Telephone\": 59, \"ToothBrush\": 60, \"Toys\": 61, \"Trash_Can\": 62, \"TV\": 63, \"Webcam\": 64}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KgRG-TlZ-CX",
        "outputId": "782e544c-2a7c-46ff-9f20-a8100841d0b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "폴더 안에 있는 파일 개수: 12375\n",
            "폴더 안에 있는 파일 개수: 3213\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def count_jpg_files_in_folder(folder_path):\n",
        "    total_jpg_files = 0\n",
        "\n",
        "    files = os.listdir(folder_path)\n",
        "    for file in files:\n",
        "\n",
        "        file_path = os.path.join(folder_path, file)\n",
        "        if os.path.isfile(file_path):\n",
        "            if file.endswith('.jpg'):\n",
        "                total_jpg_files += 1\n",
        "        # 폴더인 경우 재귀적으로 하위 폴더 탐색\n",
        "        elif os.path.isdir(file_path):\n",
        "            total_jpg_files += count_jpg_files_in_folder(file_path)\n",
        "\n",
        "    return total_jpg_files\n",
        "\n",
        "# 예시: \"data\" 폴더 안에 있는 모든 파일 개수를 반환\n",
        "file_count = count_jpg_files_in_folder(\"/content/train\")\n",
        "print(\"폴더 안에 있는 파일 개수:\", file_count)\n",
        "\n",
        "file_count = count_jpg_files_in_folder(\"/content/test\")\n",
        "print(\"폴더 안에 있는 파일 개수:\", file_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWfOm2NwaBrV"
      },
      "outputs": [],
      "source": [
        "def extract_image(source_path, destination_path):\n",
        "  i = 0\n",
        "  for root, dirs, files in os.walk(source_path):\n",
        "    for file in files:\n",
        "      if file.endswith(('jpg')):\n",
        "        source_file_path = os.path.join(root,file)\n",
        "        label_root = root.split(\"/\")\n",
        "        label_1 = label_root[3]\n",
        "        label_2 = label_root[4]\n",
        "        #print(f\"label_style {label_1} , label_item {label_2}\")\n",
        "        destination_file_name = f\"{label_1}-{label_2}-{i}.jpg\"\n",
        "        destination_file_path = os.path.join(destination_path, destination_file_name)\n",
        "        shutil.copyfile(source_file_path, destination_file_path)\n",
        "        i += 1\n",
        "\n",
        "\n",
        "\n",
        "dataset_path = \"/content/testset/\"\n",
        "os.makedirs(dataset_path, exist_ok = True)\n",
        "extract_image(\"/content/test\", dataset_path)\n",
        "\n",
        "image_path_test = sorted(glob.glob(\"/content/testset/*.jpg\"))\n",
        "\n",
        "dataset_path = \"/content/trainset/\"\n",
        "os.makedirs(dataset_path, exist_ok = True)\n",
        "extract_image(\"/content/train\", dataset_path)\n",
        "\n",
        "image_path_train = sorted(glob.glob(\"/content/trainset/*.jpg\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAx6rMmXaEHr"
      },
      "outputs": [],
      "source": [
        "class Office_Home(Dataset):\n",
        "  def __init__(self, image_path, check):\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    if check == False:\n",
        "      self.transform = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                           transforms.ToTensor(),\n",
        "                                           transforms.Normalize(mean, std)])\n",
        "\n",
        "    else:\n",
        "      self.transform = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                           transforms.ToTensor(),\n",
        "                                           transforms.Normalize(mean, std),\n",
        "                                           RandomRotation(10),\n",
        "                                           RandomHorizontalFlip()])\n",
        "\n",
        "    self.image_path = image_path\n",
        "    self.images = []\n",
        "    self.styles = []\n",
        "    self.items = []\n",
        "\n",
        "\n",
        "    for path in image_path:\n",
        "      temp_path = path.split(\"/\")\n",
        "      temp_filename = temp_path[3]\n",
        "      filename = temp_filename.split(\"-\")\n",
        "\n",
        "      self.images.append(path)\n",
        "      self.styles.append(dataset_dict['style_id'].get(filename[0]))\n",
        "      self.items.append(dataset_dict['item_id'].get(filename[1]))\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # Load an Image\n",
        "    img = Image.open(self.images[index]).convert('RGB')\n",
        "    # Transform it\n",
        "    img = self.transform(img)\n",
        "\n",
        "    # Get the Labels\n",
        "    style = self.styles[index]\n",
        "    item = self.items[index]\n",
        "\n",
        "    if(img == None or style == None or item == None):\n",
        "      print(\"error\")\n",
        "      print(self.images[index])\n",
        "\n",
        "\n",
        "    # Return the sample of the dataset\n",
        "    sample = {'image':img, 'style': style, 'item': item}\n",
        "    #print(f\"style : {style} , item : {item}\") #item이 none 처리가 된다\n",
        "\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auMdPOxWaGci"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "Batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader( Office_Home(image_path_train, check = True) , shuffle = True, batch_size = Batch_size)\n",
        "test_dataloader = DataLoader( Office_Home(image_path_test, check = False), shuffle = False, batch_size = Batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "kUWfbidAaLIa",
        "outputId": "a0c4f8e6-8979-4c30-ee6d-a94499dfa245"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n",
            "62\n"
          ]
        }
      ],
      "source": [
        "## dataloader에 잘 올라가있는 지 확인하는 코드\n",
        "def imshow(img):\n",
        "    # Unnormalize\n",
        "    img = img / 2 + 0.5\n",
        "\t# convert to Numpy\n",
        "    npimg = img.numpy()\n",
        "\t# Transpose to get the correct color\n",
        "    npimg=np.transpose(npimg, (1, 2, 0))\n",
        "    plt.imshow(npimg)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "sample = next(iter(train_dataloader))\n",
        "imshow(sample['image'][0])\n",
        "print(sample['style'][0].item())\n",
        "print(sample['item'][0].item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cnwL12JaUDx"
      },
      "source": [
        "##VERSION 1 Original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBvnFE1ffZUn",
        "outputId": "2b68d78e-82f9-4af4-b0e7-0ecf7d8eea4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "self.feature_dim 2048\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 296MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MultiTaskNet(\n",
            "  (net): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (global_avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (classifier): Identity()\n",
            "    (fc1): Sequential(\n",
            "      (linear): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "      (relu1): ReLU()\n",
            "      (dropout1): Dropout(p=0.5, inplace=False)\n",
            "      (final): Linear(in_features=2048, out_features=4, bias=True)\n",
            "    )\n",
            "    (fc2): Sequential(\n",
            "      (linear): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "      (relu1): ReLU()\n",
            "      (dropout1): Dropout(p=0.5, inplace=False)\n",
            "      (final): Linear(in_features=2048, out_features=65, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "from torchvision.models.resnet import BasicBlock\n",
        "from torch.utils import model_zoo\n",
        "\n",
        "\n",
        "class MixStyle(nn.Module):\n",
        "    \"\"\"MixStyle.\n",
        "\n",
        "    Reference:\n",
        "      Zhou et al. Domain Generalization with MixStyle. ICLR 2021.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5, alpha=0.1, eps=1e-6, mix='random'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          p (float): probability of using MixStyle.\n",
        "          alpha (float): parameter of the Beta distribution.\n",
        "          eps (float): scaling parameter to avoid numerical issues.\n",
        "          mix (str): how to mix.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        self.beta = torch.distributions.Beta(alpha, alpha)\n",
        "        self.eps = eps\n",
        "        self.alpha = alpha\n",
        "        self.mix = mix\n",
        "        self._activated = True\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'MixStyle(p={self.p}, alpha={self.alpha}, eps={self.eps}, mix={self.mix})'\n",
        "\n",
        "    def set_activation_status(self, status=True):\n",
        "        self._activated = status\n",
        "\n",
        "    def update_mix_method(self, mix='random'):\n",
        "        self.mix = mix\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.training or not self._activated:\n",
        "            return x\n",
        "\n",
        "        if random.random() > self.p:\n",
        "            return x\n",
        "\n",
        "        B = x.size(0)\n",
        "\n",
        "        mu = x.mean(dim=[2, 3], keepdim=True)\n",
        "        var = x.var(dim=[2, 3], keepdim=True)\n",
        "        sig = (var + self.eps).sqrt()\n",
        "        mu, sig = mu.detach(), sig.detach()\n",
        "        x_normed = (x-mu) / sig\n",
        "\n",
        "        lmda = self.beta.sample((B, 1, 1, 1))\n",
        "\n",
        "        # for i in range(B):\n",
        "        #   if lmda[i].item() < 0.5:\n",
        "        #     lmda[i] = 1 - lmda[i]\n",
        "\n",
        "        lmda = lmda.to(x.device)\n",
        "\n",
        "        if self.mix == 'random':\n",
        "            # random shuffle\n",
        "            perm = torch.randperm(B)\n",
        "            #print(f'random apply perm{perm}')\n",
        "\n",
        "        elif self.mix == 'crossdomain':\n",
        "            # split into two halves and swap the order\n",
        "            perm = torch.arange(B - 1, -1, -1) # inverse index\n",
        "            perm_b, perm_a = perm.chunk(2)\n",
        "            perm_b = perm_b[torch.randperm(B // 2)]\n",
        "            perm_a = perm_a[torch.randperm(B // 2)]\n",
        "            perm = torch.cat([perm_b, perm_a], 0)\n",
        "            #print(f'pseudo crossdomain apply perm{perm}')\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        mu2, sig2 = mu[perm], sig[perm]\n",
        "        mu_mix = mu*lmda + mu2 * (1-lmda)\n",
        "        sig_mix = sig*lmda + sig2 * (1-lmda)\n",
        "\n",
        "        return x_normed*sig_mix + mu_mix\n",
        "\n",
        "\n",
        "from __future__ import division, absolute_import\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    'resnet18',\n",
        "    'resnet50'\n",
        "]\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18':\n",
        "    'https://download.pytorch.org/models/resnet18-f37072fd.pth',\n",
        "    'resnet50':\n",
        "    'https://download.pytorch.org/models/resnet50-11ad3fa6.pth'\n",
        "}\n",
        "\n",
        "'''\n",
        "Reference : https://pytorch.org/vision/main/_modules/torchvision/models/resnet.html\n",
        "'''\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation\n",
        "    )\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes, out_planes, kernel_size=1, stride=stride, bias=False\n",
        "    )\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride=1,\n",
        "        downsample=None,\n",
        "        groups=1,\n",
        "        base_width=64,\n",
        "        dilation=1,\n",
        "        norm_layer=None\n",
        "    ):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError(\n",
        "                'BasicBlock only supports groups=1 and base_width=64'\n",
        "            )\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\n",
        "                \"Dilation > 1 not supported in BasicBlock\"\n",
        "            )\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\" https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride = 1,\n",
        "        downsample = None,\n",
        "        groups = 1,\n",
        "        base_width = 64,\n",
        "        dilation = 1,\n",
        "        norm_layer = None,\n",
        "    ):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.0)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        loss,\n",
        "        block,\n",
        "        layers,\n",
        "        zero_init_residual=False,\n",
        "        groups=1,\n",
        "        width_per_group=64,\n",
        "        replace_stride_with_dilation=None,\n",
        "        norm_layer=None,\n",
        "        last_stride=2,\n",
        "        fc_dims=None,\n",
        "        dropout_p=None,\n",
        "        mixstyle_layers=[],\n",
        "        mixstyle_p=0.5,\n",
        "        mixstyle_alpha=0.1,\n",
        "    ):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "        self.loss = loss\n",
        "        self.feature_dim = 512 * block.expansion\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\n",
        "                \"replace_stride_with_dilation should be None \"\n",
        "                \"or a 3-element tuple, got {}\".\n",
        "                format(replace_stride_with_dilation)\n",
        "            )\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False\n",
        "        )\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(\n",
        "            block,\n",
        "            128,\n",
        "            layers[1],\n",
        "            stride=2,\n",
        "            dilate=replace_stride_with_dilation[0]\n",
        "        )\n",
        "        self.layer3 = self._make_layer(\n",
        "            block,\n",
        "            256,\n",
        "            layers[2],\n",
        "            stride=2,\n",
        "            dilate=replace_stride_with_dilation[1]\n",
        "        )\n",
        "        self.layer4 = self._make_layer(\n",
        "            block,\n",
        "            512,\n",
        "            layers[3],\n",
        "            stride=last_stride,\n",
        "            dilate=replace_stride_with_dilation[2]\n",
        "        )\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = self._construct_fc_layer(\n",
        "            fc_dims, 512 * block.expansion, dropout_p\n",
        "        )\n",
        "        self.classifier = nn.Linear(self.feature_dim, num_classes)\n",
        "        print(f\"self.feature_dim {self.feature_dim}\")\n",
        "\n",
        "        self.mixstyle = None\n",
        "        if mixstyle_layers:\n",
        "            self.mixstyle = MixStyle(p=mixstyle_p, alpha=mixstyle_alpha, mix='random')\n",
        "            print('Insert MixStyle after the following layers: {}'.format(mixstyle_layers))\n",
        "        self.mixstyle_layers = mixstyle_layers\n",
        "\n",
        "        self._init_params()\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "              if isinstance(m, BasicBlock):\n",
        "                nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes, planes, stride, downsample, self.groups,\n",
        "                self.base_width, previous_dilation, norm_layer\n",
        "            )\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _construct_fc_layer(self, fc_dims, input_dim, dropout_p=None):\n",
        "\n",
        "        if fc_dims is None:\n",
        "            self.feature_dim = input_dim\n",
        "            return None\n",
        "\n",
        "        assert isinstance(\n",
        "            fc_dims, (list, tuple)\n",
        "        ), 'fc_dims must be either list or tuple, but got {}'.format(\n",
        "            type(fc_dims)\n",
        "        )\n",
        "\n",
        "        layers = []\n",
        "        for dim in fc_dims:\n",
        "            layers.append(nn.Linear(input_dim, dim))\n",
        "            layers.append(nn.BatchNorm1d(dim))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            if dropout_p is not None:\n",
        "                layers.append(nn.Dropout(p=dropout_p))\n",
        "            input_dim = dim\n",
        "\n",
        "        self.feature_dim = fc_dims[-1]\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _init_params(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(\n",
        "                    m.weight, mode='fan_out', nonlinearity='relu'\n",
        "                )\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def featuremaps(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        if 'layer1' in self.mixstyle_layers:\n",
        "            x = self.mixstyle(x)\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        if 'layer2' in self.mixstyle_layers:\n",
        "            x = self.mixstyle(x)\n",
        "\n",
        "        x = self.layer3(x)\n",
        "        if 'layer3' in self.mixstyle_layers:\n",
        "            x = self.mixstyle(x)\n",
        "\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.featuremaps(x)\n",
        "        v = self.global_avgpool(f)\n",
        "        v = v.view(v.size(0), -1)\n",
        "\n",
        "        if self.fc is not None:\n",
        "            v = self.fc(v)\n",
        "\n",
        "        if not self.training:\n",
        "            return v\n",
        "\n",
        "        y = self.classifier(v)\n",
        "\n",
        "        if self.loss == 'softmax':\n",
        "            return y\n",
        "        else:\n",
        "            raise KeyError(\"Unsupported loss: {}\".format(self.loss))\n",
        "\n",
        "class MultiTaskNet(nn.Module):\n",
        "    def __init__(self, net, backbone,num_classes_style = 4, num_classes_item = 65, pretrained=True):\n",
        "        super(MultiTaskNet, self).__init__()\n",
        "        self.net = net\n",
        "        self.net.classifier = nn.Identity()\n",
        "        if backbone == \"resnet18\":\n",
        "          self.net.fc1 = nn.Sequential(OrderedDict([('linear', nn.Linear(512,512)),('relu1', nn.ReLU()),('dropout1', nn.Dropout(0.5)),('final', nn.Linear(512, 4))]))\n",
        "          self.net.fc2 = nn.Sequential(OrderedDict([('linear', nn.Linear(512,512)),('relu1', nn.ReLU()),('dropout1', nn.Dropout(0.5)),('final', nn.Linear(512, 65))]))\n",
        "        elif backbone == \"resnet50\":\n",
        "          self.net.fc1 = nn.Sequential(OrderedDict([('linear', nn.Linear(2048,2048)),('relu1', nn.ReLU()),('dropout1', nn.Dropout(0.5)),('final', nn.Linear(2048, 4))]))\n",
        "          self.net.fc2 = nn.Sequential(OrderedDict([('linear', nn.Linear(2048,2048)),('relu1', nn.ReLU()),('dropout1', nn.Dropout(0.5)),('final', nn.Linear(2048, 65))]))\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        style_head = self.net.fc1(x)\n",
        "        item_head = self.net.fc2(x)\n",
        "\n",
        "        return style_head, item_head\n",
        "\n",
        "def init_pretrained_weights(model, model_url):\n",
        "    \"\"\"Initializes model with pretrained weights.\n",
        "\n",
        "    Layers that don't match with pretrained layers in name or size are kept unchanged.\n",
        "    \"\"\"\n",
        "    pretrain_dict = model_zoo.load_url(model_url)\n",
        "    model_dict = model.state_dict()\n",
        "    pretrain_dict = {\n",
        "        k: v\n",
        "        for k, v in pretrain_dict.items()\n",
        "        if k in model_dict and model_dict[k].size() == v.size()\n",
        "    }\n",
        "    model_dict.update(pretrain_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "\n",
        "\n",
        "def resnet18(num_classes=1000, loss='softmax', pretrained=True, **kwargs):\n",
        "    model = ResNet(\n",
        "        num_classes=num_classes,\n",
        "        loss=loss,\n",
        "        block=BasicBlock,\n",
        "        layers=[2, 2, 2, 2],\n",
        "        last_stride=2,\n",
        "        fc_dims=None,\n",
        "        dropout_p=None,\n",
        "        mixstyle_layers=[],#'layer1', 'layer2', 'layer3'\n",
        "        mixstyle_alpha=0.1,\n",
        "        **kwargs\n",
        "    )\n",
        "    if pretrained:\n",
        "        init_pretrained_weights(model, model_urls['resnet18'])\n",
        "    return model\n",
        "\n",
        "def resnet50(num_classes=1000, loss='softmax', pretrained=True, **kwargs):\n",
        "    model = ResNet(\n",
        "        num_classes=num_classes,\n",
        "        loss=loss,\n",
        "        block=Bottleneck,\n",
        "        layers=[3, 4, 6, 3],\n",
        "        last_stride=2,\n",
        "        fc_dims=None,\n",
        "        dropout_p=None,\n",
        "        mixstyle_layers=[],#'layer1', 'layer2'\n",
        "        mixstyle_alpha=0.1,\n",
        "        **kwargs\n",
        "    )\n",
        "    if pretrained:\n",
        "        init_pretrained_weights(model, model_urls['resnet50'])\n",
        "    return model\n",
        "\n",
        "backbone = [\"resnet18\" , \"resnet50\"]\n",
        "model_pretrained_ = resnet50()\n",
        "model_mixstyle_ = MultiTaskNet(model_pretrained_, backbone = \"resnet50\")\n",
        "model_to_train = model_mixstyle_.to(device)\n",
        "\n",
        "print(model_mixstyle_)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model_to_train.parameters(), lr = 0.0004, weight_decay = 0.001)\n",
        "\n",
        "def training_batch(train_loader, model,criterion, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    training_loss = []\n",
        "    training_style_loss = []\n",
        "    training_item_loss = []\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "      input = data[\"image\"].to(device)\n",
        "      style_label = data[\"style\"].to(device)\n",
        "      item_label = data[\"item\"].to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      style_output, item_output = model(input)\n",
        "\n",
        "      loss_1 = criterion(style_output, style_label)\n",
        "      loss_2 = criterion(item_output, item_label)\n",
        "\n",
        "      loss = loss_1 + loss_2\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      training_loss.append(loss.detach().item())\n",
        "      training_style_loss.append(loss_1.detach().item())\n",
        "      training_item_loss.append(loss_2.detach().item())\n",
        "\n",
        "\n",
        "    return training_style_loss, training_item_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHGgL-1affMj"
      },
      "outputs": [],
      "source": [
        "def test_batch(data, model, criterion):\n",
        "    model.eval()\n",
        "    input = data[\"image\"].to(device)\n",
        "    style_label = data[\"style\"].to(device)\n",
        "    item_label = data[\"item\"].to(device)\n",
        "\n",
        "    style_output, item_output = model(input)\n",
        "    loss_1 = criterion(style_output, style_label)\n",
        "    loss_2 = criterion(item_output, item_label)\n",
        "\n",
        "    total_loss = loss_1 + loss_2\n",
        "    return  loss_1.item(), loss_2.item()\n",
        "\n",
        "def accuracy(data, model):\n",
        "    model.eval()\n",
        "\n",
        "    input = data[\"image\"].to(device)\n",
        "    style_label = data[\"style\"].to(device)\n",
        "    item_label = data[\"item\"].to(device)\n",
        "\n",
        "    style_output, item_output = model(input)\n",
        "\n",
        "    _, pred_label_style = style_output.max(-1)\n",
        "    _, pred_label_item = item_output.max(-1)\n",
        "\n",
        "    correct_style = (pred_label_style == style_label)\n",
        "    correct_item = (pred_label_item == item_label)\n",
        "\n",
        "    return correct_style.cpu().detach().numpy().tolist(), correct_item.cpu().detach().numpy().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-HtemXHfoFt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchmetrics.classification import MulticlassAveragePrecision\n",
        "\n",
        "def classification_map(data_loader, model):\n",
        "    # MulticlassAveragePrecision 메트릭스 초기화\n",
        "    map_metric_style = MulticlassAveragePrecision(num_classes=4, average=\"macro\")\n",
        "    map_metric_item = MulticlassAveragePrecision(num_classes=65, average=\"macro\")\n",
        "\n",
        "    device = next(model.parameters()).device  # 모델의 디바이스 확인\n",
        "\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "\n",
        "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
        "        for data in data_loader:\n",
        "            input = data[\"image\"].to(device)\n",
        "            style_true = data[\"style\"].to(device)\n",
        "            item_true = data[\"item\"].to(device)\n",
        "\n",
        "            style_pred, item_pred = model(input)\n",
        "\n",
        "            # MulticlassAveragePrecision 클래스는 확률을 입력으로 받으므로 softmax를 적용하여 확률로 변환\n",
        "            style_pred_prob = torch.softmax(style_pred, dim=1)\n",
        "            item_pred_prob = torch.softmax(item_pred, dim=1)\n",
        "\n",
        "            # 메트릭스 업데이트\n",
        "            map_metric_style.update(style_pred_prob, style_true)\n",
        "            map_metric_item.update(item_pred_prob, item_true)\n",
        "\n",
        "    # 평균 정밀도 계산\n",
        "    mAP_style = map_metric_style.compute()\n",
        "    mAP_item = map_metric_item.compute()\n",
        "\n",
        "    return mAP_style, mAP_item\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_IZSTR9Tfqd4",
        "outputId": "57fe5960-09de-4012-8103-016379bf281f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Respective Execution time : 538.563346862793sec    1 epochs\n",
            "Epoch: 1/20\t| Training loss style: 0.6676 | Training loss item : 1.6600\n",
            " Training accuracy style: 0.8014 |Training accuracy item: 0.7986 Test loss style: 0.5831 | Test loss item: 0.9766 |  Test accuracy style: 0.7638 Test accuracy item: 0.7404\n",
            "Test Mean Average Precision (Style): 0.8377708792686462 | Test Mean Average Precision (Item): 0.831500232219696\n",
            "\n",
            "Respective Execution time : 547.4498507976532sec    2 epochs\n",
            "Epoch: 2/20\t| Training loss style: 0.4519 | Training loss item : 0.7173\n",
            " Training accuracy style: 0.8587 |Training accuracy item: 0.8646 Test loss style: 0.5204 | Test loss item: 0.8213 |  Test accuracy style: 0.7983 Test accuracy item: 0.7831\n",
            "Test Mean Average Precision (Style): 0.8657757043838501 | Test Mean Average Precision (Item): 0.860031008720398\n",
            "\n",
            "Respective Execution time : 548.7480638027191sec    3 epochs\n",
            "Epoch: 3/20\t| Training loss style: 0.3388 | Training loss item : 0.4831\n",
            " Training accuracy style: 0.8959 |Training accuracy item: 0.9103 Test loss style: 0.4747 | Test loss item: 0.7461 |  Test accuracy style: 0.8288 Test accuracy item: 0.8042\n",
            "Test Mean Average Precision (Style): 0.8805463314056396 | Test Mean Average Precision (Item): 0.8782650232315063\n",
            "\n",
            "Respective Execution time : 547.0363187789917sec    4 epochs\n",
            "Epoch: 4/20\t| Training loss style: 0.2869 | Training loss item : 0.3371\n",
            " Training accuracy style: 0.9202 |Training accuracy item: 0.9340 Test loss style: 0.4844 | Test loss item: 0.7738 |  Test accuracy style: 0.8291 Test accuracy item: 0.7924\n",
            "Test Mean Average Precision (Style): 0.8885965347290039 | Test Mean Average Precision (Item): 0.8737553954124451\n",
            "\n",
            "Respective Execution time : 554.2803266048431sec    5 epochs\n",
            "Epoch: 5/20\t| Training loss style: 0.2323 | Training loss item : 0.2844\n",
            " Training accuracy style: 0.9307 |Training accuracy item: 0.9427 Test loss style: 0.4773 | Test loss item: 0.7644 |  Test accuracy style: 0.8254 Test accuracy item: 0.8058\n",
            "Test Mean Average Precision (Style): 0.8879024982452393 | Test Mean Average Precision (Item): 0.8829308748245239\n",
            "\n",
            "Respective Execution time : 546.0549352169037sec    6 epochs\n",
            "Epoch: 6/20\t| Training loss style: 0.1811 | Training loss item : 0.2032\n",
            " Training accuracy style: 0.9201 |Training accuracy item: 0.9560 Test loss style: 0.6077 | Test loss item: 0.7753 |  Test accuracy style: 0.8120 Test accuracy item: 0.8061\n",
            "Test Mean Average Precision (Style): 0.8719944357872009 | Test Mean Average Precision (Item): 0.880566418170929\n",
            "\n",
            "Respective Execution time : 546.2460429668427sec    7 epochs\n",
            "Epoch: 7/20\t| Training loss style: 0.1560 | Training loss item : 0.1865\n",
            " Training accuracy style: 0.9320 |Training accuracy item: 0.9584 Test loss style: 0.5295 | Test loss item: 0.8413 |  Test accuracy style: 0.8251 Test accuracy item: 0.7955\n",
            "Test Mean Average Precision (Style): 0.889452338218689 | Test Mean Average Precision (Item): 0.8734295964241028\n",
            "\n",
            "Respective Execution time : 548.489996433258sec    8 epochs\n",
            "Epoch: 8/20\t| Training loss style: 0.0686 | Training loss item : 0.0802\n",
            " Training accuracy style: 0.9951 |Training accuracy item: 0.9936 Test loss style: 0.4438 | Test loss item: 0.6546 |  Test accuracy style: 0.8774 Test accuracy item: 0.8512\n",
            "Test Mean Average Precision (Style): 0.9243935942649841 | Test Mean Average Precision (Item): 0.906900942325592\n",
            "\n",
            "Respective Execution time : 546.9589262008667sec    9 epochs\n",
            "Epoch: 9/20\t| Training loss style: 0.0326 | Training loss item : 0.0427\n",
            " Training accuracy style: 0.9956 |Training accuracy item: 0.9918 Test loss style: 0.5417 | Test loss item: 0.6893 |  Test accuracy style: 0.8727 Test accuracy item: 0.8503\n",
            "Test Mean Average Precision (Style): 0.9180480241775513 | Test Mean Average Precision (Item): 0.907755434513092\n",
            "\n",
            "Respective Execution time : 545.5405385494232sec    10 epochs\n",
            "Epoch: 10/20\t| Training loss style: 0.0205 | Training loss item : 0.0332\n",
            " Training accuracy style: 0.9975 |Training accuracy item: 0.9927 Test loss style: 0.5595 | Test loss item: 0.7040 |  Test accuracy style: 0.8783 Test accuracy item: 0.8556\n",
            "Test Mean Average Precision (Style): 0.9201735258102417 | Test Mean Average Precision (Item): 0.9103093147277832\n",
            "\n",
            "Respective Execution time : 544.8390214443207sec    11 epochs\n",
            "Epoch: 11/20\t| Training loss style: 0.0186 | Training loss item : 0.0304\n",
            " Training accuracy style: 0.9982 |Training accuracy item: 0.9931 Test loss style: 0.5620 | Test loss item: 0.7127 |  Test accuracy style: 0.8746 Test accuracy item: 0.8509\n",
            "Test Mean Average Precision (Style): 0.9234463572502136 | Test Mean Average Precision (Item): 0.9083907604217529\n",
            "\n",
            "Respective Execution time : 541.9195365905762sec    12 epochs\n",
            "Epoch: 12/20\t| Training loss style: 0.0151 | Training loss item : 0.0292\n",
            " Training accuracy style: 0.9983 |Training accuracy item: 0.9940 Test loss style: 0.6200 | Test loss item: 0.6831 |  Test accuracy style: 0.8755 Test accuracy item: 0.8553\n",
            "Test Mean Average Precision (Style): 0.9222151637077332 | Test Mean Average Precision (Item): 0.9127035140991211\n",
            "\n",
            "Respective Execution time : 545.2460017204285sec    13 epochs\n",
            "Epoch: 13/20\t| Training loss style: 0.0165 | Training loss item : 0.0259\n",
            " Training accuracy style: 0.9972 |Training accuracy item: 0.9934 Test loss style: 0.5967 | Test loss item: 0.6922 |  Test accuracy style: 0.8814 Test accuracy item: 0.8553\n",
            "Test Mean Average Precision (Style): 0.9213570356369019 | Test Mean Average Precision (Item): 0.9113077521324158\n",
            "\n",
            "Respective Execution time : 507.33251786231995sec    14 epochs\n",
            "Epoch: 14/20\t| Training loss style: 0.0190 | Training loss item : 0.0262\n",
            " Training accuracy style: 0.9969 |Training accuracy item: 0.9930 Test loss style: 0.6600 | Test loss item: 0.7245 |  Test accuracy style: 0.8715 Test accuracy item: 0.8547\n",
            "Test Mean Average Precision (Style): 0.9152470827102661 | Test Mean Average Precision (Item): 0.9072187542915344\n",
            "\n",
            "Respective Execution time : 504.614129781723sec    15 epochs\n",
            "Epoch: 15/20\t| Training loss style: 0.0106 | Training loss item : 0.0220\n",
            " Training accuracy style: 0.9990 |Training accuracy item: 0.9943 Test loss style: 0.6113 | Test loss item: 0.7020 |  Test accuracy style: 0.8718 Test accuracy item: 0.8534\n",
            "Test Mean Average Precision (Style): 0.9223878383636475 | Test Mean Average Precision (Item): 0.9112893342971802\n",
            "\n",
            "Respective Execution time : 503.2637689113617sec    16 epochs\n",
            "Epoch: 16/20\t| Training loss style: 0.0065 | Training loss item : 0.0188\n",
            " Training accuracy style: 0.9990 |Training accuracy item: 0.9942 Test loss style: 0.6482 | Test loss item: 0.7097 |  Test accuracy style: 0.8777 Test accuracy item: 0.8547\n",
            "Test Mean Average Precision (Style): 0.9219834804534912 | Test Mean Average Precision (Item): 0.9117889404296875\n",
            "\n",
            "Respective Execution time : 502.9251947402954sec    17 epochs\n",
            "Epoch: 17/20\t| Training loss style: 0.0065 | Training loss item : 0.0166\n",
            " Training accuracy style: 0.9990 |Training accuracy item: 0.9942 Test loss style: 0.6403 | Test loss item: 0.7205 |  Test accuracy style: 0.8777 Test accuracy item: 0.8553\n",
            "Test Mean Average Precision (Style): 0.9223483204841614 | Test Mean Average Precision (Item): 0.911687433719635\n",
            "\n",
            "Respective Execution time : 502.12090587615967sec    18 epochs\n",
            "Epoch: 18/20\t| Training loss style: 0.0061 | Training loss item : 0.0158\n",
            " Training accuracy style: 0.9991 |Training accuracy item: 0.9945 Test loss style: 0.6678 | Test loss item: 0.7166 |  Test accuracy style: 0.8771 Test accuracy item: 0.8562\n",
            "Test Mean Average Precision (Style): 0.9238031506538391 | Test Mean Average Precision (Item): 0.9127238392829895\n",
            "\n",
            "Respective Execution time : 508.06607818603516sec    19 epochs\n",
            "Epoch: 19/20\t| Training loss style: 0.0052 | Training loss item : 0.0161\n",
            " Training accuracy style: 0.9989 |Training accuracy item: 0.9941 Test loss style: 0.6441 | Test loss item: 0.7246 |  Test accuracy style: 0.8774 Test accuracy item: 0.8621\n",
            "Test Mean Average Precision (Style): 0.9238873720169067 | Test Mean Average Precision (Item): 0.9129365086555481\n",
            "\n",
            "Respective Execution time : 508.0702700614929sec    20 epochs\n",
            "Epoch: 20/20\t| Training loss style: 0.0051 | Training loss item : 0.0154\n",
            " Training accuracy style: 0.9992 |Training accuracy item: 0.9941 Test loss style: 0.6683 | Test loss item: 0.7395 |  Test accuracy style: 0.8771 Test accuracy item: 0.8537\n",
            "Test Mean Average Precision (Style): 0.924629807472229 | Test Mean Average Precision (Item): 0.9126977920532227\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nmean_precision_style = np.mean(all_precisions_style)\\nmean_recall_style = np.mean(all_recalls_style)\\nmean_precision_item =  np.mean(all_precisions_item)\\nmean_recall_item = np.mean(all_recalls_item)\\n\\n  # 평균 성능 결과 출력\\nprint(\"Mean Precision Style:\", mean_precision_style)\\nprint(\"Mean Recall Style:\", mean_recall_style)\\nprint(\"Mean Precision Style:\", mean_precision_item)\\nprint(\"Mean Recall Style:\", mean_recall_item)\\n'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "Actual Training\n",
        "'''\n",
        "\n",
        "n_epochs = 20\n",
        "scheduler = StepLR(optimizer, step_size=7, gamma=0.3)\n",
        "\n",
        "train_loss_style, test_loss_style = [], []\n",
        "train_loss_item, test_loss_item = [], []\n",
        "train_accuracy_style, test_accuracy_style = [], []\n",
        "train_accuracy_item, test_accuracy_item = [], []\n",
        "all_precisions_style = []\n",
        "all_recalls_style = []\n",
        "all_precisions_item = []\n",
        "all_recalls_item = []\n",
        "\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_losses_style, train_losses_item = [], []\n",
        "    test_losses_style,test_losses_item = [],[]\n",
        "\n",
        "    train_accuracies_style, train_accuracies_item = [],[]\n",
        "    test_accuracies_style, test_accuracies_item = [], []\n",
        "\n",
        "\n",
        "    time1 = time.time()\n",
        "    train_losses_style, train_losses_item = training_batch(train_dataloader, model_to_train, criterion, optimizer)\n",
        "\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        train_batch_correct_style, train_batch_correct_item = accuracy(data, model_to_train)\n",
        "        train_accuracies_style.extend(train_batch_correct_style)\n",
        "        train_accuracies_item.extend(train_batch_correct_item)\n",
        "\n",
        "\n",
        "    train_per_epoch_loss_style = np.array(train_losses_style).mean()\n",
        "    train_per_epoch_loss_item = np.array(train_losses_item).mean()\n",
        "    train_per_epoch_accuracy_style = np.mean(train_accuracies_style)\n",
        "    train_per_epoch_accuracy_item = np.mean(train_accuracies_item)\n",
        "\n",
        "    #train_mAP_style, train_mAP_item = mean_average_precision(train_dataloader, model_to_train)\n",
        "    #print(\"Test Mean Average Precision (Style):\", train_mAP_style)\n",
        "    #print(\"Test Mean Average Precision (Item):\", train_mAP_item)\n",
        "\n",
        "    time2 = time.time()\n",
        "    print(\"Respective Execution time : \" + str(time2 - time1) + \"sec    \" + str(epoch+1) + \" epochs\")\n",
        "\n",
        "    learning_rate = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #   for i, data in enumerate(val_dataloader):\n",
        "    #       val_batch_correct_style,val_batch_correct_item = accuracy(data, model_to_train)\n",
        "    #       val_accuracies_style.extend(val_batch_correct_style)\n",
        "    #       val_accuracies_item.extend(val_batch_correct_item)\n",
        "\n",
        "    #   val_per_epoch_accuracy_style = np.mean(val_accuracies_style)\n",
        "    #   val_per_epoch_accuracy_item = np.mean(val_accuracies_item)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for i, data in enumerate(test_dataloader):\n",
        "        test_batch_loss_style, test_batch_loss_item = test_batch(data, model_to_train, criterion)\n",
        "        test_losses_style.append(test_batch_loss_style)\n",
        "        test_losses_item.append(test_batch_loss_item)\n",
        "\n",
        "        test_batch_correct_style, test_batch_correct_item = accuracy(data , model_to_train)\n",
        "        test_accuracies_style.extend(test_batch_correct_style)\n",
        "        test_accuracies_item.extend(test_batch_correct_item)\n",
        "\n",
        "    test_mAP_style, test_mAP_item  = classification_map(test_dataloader, model_to_train)\n",
        "    #average_precision_style, average_recall_style, average_precision_item, average_recall_item = test_precision_recall( model_to_train, test_dataloader)\n",
        "\n",
        "    #all_precisions_style.append(average_precision_style)\n",
        "    #all_recalls_style.append(average_recall_style)\n",
        "\n",
        "    #all_precisions_item.append(average_precision_item)\n",
        "    #all_recalls_item.append(average_recall_item)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    test_per_epoch_loss_style = np.array(test_losses_style).mean()\n",
        "    test_per_epoch_loss_item = np.array(test_losses_item).mean()\n",
        "    test_per_epoch_accuracy_style = np.mean(test_accuracies_style)\n",
        "    test_per_epoch_accuracy_item = np.mean(test_accuracies_item)\n",
        "\n",
        "\n",
        "\n",
        "    # train, test\n",
        "    # train_loss_style.append(train_per_epoch_loss_style)\n",
        "    # train_accuracy_style.append(train_per_epoch_accuracy_style)\n",
        "\n",
        "    # train_loss_item.append(train_per_epoch_loss_item)\n",
        "    # train_accuracy_item.append(train_per_epoch_accuracy_item)\n",
        "\n",
        "    # test_loss_style.append(test_per_epoch_loss_style)\n",
        "    # test_accuracy_style.append(test_per_epoch_accuracy_style)\n",
        "\n",
        "    # test_loss_item.append(test_per_epoch_loss_item)\n",
        "    # test_accuracy_item.append(test_per_epoch_accuracy_item)\n",
        "\n",
        "\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}/{n_epochs}\\t| Training loss style: {train_per_epoch_loss_style:.4f} | Training loss item : {train_per_epoch_loss_item:.4f}')\n",
        "    print(f' Training accuracy style: {train_per_epoch_accuracy_style:.4f} |Training accuracy item: {train_per_epoch_accuracy_item:.4f} ', end='')\n",
        "    print(f'Test loss style: {test_per_epoch_loss_style:.4f} | Test loss item: {test_per_epoch_loss_item:.4f} |  Test accuracy style: {test_per_epoch_accuracy_style:.4f} Test accuracy item: {test_per_epoch_accuracy_item:.4f}')\n",
        "    print(f\"Test Mean Average Precision (Style): {test_mAP_style} | Test Mean Average Precision (Item): {test_mAP_item}\" + '\\n')\n",
        "    #print(f\"Test Mean Average Precision (Style): {average_precision_style} | Test Mean Average Precision (Item): {average_precision_item}\")\n",
        "    #print(f\"Test Mean Average Recall (Style): {average_recall_style} | Test Mean Average Recall (Item): {average_recall_item}\")\n",
        "\n",
        "\n",
        "'''\n",
        "mean_precision_style = np.mean(all_precisions_style)\n",
        "mean_recall_style = np.mean(all_recalls_style)\n",
        "mean_precision_item =  np.mean(all_precisions_item)\n",
        "mean_recall_item = np.mean(all_recalls_item)\n",
        "\n",
        "  # 평균 성능 결과 출력\n",
        "print(\"Mean Precision Style:\", mean_precision_style)\n",
        "print(\"Mean Recall Style:\", mean_recall_style)\n",
        "print(\"Mean Precision Style:\", mean_precision_item)\n",
        "print(\"Mean Recall Style:\", mean_recall_item)\n",
        "'''\n",
        "\n",
        "    #print(\"Test Mean Average Precision (Style):\", test_mAP_style)\n",
        "    #print(\"Test Mean Average Precision (Item):\", test_mAP_item)\n",
        "    # print(f'Test loss style: {test_per_epoch_loss_style:.4f} | Test loss item: {test_per_epoch_loss_item:.4f} |  Test accuracy style: {test_per_epoch_accuracy_style:.4f} Test accuracy item: {test_per_epoch_accuracy_item:.4f}' + '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDbZpzVhfQjs"
      },
      "source": [
        "## VERSION 2 Modify Sampling function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l60_AkkYf2w3"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "from torchvision.models.resnet import BasicBlock\n",
        "from torch.utils import model_zoo\n",
        "\n",
        "\n",
        "class MixStyle(nn.Module):\n",
        "    \"\"\"MixStyle.\n",
        "\n",
        "    Reference:\n",
        "      Zhou et al. Domain Generalization with MixStyle. ICLR 2021.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5, alpha=0.1, eps=1e-6, mix='random'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          p (float): probability of using MixStyle.\n",
        "          alpha (float): parameter of the Beta distribution.\n",
        "          eps (float): scaling parameter to avoid numerical issues.\n",
        "          mix (str): how to mix.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        self.beta = torch.distributions.Beta(alpha, alpha)\n",
        "        self.eps = eps\n",
        "        self.alpha = alpha\n",
        "        self.mix = mix\n",
        "        self._activated = True\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'MixStyle(p={self.p}, alpha={self.alpha}, eps={self.eps}, mix={self.mix})'\n",
        "\n",
        "    def set_activation_status(self, status=True):\n",
        "        self._activated = status\n",
        "\n",
        "    def update_mix_method(self, mix='random'):\n",
        "        self.mix = mix\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.training or not self._activated:\n",
        "            return x\n",
        "\n",
        "        if random.random() > self.p:\n",
        "            return x\n",
        "\n",
        "        B = x.size(0)\n",
        "\n",
        "        mu = x.mean(dim=[2, 3], keepdim=True)\n",
        "        var = x.var(dim=[2, 3], keepdim=True)\n",
        "        sig = (var + self.eps).sqrt()\n",
        "        mu, sig = mu.detach(), sig.detach()\n",
        "        x_normed = (x-mu) / sig\n",
        "\n",
        "        lmda = self.beta.sample((B, 1, 1, 1))\n",
        "\n",
        "        for i in range(B):\n",
        "            if lmda[i].item() < 0.5:\n",
        "                lmda[i] = 1 - lmda[i]\n",
        "\n",
        "        lmda = lmda.to(x.device)\n",
        "\n",
        "        if self.mix == 'random':\n",
        "            # random shuffle\n",
        "            perm = torch.randperm(B)\n",
        "            #print(f'random apply perm{perm}')\n",
        "\n",
        "        elif self.mix == 'crossdomain':\n",
        "            # split into two halves and swap the order\n",
        "            perm = torch.arange(B - 1, -1, -1) # inverse index\n",
        "            perm_b, perm_a = perm.chunk(2)\n",
        "            perm_b = perm_b[torch.randperm(B // 2)]\n",
        "            perm_a = perm_a[torch.randperm(B // 2)]\n",
        "            perm = torch.cat([perm_b, perm_a], 0)\n",
        "            #print(f'pseudo crossdomain apply perm{perm}')\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        mu2, sig2 = mu[perm], sig[perm]\n",
        "        mu_mix = mu*lmda + mu2 * (1-lmda)\n",
        "        sig_mix = sig*lmda + sig2 * (1-lmda)\n",
        "\n",
        "        return x_normed*sig_mix + mu_mix\n",
        "\n",
        "\n",
        "from __future__ import division, absolute_import\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    'resnet18',\n",
        "    'resnet50'\n",
        "]\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18':\n",
        "    'https://download.pytorch.org/models/resnet18-f37072fd.pth',\n",
        "    'resnet50':\n",
        "    'https://download.pytorch.org/models/resnet50-11ad3fa6.pth'\n",
        "}\n",
        "\n",
        "'''\n",
        "Reference : https://pytorch.org/vision/main/_modules/torchvision/models/resnet.html\n",
        "'''\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation\n",
        "    )\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes, out_planes, kernel_size=1, stride=stride, bias=False\n",
        "    )\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride=1,\n",
        "        downsample=None,\n",
        "        groups=1,\n",
        "        base_width=64,\n",
        "        dilation=1,\n",
        "        norm_layer=None\n",
        "    ):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError(\n",
        "                'BasicBlock only supports groups=1 and base_width=64'\n",
        "            )\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\n",
        "                \"Dilation > 1 not supported in BasicBlock\"\n",
        "            )\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\" https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride = 1,\n",
        "        downsample = None,\n",
        "        groups = 1,\n",
        "        base_width = 64,\n",
        "        dilation = 1,\n",
        "        norm_layer = None,\n",
        "    ):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.0)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        loss,\n",
        "        block,\n",
        "        layers,\n",
        "        zero_init_residual=False,\n",
        "        groups=1,\n",
        "        width_per_group=64,\n",
        "        replace_stride_with_dilation=None,\n",
        "        norm_layer=None,\n",
        "        last_stride=2,\n",
        "        fc_dims=None,\n",
        "        dropout_p=None,\n",
        "        mixstyle_layers=[],\n",
        "        mixstyle_p=0.5,\n",
        "        mixstyle_alpha=0.1,\n",
        "    ):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "        self.loss = loss\n",
        "        self.feature_dim = 512 * block.expansion\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\n",
        "                \"replace_stride_with_dilation should be None \"\n",
        "                \"or a 3-element tuple, got {}\".\n",
        "                format(replace_stride_with_dilation)\n",
        "            )\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False\n",
        "        )\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(\n",
        "            block,\n",
        "            128,\n",
        "            layers[1],\n",
        "            stride=2,\n",
        "            dilate=replace_stride_with_dilation[0]\n",
        "        )\n",
        "        self.layer3 = self._make_layer(\n",
        "            block,\n",
        "            256,\n",
        "            layers[2],\n",
        "            stride=2,\n",
        "            dilate=replace_stride_with_dilation[1]\n",
        "        )\n",
        "        self.layer4 = self._make_layer(\n",
        "            block,\n",
        "            512,\n",
        "            layers[3],\n",
        "            stride=last_stride,\n",
        "            dilate=replace_stride_with_dilation[2]\n",
        "        )\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = self._construct_fc_layer(\n",
        "            fc_dims, 512 * block.expansion, dropout_p\n",
        "        )\n",
        "        self.classifier = nn.Linear(self.feature_dim, num_classes)\n",
        "        print(f\"self.feature_dim {self.feature_dim}\")\n",
        "\n",
        "        self.mixstyle = None\n",
        "        if mixstyle_layers:\n",
        "            self.mixstyle = MixStyle(p=mixstyle_p, alpha=mixstyle_alpha, mix='random')\n",
        "            print('Insert MixStyle after the following layers: {}'.format(mixstyle_layers))\n",
        "        self.mixstyle_layers = mixstyle_layers\n",
        "\n",
        "        self._init_params()\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "              if isinstance(m, BasicBlock):\n",
        "                nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes, planes, stride, downsample, self.groups,\n",
        "                self.base_width, previous_dilation, norm_layer\n",
        "            )\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _construct_fc_layer(self, fc_dims, input_dim, dropout_p=None):\n",
        "\n",
        "        if fc_dims is None:\n",
        "            self.feature_dim = input_dim\n",
        "            return None\n",
        "\n",
        "        assert isinstance(\n",
        "            fc_dims, (list, tuple)\n",
        "        ), 'fc_dims must be either list or tuple, but got {}'.format(\n",
        "            type(fc_dims)\n",
        "        )\n",
        "\n",
        "        layers = []\n",
        "        for dim in fc_dims:\n",
        "            layers.append(nn.Linear(input_dim, dim))\n",
        "            layers.append(nn.BatchNorm1d(dim))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            if dropout_p is not None:\n",
        "                layers.append(nn.Dropout(p=dropout_p))\n",
        "            input_dim = dim\n",
        "\n",
        "        self.feature_dim = fc_dims[-1]\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _init_params(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(\n",
        "                    m.weight, mode='fan_out', nonlinearity='relu'\n",
        "                )\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def featuremaps(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        if 'layer1' in self.mixstyle_layers:\n",
        "            x = self.mixstyle(x)\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        if 'layer2' in self.mixstyle_layers:\n",
        "            x = self.mixstyle(x)\n",
        "\n",
        "        x = self.layer3(x)\n",
        "        if 'layer3' in self.mixstyle_layers:\n",
        "            x = self.mixstyle(x)\n",
        "\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.featuremaps(x)\n",
        "        v = self.global_avgpool(f)\n",
        "        v = v.view(v.size(0), -1)\n",
        "\n",
        "        if self.fc is not None:\n",
        "            v = self.fc(v)\n",
        "\n",
        "        if not self.training:\n",
        "            return v\n",
        "\n",
        "        y = self.classifier(v)\n",
        "\n",
        "        if self.loss == 'softmax':\n",
        "            return y\n",
        "        else:\n",
        "            raise KeyError(\"Unsupported loss: {}\".format(self.loss))\n",
        "\n",
        "class MultiTaskNet(nn.Module):\n",
        "    def __init__(self, net, backbone, num_classes_style = 4, num_classes_item = 65, pretrained=True):\n",
        "        super(MultiTaskNet, self).__init__()\n",
        "        self.net = net\n",
        "        self.net.classifier = nn.Identity()\n",
        "        if backbone == \"resnet18\":\n",
        "          self.net.fc1 = nn.Sequential(OrderedDict([('linear', nn.Linear(512,512)),('relu1', nn.ReLU()),('final', nn.Linear(512, 4))]))\n",
        "          self.net.fc2 = nn.Sequential(OrderedDict([('linear', nn.Linear(512,512)),('relu1', nn.ReLU()),('final', nn.Linear(512, 65))]))\n",
        "        elif backbone == \"resnet50\":\n",
        "          self.net.fc1 = nn.Sequential(OrderedDict([('linear', nn.Linear(2048,2048)),('relu1', nn.ReLU()),('final', nn.Linear(2048, 4))]))\n",
        "          self.net.fc2 = nn.Sequential(OrderedDict([('linear', nn.Linear(2048,2048)),('relu1', nn.ReLU()),('final', nn.Linear(2048, 65))]))\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        style_head = self.net.fc1(x)\n",
        "        item_head = self.net.fc2(x)\n",
        "\n",
        "        return style_head, item_head\n",
        "\n",
        "def init_pretrained_weights(model, model_url):\n",
        "    \"\"\"Initializes model with pretrained weights.\n",
        "\n",
        "    Layers that don't match with pretrained layers in name or size are kept unchanged.\n",
        "    \"\"\"\n",
        "    pretrain_dict = model_zoo.load_url(model_url)\n",
        "    model_dict = model.state_dict()\n",
        "    pretrain_dict = {\n",
        "        k: v\n",
        "        for k, v in pretrain_dict.items()\n",
        "        if k in model_dict and model_dict[k].size() == v.size()\n",
        "    }\n",
        "    model_dict.update(pretrain_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "\n",
        "\n",
        "def resnet18(num_classes=1000, loss='softmax', pretrained=True, **kwargs):\n",
        "    model = ResNet(\n",
        "        num_classes=num_classes,\n",
        "        loss=loss,\n",
        "        block=BasicBlock,\n",
        "        layers=[2, 2, 2, 2],\n",
        "        last_stride=2,\n",
        "        fc_dims=None,\n",
        "        dropout_p=None,\n",
        "        mixstyle_layers=['layer1', 'layer2', 'layer3'],\n",
        "        mixstyle_alpha=0.1,\n",
        "        **kwargs\n",
        "    )\n",
        "    if pretrained:\n",
        "        init_pretrained_weights(model, model_urls['resnet18'])\n",
        "    return model\n",
        "\n",
        "def resnet50(num_classes=1000, loss='softmax', pretrained=True, **kwargs):\n",
        "    model = ResNet(\n",
        "        num_classes=num_classes,\n",
        "        loss=loss,\n",
        "        block=Bottleneck,\n",
        "        layers=[3, 4, 6, 3],\n",
        "        last_stride=2,\n",
        "        fc_dims=None,\n",
        "        dropout_p=None,\n",
        "        mixstyle_layers=['layer1', 'layer2'],\n",
        "        mixstyle_alpha=0.1,\n",
        "        **kwargs\n",
        "    )\n",
        "    if pretrained:\n",
        "        init_pretrained_weights(model, model_urls['resnet50'])\n",
        "    return model\n",
        "\n",
        "backbone = [\"resnet18\" , \"resnet50\"]\n",
        "\n",
        "model_pretrained_ = resnet50()\n",
        "model_mixstyle_ = MultiTaskNet(model_pretrained_,backbone = \"resnet50\")\n",
        "model_to_train = model_mixstyle_.to(device)\n",
        "\n",
        "print(model_mixstyle_)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model_to_train.parameters(), lr = 0.0004, weight_decay = 0.001)\n",
        "\n",
        "def training_batch(train_loader, model,criterion, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    training_loss = []\n",
        "    training_style_loss = []\n",
        "    training_item_loss = []\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "      input = data[\"image\"].to(device)\n",
        "      style_label = data[\"style\"].to(device)\n",
        "      item_label = data[\"item\"].to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      style_output, item_output = model(input)\n",
        "\n",
        "      loss_1 = criterion(style_output, style_label)\n",
        "      loss_2 = criterion(item_output, item_label)\n",
        "\n",
        "      loss = loss_1 + loss_2\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      training_loss.append(loss.detach().item())\n",
        "      training_style_loss.append(loss_1.detach().item())\n",
        "      training_item_loss.append(loss_2.detach().item())\n",
        "\n",
        "\n",
        "    return training_style_loss, training_item_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2smtPL8f7Qs"
      },
      "outputs": [],
      "source": [
        "def test_batch(data, model, criterion):\n",
        "    model.eval()\n",
        "    input = data[\"image\"].to(device)\n",
        "    style_label = data[\"style\"].to(device)\n",
        "    item_label = data[\"item\"].to(device)\n",
        "\n",
        "    style_output, item_output = model(input)\n",
        "    loss_1 = criterion(style_output, style_label)\n",
        "    loss_2 = criterion(item_output, item_label)\n",
        "\n",
        "    total_loss = loss_1 + loss_2\n",
        "    return  loss_1.item(), loss_2.item()\n",
        "\n",
        "def accuracy(data, model):\n",
        "    model.eval()\n",
        "\n",
        "    input = data[\"image\"].to(device)\n",
        "    style_label = data[\"style\"].to(device)\n",
        "    item_label = data[\"item\"].to(device)\n",
        "\n",
        "    style_output, item_output = model(input)\n",
        "\n",
        "    _, pred_label_style = style_output.max(-1)\n",
        "    _, pred_label_item = item_output.max(-1)\n",
        "\n",
        "    correct_style = (pred_label_style == style_label)\n",
        "    correct_item = (pred_label_item == item_label)\n",
        "\n",
        "    return correct_style.cpu().detach().numpy().tolist(), correct_item.cpu().detach().numpy().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSbEush0f-rv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchmetrics.classification import MulticlassAveragePrecision\n",
        "\n",
        "def classification_map(data_loader, model):\n",
        "    # MulticlassAveragePrecision 메트릭스 초기화\n",
        "    map_metric_style = MulticlassAveragePrecision(num_classes=4, average=\"macro\")\n",
        "    map_metric_item = MulticlassAveragePrecision(num_classes=65, average=\"macro\")\n",
        "\n",
        "    device = next(model.parameters()).device  # 모델의 디바이스 확인\n",
        "\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "\n",
        "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
        "        for data in data_loader:\n",
        "            input = data[\"image\"].to(device)\n",
        "            style_true = data[\"style\"].to(device)\n",
        "            item_true = data[\"item\"].to(device)\n",
        "\n",
        "            style_pred, item_pred = model(input)\n",
        "\n",
        "            # MulticlassAveragePrecision 클래스는 확률을 입력으로 받으므로 softmax를 적용하여 확률로 변환\n",
        "            style_pred_prob = torch.softmax(style_pred, dim=1)\n",
        "            item_pred_prob = torch.softmax(item_pred, dim=1)\n",
        "\n",
        "            # 메트릭스 업데이트\n",
        "            map_metric_style.update(style_pred_prob, style_true)\n",
        "            map_metric_item.update(item_pred_prob, item_true)\n",
        "\n",
        "    # 평균 정밀도 계산\n",
        "    mAP_style = map_metric_style.compute()\n",
        "    mAP_item = map_metric_item.compute()\n",
        "\n",
        "    return mAP_style, mAP_item\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dKdII4ZgB4y"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Actual Training\n",
        "'''\n",
        "\n",
        "n_epochs = 10\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.9)\n",
        "\n",
        "train_loss_style, test_loss_style = [], []\n",
        "train_loss_item, test_loss_item = [], []\n",
        "train_accuracy_style, test_accuracy_style = [], []\n",
        "train_accuracy_item, test_accuracy_item = [], []\n",
        "all_precisions_style = []\n",
        "all_recalls_style = []\n",
        "all_precisions_item = []\n",
        "all_recalls_item = []\n",
        "\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_losses_style, train_losses_item = [], []\n",
        "    test_losses_style,test_losses_item = [],[]\n",
        "\n",
        "    train_accuracies_style, train_accuracies_item = [],[]\n",
        "    test_accuracies_style, test_accuracies_item = [], []\n",
        "\n",
        "\n",
        "    time1 = time.time()\n",
        "    train_losses_style, train_losses_item = training_batch(train_dataloader, model_to_train, criterion, optimizer)\n",
        "\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        train_batch_correct_style, train_batch_correct_item = accuracy(data, model_to_train)\n",
        "        train_accuracies_style.extend(train_batch_correct_style)\n",
        "        train_accuracies_item.extend(train_batch_correct_item)\n",
        "\n",
        "\n",
        "    train_per_epoch_loss_style = np.array(train_losses_style).mean()\n",
        "    train_per_epoch_loss_item = np.array(train_losses_item).mean()\n",
        "    train_per_epoch_accuracy_style = np.mean(train_accuracies_style)\n",
        "    train_per_epoch_accuracy_item = np.mean(train_accuracies_item)\n",
        "\n",
        "    #train_mAP_style, train_mAP_item = mean_average_precision(train_dataloader, model_to_train)\n",
        "    #print(\"Test Mean Average Precision (Style):\", train_mAP_style)\n",
        "    #print(\"Test Mean Average Precision (Item):\", train_mAP_item)\n",
        "\n",
        "    time2 = time.time()\n",
        "    print(\"Respective Execution time : \" + str(time2 - time1) + \"sec    \" + str(epoch+1) + \" epochs\")\n",
        "\n",
        "    learning_rate = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #   for i, data in enumerate(val_dataloader):\n",
        "    #       val_batch_correct_style,val_batch_correct_item = accuracy(data, model_to_train)\n",
        "    #       val_accuracies_style.extend(val_batch_correct_style)\n",
        "    #       val_accuracies_item.extend(val_batch_correct_item)\n",
        "\n",
        "    #   val_per_epoch_accuracy_style = np.mean(val_accuracies_style)\n",
        "    #   val_per_epoch_accuracy_item = np.mean(val_accuracies_item)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for i, data in enumerate(test_dataloader):\n",
        "        test_batch_loss_style, test_batch_loss_item = test_batch(data, model_to_train, criterion)\n",
        "        test_losses_style.append(test_batch_loss_style)\n",
        "        test_losses_item.append(test_batch_loss_item)\n",
        "\n",
        "        test_batch_correct_style, test_batch_correct_item = accuracy(data , model_to_train)\n",
        "        test_accuracies_style.extend(test_batch_correct_style)\n",
        "        test_accuracies_item.extend(test_batch_correct_item)\n",
        "\n",
        "    test_mAP_style, test_mAP_item  = classification_map(test_dataloader, model_to_train)\n",
        "    #average_precision_style, average_recall_style, average_precision_item, average_recall_item = test_precision_recall( model_to_train, test_dataloader)\n",
        "\n",
        "    #all_precisions_style.append(average_precision_style)\n",
        "    #all_recalls_style.append(average_recall_style)\n",
        "\n",
        "    #all_precisions_item.append(average_precision_item)\n",
        "    #all_recalls_item.append(average_recall_item)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    test_per_epoch_loss_style = np.array(test_losses_style).mean()\n",
        "    test_per_epoch_loss_item = np.array(test_losses_item).mean()\n",
        "    test_per_epoch_accuracy_style = np.mean(test_accuracies_style)\n",
        "    test_per_epoch_accuracy_item = np.mean(test_accuracies_item)\n",
        "\n",
        "\n",
        "\n",
        "    # train, test\n",
        "    # train_loss_style.append(train_per_epoch_loss_style)\n",
        "    # train_accuracy_style.append(train_per_epoch_accuracy_style)\n",
        "\n",
        "    # train_loss_item.append(train_per_epoch_loss_item)\n",
        "    # train_accuracy_item.append(train_per_epoch_accuracy_item)\n",
        "\n",
        "    # test_loss_style.append(test_per_epoch_loss_style)\n",
        "    # test_accuracy_style.append(test_per_epoch_accuracy_style)\n",
        "\n",
        "    # test_loss_item.append(test_per_epoch_loss_item)\n",
        "    # test_accuracy_item.append(test_per_epoch_accuracy_item)\n",
        "\n",
        "\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}/{n_epochs}\\t| Training loss style: {train_per_epoch_loss_style:.4f} | Training loss item : {train_per_epoch_loss_item:.4f}')\n",
        "    print(f' Training accuracy style: {train_per_epoch_accuracy_style:.4f} |Training accuracy item: {train_per_epoch_accuracy_item:.4f} ', end='')\n",
        "    print(f'Test loss style: {test_per_epoch_loss_style:.4f} | Test loss item: {test_per_epoch_loss_item:.4f} |  Test accuracy style: {test_per_epoch_accuracy_style:.4f} Test accuracy item: {test_per_epoch_accuracy_item:.4f}')\n",
        "    print(f\"Test Mean Average Precision (Style): {test_mAP_style} | Test Mean Average Precision (Item): {test_mAP_item}\" + '\\n')\n",
        "    #print(f\"Test Mean Average Precision (Style): {average_precision_style} | Test Mean Average Precision (Item): {average_precision_item}\")\n",
        "    #print(f\"Test Mean Average Recall (Style): {average_recall_style} | Test Mean Average Recall (Item): {average_recall_item}\")\n",
        "\n",
        "\n",
        "'''\n",
        "mean_precision_style = np.mean(all_precisions_style)\n",
        "mean_recall_style = np.mean(all_recalls_style)\n",
        "mean_precision_item =  np.mean(all_precisions_item)\n",
        "mean_recall_item = np.mean(all_recalls_item)\n",
        "\n",
        "  # 평균 성능 결과 출력\n",
        "print(\"Mean Precision Style:\", mean_precision_style)\n",
        "print(\"Mean Recall Style:\", mean_recall_style)\n",
        "print(\"Mean Precision Style:\", mean_precision_item)\n",
        "print(\"Mean Recall Style:\", mean_recall_item)\n",
        "'''\n",
        "\n",
        "    #print(\"Test Mean Average Precision (Style):\", test_mAP_style)\n",
        "    #print(\"Test Mean Average Precision (Item):\", test_mAP_item)\n",
        "    # print(f'Test loss style: {test_per_epoch_loss_style:.4f} | Test loss item: {test_per_epoch_loss_item:.4f} |  Test accuracy style: {test_per_epoch_accuracy_style:.4f} Test accuracy item: {test_per_epoch_accuracy_item:.4f}' + '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4EFwAEHftow"
      },
      "source": [
        "##VERSION 3 using Domain label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_zkeR15gPeE",
        "outputId": "9763b212-9590-4447-9756-d6c692daa785"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 136MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MultiTaskNet(\n",
            "  (net): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (global_avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Identity()\n",
            "    (mixstyle): MixStyle(p=0.5, alpha=0.1, eps=1e-06, mix=random)\n",
            "    (fc1): Sequential(\n",
            "      (linear): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "      (relu1): ReLU()\n",
            "      (final): Linear(in_features=2048, out_features=4, bias=True)\n",
            "    )\n",
            "    (fc2): Sequential(\n",
            "      (linear): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "      (relu1): ReLU()\n",
            "      (final): Linear(in_features=2048, out_features=65, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "from torchvision.models.resnet import BasicBlock\n",
        "from torch.utils import model_zoo\n",
        "\n",
        "\n",
        "class MixStyle(nn.Module):\n",
        "    \"\"\"MixStyle.\n",
        "\n",
        "    Reference:\n",
        "      Zhou et al. Domain Generalization with MixStyle. ICLR 2021.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5, alpha=0.1, eps=1e-6, mix='random'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          p (float): probability of using MixStyle.\n",
        "          alpha (float): parameter of the Beta distribution.\n",
        "          eps (float): scaling parameter to avoid numerical issues.\n",
        "          mix (str): how to mix.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        self.beta = torch.distributions.Beta(alpha, alpha)\n",
        "        self.eps = eps\n",
        "        self.alpha = alpha\n",
        "        self.mix = mix\n",
        "        self._activated = True\n",
        "        self.lmda = None\n",
        "        self.perm = None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'MixStyle(p={self.p}, alpha={self.alpha}, eps={self.eps}, mix={self.mix})'\n",
        "\n",
        "    def set_activation_status(self, status=True):\n",
        "        self._activated = status\n",
        "\n",
        "    def update_mix_method(self, mix='random'):\n",
        "        self.mix = mix\n",
        "\n",
        "    def forward(self, x , perm):\n",
        "\n",
        "        if not self.training or not self._activated:\n",
        "            return x\n",
        "\n",
        "        if random.random() > self.p:\n",
        "            self.lmda = torch.full((x.size()[0], 1, 1, 1), 1)\n",
        "            #self.perm = torch.arange(1, 65)\n",
        "            return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        B = x.size(0)\n",
        "\n",
        "        mu = x.mean(dim=[2, 3], keepdim=True)\n",
        "        var = x.var(dim=[2, 3], keepdim=True)\n",
        "        sig = (var + self.eps).sqrt()\n",
        "        mu, sig = mu.detach(), sig.detach()\n",
        "        x_normed = (x-mu) / sig\n",
        "\n",
        "\n",
        "        sampled = self.beta.sample()\n",
        "\n",
        "        if sampled < 0.5 :\n",
        "          sampled = 1- sampled\n",
        "        lmda = torch.full((x.size()[0], 1, 1, 1), sampled)\n",
        "\n",
        "        self.lmda = lmda.to(x.device)\n",
        "\n",
        "        #lmda = self.beta.sample((B, 1, 1, 1))\n",
        "        #lmda = torch.tensor(lmda)\n",
        "        # for i in range(B):\n",
        "        #   if lmda[i].item() < 0.5:\n",
        "        #     lmda[i] = 1 - lmda[i]\n",
        "\n",
        "        #print(f\"lmda : {lmda}\")\n",
        "\n",
        "\n",
        "        mu2, sig2 = mu[perm], sig[perm]\n",
        "        mu_mix = mu*self.lmda + mu2 * (1-self.lmda)\n",
        "        sig_mix = sig*self.lmda + sig2 * (1-self.lmda)\n",
        "\n",
        "        return x_normed*sig_mix + mu_mix\n",
        "\n",
        "\n",
        "from __future__ import division, absolute_import\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    'resnet18',\n",
        "    'resnet50'\n",
        "]\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18':\n",
        "    'https://download.pytorch.org/models/resnet18-f37072fd.pth',\n",
        "    'resnet50':\n",
        "    'https://download.pytorch.org/models/resnet50-11ad3fa6.pth'\n",
        "}\n",
        "\n",
        "'''\n",
        "Reference : https://pytorch.org/vision/main/_modules/torchvision/models/resnet.html\n",
        "'''\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation\n",
        "    )\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes, out_planes, kernel_size=1, stride=stride, bias=False\n",
        "    )\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride=1,\n",
        "        downsample=None,\n",
        "        groups=1,\n",
        "        base_width=64,\n",
        "        dilation=1,\n",
        "        norm_layer=None\n",
        "    ):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError(\n",
        "                'BasicBlock only supports groups=1 and base_width=64'\n",
        "            )\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\n",
        "                \"Dilation > 1 not supported in BasicBlock\"\n",
        "            )\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\" https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride = 1,\n",
        "        downsample = None,\n",
        "        groups = 1,\n",
        "        base_width = 64,\n",
        "        dilation = 1,\n",
        "        norm_layer = None,\n",
        "    ):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.0)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        loss,\n",
        "        block,\n",
        "        layers,\n",
        "        zero_init_residual=False,\n",
        "        groups=1,\n",
        "        width_per_group=64,\n",
        "        replace_stride_with_dilation=None,\n",
        "        norm_layer=None,\n",
        "        last_stride=2,\n",
        "        fc_dims=None,\n",
        "        dropout_p=None,\n",
        "        mixstyle_layers=[],\n",
        "        mixstyle_p=0.5,\n",
        "        mixstyle_alpha=0.1,\n",
        "    ):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "        self.loss = loss\n",
        "        self.feature_dim = 512 * block.expansion\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\n",
        "\n",
        "                format(replace_stride_with_dilation)\n",
        "            )\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False\n",
        "        )\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(\n",
        "            block,\n",
        "            128,\n",
        "            layers[1],\n",
        "            stride=2,\n",
        "            dilate=replace_stride_with_dilation[0]\n",
        "        )\n",
        "        self.layer3 = self._make_layer(\n",
        "            block,\n",
        "            256,\n",
        "            layers[2],\n",
        "            stride=2,\n",
        "            dilate=replace_stride_with_dilation[1]\n",
        "        )\n",
        "        self.layer4 = self._make_layer(\n",
        "            block,\n",
        "            512,\n",
        "            layers[3],\n",
        "            stride=last_stride,\n",
        "            dilate=replace_stride_with_dilation[2]\n",
        "        )\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "\n",
        "        self.mixstyle = MixStyle(p=mixstyle_p, alpha=mixstyle_alpha, mix='random')\n",
        "        self.mixstyle_layers = mixstyle_layers\n",
        "\n",
        "        self.lmda_1 = None\n",
        "\n",
        "        self.lmda_2 = None\n",
        "\n",
        "        self.lmda_3 = torch.full((64, 1, 1, 1), 1)\n",
        "\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n",
        "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
        "                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes, planes, stride, downsample, self.groups,\n",
        "                self.base_width, previous_dilation, norm_layer\n",
        "            )\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, perm_1, perm_2, perm_3):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        if 'layer1' in self.mixstyle_layers:\n",
        "            x = self.mixstyle(x, perm_1)\n",
        "            #print(self.mixstyle.lmda_1)\n",
        "            #print(self.mixstyle.perm_1)\n",
        "            self.lmda_1 = self.mixstyle.lmda\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        if 'layer2' in self.mixstyle_layers:\n",
        "            x = self.mixstyle(x, perm_2)\n",
        "            self.lmda_2 = self.mixstyle.lmda\n",
        "\n",
        "        x = self.layer3(x)\n",
        "        if 'layer3' in self.mixstyle_layers:\n",
        "            x = self.mixstyle(x, perm_3)\n",
        "            self.lmda_3 = self.mixstyle.lmda\n",
        "\n",
        "        f = self.layer4(x)\n",
        "        v = self.global_avgpool(f)\n",
        "        v = v.view(v.size(0), -1)\n",
        "\n",
        "        y = self.fc(v)\n",
        "\n",
        "        return y\n",
        "\n",
        "class MultiTaskNet(nn.Module):\n",
        "    def __init__(self, net, backbone, num_classes_style = 4, num_classes_item = 65, pretrained=True ):\n",
        "        super(MultiTaskNet, self).__init__()\n",
        "        self.net = net\n",
        "        self.net.fc = nn.Identity()\n",
        "\n",
        "        if backbone == \"resnet18\":\n",
        "          self.net.fc1 = nn.Sequential(OrderedDict([('linear', nn.Linear(512,512)),('relu1', nn.ReLU()),('final', nn.Linear(512, 4))]))\n",
        "          self.net.fc2 = nn.Sequential(OrderedDict([('linear', nn.Linear(512,512)),('relu1', nn.ReLU()),('final', nn.Linear(512, 65))]))\n",
        "        elif backbone == \"resnet50\":\n",
        "          self.net.fc1 = nn.Sequential(OrderedDict([('linear', nn.Linear(2048,2048)),('relu1', nn.ReLU()),('final', nn.Linear(2048, 4))]))\n",
        "          self.net.fc2 = nn.Sequential(OrderedDict([('linear', nn.Linear(2048,2048)),('relu1', nn.ReLU()),('final', nn.Linear(2048, 65))]))\n",
        "\n",
        "\n",
        "    def forward(self, x, perm_1, perm_2, perm_3):\n",
        "        x = self.net(x, perm_1, perm_2, perm_3)\n",
        "        style_head = self.net.fc1(x)\n",
        "        item_head = self.net.fc2(x)\n",
        "        #print(self.net.lmda_1)\n",
        "        #print(self.net.perm_1)\n",
        "\n",
        "        #print(self.net.mixstyle.lmda)\n",
        "        #print(self.net.mixstyle.perm)\n",
        "\n",
        "        return style_head, item_head\n",
        "\n",
        "def init_pretrained_weights(model, model_url):\n",
        "    \"\"\"Initializes model with pretrained weights.\n",
        "\n",
        "    Layers that don't match with pretrained layers in name or size are kept unchanged.\n",
        "    \"\"\"\n",
        "    pretrain_dict = model_zoo.load_url(model_url)\n",
        "    model_dict = model.state_dict()\n",
        "    pretrain_dict = {\n",
        "        k: v\n",
        "        for k, v in pretrain_dict.items()\n",
        "        if k in model_dict and model_dict[k].size() == v.size()\n",
        "    }\n",
        "    model_dict.update(pretrain_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "\n",
        "\n",
        "def resnet18(num_classes=1000, loss='softmax', pretrained=True, **kwargs):\n",
        "    model = ResNet(\n",
        "        num_classes=num_classes,\n",
        "        loss=loss,\n",
        "        block=BasicBlock,\n",
        "        layers=[2, 2, 2, 2],\n",
        "        last_stride=2,\n",
        "        fc_dims=None,\n",
        "        dropout_p=None,\n",
        "        mixstyle_layers=['layer1', 'layer2', 'layer3'],\n",
        "        mixstyle_alpha=0.1,\n",
        "        **kwargs\n",
        "    )\n",
        "    if pretrained:\n",
        "        init_pretrained_weights(model, model_urls['resnet18'])\n",
        "    return model\n",
        "\n",
        "def resnet50(num_classes=1000, loss='softmax', pretrained=True, **kwargs):\n",
        "    model = ResNet(\n",
        "        num_classes=num_classes,\n",
        "        loss=loss,\n",
        "        block=Bottleneck,\n",
        "        layers=[3, 4, 6, 3],\n",
        "        last_stride=2,\n",
        "        fc_dims=None,\n",
        "        dropout_p=None,\n",
        "        mixstyle_layers=['layer1', 'layer2'],\n",
        "        mixstyle_alpha=0.1,\n",
        "        **kwargs\n",
        "    )\n",
        "    if pretrained:\n",
        "        init_pretrained_weights(model, model_urls['resnet50'])\n",
        "    return model\n",
        "\n",
        "backbone = [\"resnet18\" , \"resnet50\"]\n",
        "\n",
        "model_pretrained_ = resnet50()\n",
        "model_mixstyle_ = MultiTaskNet(model_pretrained_, backbone = \"resnet50\")\n",
        "model_to_train = model_mixstyle_.to(device)\n",
        "\n",
        "print(model_mixstyle_)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model_to_train.parameters(), lr = 0.0004, weight_decay = 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "VAY-JX5_gSwB",
        "outputId": "52b05a78-b403-47d3-d1c8-0ebb4ed466ba"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n    data = data.to(device)\\n    labels = labels.to(device)\\n    output = model(data)\\n    _, pred_labels = output.max(-1)\\n    correct = (pred_labels == labels)\\n    return correct.cpu().detach().numpy().tolist()\\n'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "Train and Test function\n",
        "'''\n",
        "def training_batch(train_loader, model,criterion, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    training_loss = []\n",
        "    training_style_loss = []\n",
        "    training_item_loss = []\n",
        "\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "      input = data[\"image\"].to(device)\n",
        "      style_label_ = data[\"style\"]\n",
        "      style_label = style_label_.to(device)\n",
        "      item_label = data[\"item\"].to(device)\n",
        "\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      rand_index_1 = torch.randperm(input.size()[0]).cuda()\n",
        "      rand_index_2 = torch.randperm(input.size()[0]).cuda()\n",
        "      rand_index_3 = torch.randperm(input.size()[0]).cuda()\n",
        "\n",
        "      style_output, item_output = model(input, rand_index_1, rand_index_2, rand_index_3)\n",
        "\n",
        "      #####labelling#####\n",
        "      if(model.net.lmda_1[0] != None):\n",
        "        lmda_1 = float(model.net.lmda_1[0])\n",
        "      if(model.net.lmda_2[0] != None):\n",
        "        lmda_2 = float(model.net.lmda_2[0])\n",
        "      # if(model.net.lmda_3[0] != None):\n",
        "      #   lmda_3 = float(model.net.lmda_3[0])\n",
        "\n",
        "\n",
        "      style_label_a = style_label[rand_index_1]\n",
        "      style_label_b = style_label[rand_index_2]\n",
        "      #style_label_c = style_label[rand_index_3]\n",
        "\n",
        "\n",
        "      loss_style_temp1 = criterion(style_output, style_label_a)\n",
        "      loss_style_temp2 = criterion(style_output, style_label_b)\n",
        "      #loss_style_temp3 = criterion(style_output, style_label_c)\n",
        "\n",
        "      loss_style_original = criterion(style_output, style_label)\n",
        "\n",
        "      #loss_style = lmda_3 * (lmda_2 * (lmda_1 * loss_style_original + (1 - lmda_1) * loss_style_temp1) + (1 - lmda_2) * loss_style_temp2) + (1 - lmda_3) * loss_style_temp3\n",
        "      loss_style = lmda_2 * (lmda_1 * loss_style_original + (1 - lmda_1) * loss_style_temp1) + (1 - lmda_2) * loss_style_temp2\n",
        "\n",
        "\n",
        "      loss_item = criterion(item_output, item_label)\n",
        "\n",
        "      loss = loss_style + loss_item\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      training_loss.append(loss.detach().item())\n",
        "      training_style_loss.append(loss_style_original.detach().item())\n",
        "      training_item_loss.append(loss_item.detach().item())\n",
        "\n",
        "      # print(f\"lmda_1 : {lmda_1}\")\n",
        "      # print(f\"lmda_2 : {lmda_2}\")\n",
        "      # print(f\"lmda_3 : {lmda_3}\")\n",
        "\n",
        "\n",
        "    return training_style_loss, training_item_loss\n",
        "\n",
        "'''\n",
        "def training_batch_with_cutmix(train_loader, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    beta = 1.0\n",
        "    cutmix_prob = 0.5\n",
        "\n",
        "    for _, (input, target) in enumerate(train_loader):\n",
        "        input = input.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        r = np.random.rand(1)\n",
        "\n",
        "        if beta > int(0.0) and r < cutmix_prob:\n",
        "            lam = np.random.beta(beta, beta)\n",
        "            rand_index = torch.randperm(input.size()[0]).cuda()\n",
        "            target_a = target # target이 label\n",
        "            target_b = target[rand_index]\n",
        "            bbx1, bby1, bbx2, bby2 = rand_bbox(input.size(), lam)\n",
        "            input[:, :, bbx1:bbx2, bby1:bby2] = input[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "                # adjust lambda to exactly match pixel ratio\n",
        "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (input.size()[-1] * input.size()[-2]))\n",
        "                # compute output\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target_a) * lam + criterion(output, target_b) * (1. - lam)\n",
        "        else:\n",
        "            # compute output\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        training_losses.append(loss.item())\n",
        "\n",
        "    for data, labels in train_loader:\n",
        "        training_accuracies.extend(accuracy(data, labels, model))\n",
        "'''\n",
        "\n",
        "def test_batch(data, model, criterion):\n",
        "    model.eval()\n",
        "    input = data[\"image\"].to(device)\n",
        "    style_label = data[\"style\"].to(device)\n",
        "    item_label = data[\"item\"].to(device)\n",
        "\n",
        "    style_output, item_output = model(input, None, None, None)\n",
        "    loss_1 = criterion(style_output, style_label)\n",
        "    loss_2 = criterion(item_output, item_label)\n",
        "\n",
        "    total_loss = loss_1 + loss_2\n",
        "    return  loss_1.item(), loss_2.item()\n",
        "\n",
        "def accuracy(data, model):\n",
        "    model.eval()\n",
        "\n",
        "    input = data[\"image\"].to(device)\n",
        "    style_label = data[\"style\"].to(device)\n",
        "    item_label = data[\"item\"].to(device)\n",
        "\n",
        "    style_output, item_output = model(input, None, None, None)\n",
        "\n",
        "    _, pred_label_style = style_output.max(-1)\n",
        "    _, pred_label_item = item_output.max(-1)\n",
        "\n",
        "    correct_style = (pred_label_style == style_label)\n",
        "    correct_item = (pred_label_item == item_label)\n",
        "\n",
        "    return correct_style.cpu().detach().numpy().tolist(), correct_item.cpu().detach().numpy().tolist()\n",
        "\n",
        "\n",
        "'''\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    _, pred_labels = output.max(-1)\n",
        "    correct = (pred_labels == labels)\n",
        "    return correct.cpu().detach().numpy().tolist()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ruN_DQlgVfv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchmetrics.classification import MulticlassAveragePrecision\n",
        "\n",
        "def classification_map(data_loader, model):\n",
        "    # MulticlassAveragePrecision 메트릭스 초기화\n",
        "    map_metric_style = MulticlassAveragePrecision(num_classes=4, average=\"macro\")\n",
        "    map_metric_item = MulticlassAveragePrecision(num_classes=65, average=\"macro\")\n",
        "\n",
        "    device = next(model.parameters()).device  # 모델의 디바이스 확인\n",
        "\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "\n",
        "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
        "        for data in data_loader:\n",
        "            input = data[\"image\"].to(device)\n",
        "            style_true = data[\"style\"].to(device)\n",
        "            item_true = data[\"item\"].to(device)\n",
        "\n",
        "            style_pred, item_pred = model(input,None, None, None)\n",
        "\n",
        "            # MulticlassAveragePrecision 클래스는 확률을 입력으로 받으므로 softmax를 적용하여 확률로 변환\n",
        "            style_pred_prob = torch.softmax(style_pred, dim=1)\n",
        "            item_pred_prob = torch.softmax(item_pred, dim=1)\n",
        "\n",
        "            # 메트릭스 업데이트\n",
        "            map_metric_style.update(style_pred_prob, style_true)\n",
        "            map_metric_item.update(item_pred_prob, item_true)\n",
        "\n",
        "    # 평균 정밀도 계산\n",
        "    mAP_style = map_metric_style.compute()\n",
        "    mAP_item = map_metric_item.compute()\n",
        "\n",
        "    return mAP_style, mAP_item\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "id": "W5JmbR-cgYFL",
        "outputId": "fb861835-467e-40bc-f708-6cbe8ee26f29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Respective Execution time : 562.4271512031555sec    1 epochs\n",
            "Epoch: 1/10\t| Training loss style: 0.6825 | Training loss item : 1.4756\n",
            " Training accuracy style: 0.7997 |Training accuracy item: 0.7996 Test loss style: 0.6101 | Test loss item: 1.0027 |  Test accuracy style: 0.7644 Test accuracy item: 0.7314\n",
            "Test Mean Average Precision (Style): 0.8286006450653076 | Test Mean Average Precision (Item): 0.8217507600784302\n",
            "\n",
            "Respective Execution time : 560.2008926868439sec    2 epochs\n",
            "Epoch: 2/10\t| Training loss style: 0.4775 | Training loss item : 0.6194\n",
            " Training accuracy style: 0.8536 |Training accuracy item: 0.8844 Test loss style: 0.5094 | Test loss item: 0.7979 |  Test accuracy style: 0.8080 Test accuracy item: 0.7893\n",
            "Test Mean Average Precision (Style): 0.8700501322746277 | Test Mean Average Precision (Item): 0.8696520328521729\n",
            "\n",
            "Respective Execution time : 569.5523674488068sec    3 epochs\n",
            "Epoch: 3/10\t| Training loss style: 0.3922 | Training loss item : 0.3985\n",
            " Training accuracy style: 0.8980 |Training accuracy item: 0.9289 Test loss style: 0.4538 | Test loss item: 0.7660 |  Test accuracy style: 0.8266 Test accuracy item: 0.8002\n",
            "Test Mean Average Precision (Style): 0.888477087020874 | Test Mean Average Precision (Item): 0.8815496563911438\n",
            "\n",
            "Respective Execution time : 560.6215918064117sec    4 epochs\n",
            "Epoch: 4/10\t| Training loss style: 0.3280 | Training loss item : 0.3019\n",
            " Training accuracy style: 0.9274 |Training accuracy item: 0.9573 Test loss style: 0.4573 | Test loss item: 0.6905 |  Test accuracy style: 0.8316 Test accuracy item: 0.8170\n",
            "Test Mean Average Precision (Style): 0.8864866495132446 | Test Mean Average Precision (Item): 0.8866854310035706\n",
            "\n",
            "Respective Execution time : 575.0339176654816sec    5 epochs\n",
            "Epoch: 5/10\t| Training loss style: 0.2670 | Training loss item : 0.2100\n",
            " Training accuracy style: 0.9177 |Training accuracy item: 0.9477 Test loss style: 0.4884 | Test loss item: 0.7938 |  Test accuracy style: 0.8242 Test accuracy item: 0.7989\n",
            "Test Mean Average Precision (Style): 0.8836288452148438 | Test Mean Average Precision (Item): 0.8808770775794983\n",
            "\n",
            "Respective Execution time : 577.2884774208069sec    6 epochs\n",
            "Epoch: 6/10\t| Training loss style: 0.2154 | Training loss item : 0.1967\n",
            " Training accuracy style: 0.9523 |Training accuracy item: 0.9564 Test loss style: 0.4406 | Test loss item: 0.8197 |  Test accuracy style: 0.8428 Test accuracy item: 0.7955\n",
            "Test Mean Average Precision (Style): 0.8964843153953552 | Test Mean Average Precision (Item): 0.8767226934432983\n",
            "\n",
            "Respective Execution time : 573.5974838733673sec    7 epochs\n",
            "Epoch: 7/10\t| Training loss style: 0.1861 | Training loss item : 0.1514\n",
            " Training accuracy style: 0.9455 |Training accuracy item: 0.9627 Test loss style: 0.4782 | Test loss item: 0.7625 |  Test accuracy style: 0.8313 Test accuracy item: 0.8133\n",
            "Test Mean Average Precision (Style): 0.8911440372467041 | Test Mean Average Precision (Item): 0.8897896409034729\n",
            "\n",
            "Respective Execution time : 570.5743916034698sec    8 epochs\n",
            "Epoch: 8/10\t| Training loss style: 0.1537 | Training loss item : 0.1275\n",
            " Training accuracy style: 0.9515 |Training accuracy item: 0.9773 Test loss style: 0.5089 | Test loss item: 0.7268 |  Test accuracy style: 0.8382 Test accuracy item: 0.8248\n",
            "Test Mean Average Precision (Style): 0.892183780670166 | Test Mean Average Precision (Item): 0.8980419039726257\n",
            "\n",
            "Respective Execution time : 572.0223724842072sec    9 epochs\n",
            "Epoch: 9/10\t| Training loss style: 0.1627 | Training loss item : 0.1190\n",
            " Training accuracy style: 0.9674 |Training accuracy item: 0.9722 Test loss style: 0.4667 | Test loss item: 0.8116 |  Test accuracy style: 0.8509 Test accuracy item: 0.8195\n",
            "Test Mean Average Precision (Style): 0.8884841203689575 | Test Mean Average Precision (Item): 0.88812255859375\n",
            "\n",
            "Respective Execution time : 570.3457760810852sec    10 epochs\n",
            "Epoch: 10/10\t| Training loss style: 0.1313 | Training loss item : 0.1026\n",
            " Training accuracy style: 0.9712 |Training accuracy item: 0.9731 Test loss style: 0.5167 | Test loss item: 0.8474 |  Test accuracy style: 0.8363 Test accuracy item: 0.8142\n",
            "Test Mean Average Precision (Style): 0.876855731010437 | Test Mean Average Precision (Item): 0.8855008482933044\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nmean_precision_style = np.mean(all_precisions_style)\\nmean_recall_style = np.mean(all_recalls_style)\\nmean_precision_item =  np.mean(all_precisions_item)\\nmean_recall_item = np.mean(all_recalls_item)\\n\\n  # 평균 성능 결과 출력\\nprint(\"Mean Precision Style:\", mean_precision_style)\\nprint(\"Mean Recall Style:\", mean_recall_style)\\nprint(\"Mean Precision Style:\", mean_precision_item)\\nprint(\"Mean Recall Style:\", mean_recall_item)\\n'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "Actual Training\n",
        "'''\n",
        "\n",
        "n_epochs = 10\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.9)\n",
        "\n",
        "train_loss_style, test_loss_style = [], []\n",
        "train_loss_item, test_loss_item = [], []\n",
        "train_accuracy_style, test_accuracy_style = [], []\n",
        "train_accuracy_item, test_accuracy_item = [], []\n",
        "all_precisions_style = []\n",
        "all_recalls_style = []\n",
        "all_precisions_item = []\n",
        "all_recalls_item = []\n",
        "\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_losses_style, train_losses_item = [], []\n",
        "    test_losses_style,test_losses_item = [],[]\n",
        "\n",
        "    train_accuracies_style, train_accuracies_item = [],[]\n",
        "    test_accuracies_style, test_accuracies_item = [], []\n",
        "\n",
        "\n",
        "    time1 = time.time()\n",
        "    train_losses_style, train_losses_item = training_batch(train_dataloader, model_to_train, criterion, optimizer)\n",
        "\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        train_batch_correct_style, train_batch_correct_item = accuracy(data, model_to_train)\n",
        "        train_accuracies_style.extend(train_batch_correct_style)\n",
        "        train_accuracies_item.extend(train_batch_correct_item)\n",
        "\n",
        "\n",
        "    train_per_epoch_loss_style = np.array(train_losses_style).mean()\n",
        "    train_per_epoch_loss_item = np.array(train_losses_item).mean()\n",
        "    train_per_epoch_accuracy_style = np.mean(train_accuracies_style)\n",
        "    train_per_epoch_accuracy_item = np.mean(train_accuracies_item)\n",
        "\n",
        "    #train_mAP_style, train_mAP_item = mean_average_precision(train_dataloader, model_to_train)\n",
        "    #print(\"Test Mean Average Precision (Style):\", train_mAP_style)\n",
        "    #print(\"Test Mean Average Precision (Item):\", train_mAP_item)\n",
        "\n",
        "    time2 = time.time()\n",
        "    print(\"Respective Execution time : \" + str(time2 - time1) + \"sec    \" + str(epoch+1) + \" epochs\")\n",
        "\n",
        "    learning_rate = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #   for i, data in enumerate(val_dataloader):\n",
        "    #       val_batch_correct_style,val_batch_correct_item = accuracy(data, model_to_train)\n",
        "    #       val_accuracies_style.extend(val_batch_correct_style)\n",
        "    #       val_accuracies_item.extend(val_batch_correct_item)\n",
        "\n",
        "    #   val_per_epoch_accuracy_style = np.mean(val_accuracies_style)\n",
        "    #   val_per_epoch_accuracy_item = np.mean(val_accuracies_item)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for i, data in enumerate(test_dataloader):\n",
        "        test_batch_loss_style, test_batch_loss_item = test_batch(data, model_to_train, criterion)\n",
        "        test_losses_style.append(test_batch_loss_style)\n",
        "        test_losses_item.append(test_batch_loss_item)\n",
        "\n",
        "        test_batch_correct_style, test_batch_correct_item = accuracy(data , model_to_train)\n",
        "        test_accuracies_style.extend(test_batch_correct_style)\n",
        "        test_accuracies_item.extend(test_batch_correct_item)\n",
        "\n",
        "    test_mAP_style, test_mAP_item  = classification_map(test_dataloader, model_to_train)\n",
        "    #average_precision_style, average_recall_style, average_precision_item, average_recall_item = test_precision_recall( model_to_train, test_dataloader)\n",
        "\n",
        "    #all_precisions_style.append(average_precision_style)\n",
        "    #all_recalls_style.append(average_recall_style)\n",
        "\n",
        "    #all_precisions_item.append(average_precision_item)\n",
        "    #all_recalls_item.append(average_recall_item)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    test_per_epoch_loss_style = np.array(test_losses_style).mean()\n",
        "    test_per_epoch_loss_item = np.array(test_losses_item).mean()\n",
        "    test_per_epoch_accuracy_style = np.mean(test_accuracies_style)\n",
        "    test_per_epoch_accuracy_item = np.mean(test_accuracies_item)\n",
        "\n",
        "\n",
        "\n",
        "    # train, test\n",
        "    # train_loss_style.append(train_per_epoch_loss_style)\n",
        "    # train_accuracy_style.append(train_per_epoch_accuracy_style)\n",
        "\n",
        "    # train_loss_item.append(train_per_epoch_loss_item)\n",
        "    # train_accuracy_item.append(train_per_epoch_accuracy_item)\n",
        "\n",
        "    # test_loss_style.append(test_per_epoch_loss_style)\n",
        "    # test_accuracy_style.append(test_per_epoch_accuracy_style)\n",
        "\n",
        "    # test_loss_item.append(test_per_epoch_loss_item)\n",
        "    # test_accuracy_item.append(test_per_epoch_accuracy_item)\n",
        "\n",
        "\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}/{n_epochs}\\t| Training loss style: {train_per_epoch_loss_style:.4f} | Training loss item : {train_per_epoch_loss_item:.4f}')\n",
        "    print(f' Training accuracy style: {train_per_epoch_accuracy_style:.4f} |Training accuracy item: {train_per_epoch_accuracy_item:.4f} ', end='')\n",
        "    print(f'Test loss style: {test_per_epoch_loss_style:.4f} | Test loss item: {test_per_epoch_loss_item:.4f} |  Test accuracy style: {test_per_epoch_accuracy_style:.4f} Test accuracy item: {test_per_epoch_accuracy_item:.4f}')\n",
        "    print(f\"Test Mean Average Precision (Style): {test_mAP_style} | Test Mean Average Precision (Item): {test_mAP_item}\" + '\\n')\n",
        "    #print(f\"Test Mean Average Precision (Style): {average_precision_style} | Test Mean Average Precision (Item): {average_precision_item}\")\n",
        "    #print(f\"Test Mean Average Recall (Style): {average_recall_style} | Test Mean Average Recall (Item): {average_recall_item}\")\n",
        "\n",
        "\n",
        "'''\n",
        "mean_precision_style = np.mean(all_precisions_style)\n",
        "mean_recall_style = np.mean(all_recalls_style)\n",
        "mean_precision_item =  np.mean(all_precisions_item)\n",
        "mean_recall_item = np.mean(all_recalls_item)\n",
        "\n",
        "  # 평균 성능 결과 출력\n",
        "print(\"Mean Precision Style:\", mean_precision_style)\n",
        "print(\"Mean Recall Style:\", mean_recall_style)\n",
        "print(\"Mean Precision Style:\", mean_precision_item)\n",
        "print(\"Mean Recall Style:\", mean_recall_item)\n",
        "'''\n",
        "\n",
        "    #print(\"Test Mean Average Precision (Style):\", test_mAP_style)\n",
        "    #print(\"Test Mean Average Precision (Item):\", test_mAP_item)\n",
        "    # print(f'Test loss style: {test_per_epoch_loss_style:.4f} | Test loss item: {test_per_epoch_loss_item:.4f} |  Test accuracy style: {test_per_epoch_accuracy_style:.4f} Test accuracy item: {test_per_epoch_accuracy_item:.4f}' + '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvjJ9LpQgFKP"
      },
      "source": [
        "##VERSION 4 Freeze Initial Stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SZCtIV5gyHF"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "from torchvision.models.resnet import BasicBlock\n",
        "from torch.utils import model_zoo\n",
        "\n",
        "\n",
        "class MixStyle(nn.Module):\n",
        "    \"\"\"MixStyle.\n",
        "\n",
        "    Reference:\n",
        "      Zhou et al. Domain Generalization with MixStyle. ICLR 2021.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5, alpha=0.1, eps=1e-6, mix='random'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          p (float): probability of using MixStyle.\n",
        "          alpha (float): parameter of the Beta distribution.\n",
        "          eps (float): scaling parameter to avoid numerical issues.\n",
        "          mix (str): how to mix.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        self.beta = torch.distributions.Beta(alpha, alpha)\n",
        "        self.eps = eps\n",
        "        self.alpha = alpha\n",
        "        self.mix = mix\n",
        "        self._activated = True\n",
        "        self.lmda = None\n",
        "        self.perm = None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'MixStyle(p={self.p}, alpha={self.alpha}, eps={self.eps}, mix={self.mix})'\n",
        "\n",
        "    def set_activation_status(self, status=True):\n",
        "        self._activated = status\n",
        "\n",
        "    def update_mix_method(self, mix='random'):\n",
        "        self.mix = mix\n",
        "\n",
        "    def forward(self, x , perm):\n",
        "\n",
        "        if not self.training or not self._activated:\n",
        "            return x\n",
        "\n",
        "        if random.random() > self.p:\n",
        "            self.lmda = torch.full((x.size()[0], 1, 1, 1), 1)\n",
        "            #self.perm = torch.arange(1, 65)\n",
        "            return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        B = x.size(0)\n",
        "\n",
        "        mu = x.mean(dim=[2, 3], keepdim=True)\n",
        "        var = x.var(dim=[2, 3], keepdim=True)\n",
        "        sig = (var + self.eps).sqrt()\n",
        "        mu, sig = mu.detach(), sig.detach()\n",
        "        x_normed = (x-mu) / sig\n",
        "\n",
        "\n",
        "        sampled = self.beta.sample()\n",
        "\n",
        "        if sampled < 0.5 :\n",
        "          sampled = 1- sampled\n",
        "        lmda = torch.full((x.size()[0], 1, 1, 1), sampled)\n",
        "\n",
        "        self.lmda = lmda.to(x.device)\n",
        "\n",
        "        #lmda = self.beta.sample((B, 1, 1, 1))\n",
        "        #lmda = torch.tensor(lmda)\n",
        "        # for i in range(B):\n",
        "        #   if lmda[i].item() < 0.5:\n",
        "        #     lmda[i] = 1 - lmda[i]\n",
        "\n",
        "        #print(f\"lmda : {lmda}\")\n",
        "\n",
        "\n",
        "        mu2, sig2 = mu[perm], sig[perm]\n",
        "        mu_mix = mu*self.lmda + mu2 * (1-self.lmda)\n",
        "        sig_mix = sig*self.lmda + sig2 * (1-self.lmda)\n",
        "\n",
        "        return x_normed*sig_mix + mu_mix\n",
        "\n",
        "\n",
        "from __future__ import division, absolute_import\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    'resnet18',\n",
        "    'resnet50'\n",
        "]\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18':\n",
        "    'https://download.pytorch.org/models/resnet18-f37072fd.pth',\n",
        "    'resnet50':\n",
        "    'https://download.pytorch.org/models/resnet50-11ad3fa6.pth'\n",
        "}\n",
        "\n",
        "'''\n",
        "Reference : https://pytorch.org/vision/main/_modules/torchvision/models/resnet.html\n",
        "'''\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation\n",
        "    )\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes, out_planes, kernel_size=1, stride=stride, bias=False\n",
        "    )\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride=1,\n",
        "        downsample=None,\n",
        "        groups=1,\n",
        "        base_width=64,\n",
        "        dilation=1,\n",
        "        norm_layer=None\n",
        "    ):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError(\n",
        "                'BasicBlock only supports groups=1 and base_width=64'\n",
        "            )\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\n",
        "                \"Dilation > 1 not supported in BasicBlock\"\n",
        "            )\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\" https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride = 1,\n",
        "        downsample = None,\n",
        "        groups = 1,\n",
        "        base_width = 64,\n",
        "        dilation = 1,\n",
        "        norm_layer = None,\n",
        "    ):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.0)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        loss,\n",
        "        block,\n",
        "        layers,\n",
        "        zero_init_residual=False,\n",
        "        groups=1,\n",
        "        width_per_group=64,\n",
        "        replace_stride_with_dilation=None,\n",
        "        norm_layer=None,\n",
        "        last_stride=2,\n",
        "        fc_dims=None,\n",
        "        dropout_p=None,\n",
        "        mixstyle_layers=[],\n",
        "        mixstyle_p=0.5,\n",
        "        mixstyle_alpha=0.1,\n",
        "    ):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "        self.loss = loss\n",
        "        self.feature_dim = 512 * block.expansion\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\n",
        "\n",
        "                format(replace_stride_with_dilation)\n",
        "            )\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False\n",
        "        )\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(\n",
        "            block,\n",
        "            128,\n",
        "            layers[1],\n",
        "            stride=2,\n",
        "            dilate=replace_stride_with_dilation[0]\n",
        "        )\n",
        "        self.layer3 = self._make_layer(\n",
        "            block,\n",
        "            256,\n",
        "            layers[2],\n",
        "            stride=2,\n",
        "            dilate=replace_stride_with_dilation[1]\n",
        "        )\n",
        "        self.layer4 = self._make_layer(\n",
        "            block,\n",
        "            512,\n",
        "            layers[3],\n",
        "            stride=last_stride,\n",
        "            dilate=replace_stride_with_dilation[2]\n",
        "        )\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "\n",
        "        self.mixstyle = MixStyle(p=mixstyle_p, alpha=mixstyle_alpha, mix='random')\n",
        "        self.mixstyle_layers = mixstyle_layers\n",
        "\n",
        "        self.lmda_1 = None\n",
        "\n",
        "        self.lmda_2 = None\n",
        "\n",
        "        self.lmda_3 = None\n",
        "\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n",
        "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
        "                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes, planes, stride, downsample, self.groups,\n",
        "                self.base_width, previous_dilation, norm_layer\n",
        "            )\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, perm_1, perm_2, perm_3):\n",
        "        for param in self.conv1.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.bn1.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.relu.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.maxpool.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.layer1.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        if 'layer1' in self.mixstyle_layers:\n",
        "            x = self.mixstyle(x, perm_1)\n",
        "            #print(self.mixstyle.lmda_1)\n",
        "            #print(self.mixstyle.perm_1)\n",
        "            self.lmda_1 = self.mixstyle.lmda\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        if 'layer2' in self.mixstyle_layers:\n",
        "            x = self.mixstyle(x, perm_2)\n",
        "            self.lmda_2 = self.mixstyle.lmda\n",
        "\n",
        "        x = self.layer3(x)\n",
        "        if 'layer3' in self.mixstyle_layers:\n",
        "            x = self.mixstyle(x, perm_3)\n",
        "            self.lmda_3 = self.mixstyle.lmda\n",
        "\n",
        "        f = self.layer4(x)\n",
        "        v = self.global_avgpool(f)\n",
        "        v = v.view(v.size(0), -1)\n",
        "\n",
        "        y = self.fc(v)\n",
        "\n",
        "        return y\n",
        "\n",
        "class MultiTaskNet(nn.Module):\n",
        "    def __init__(self, net, backbone, num_classes_style = 4, num_classes_item = 65, pretrained=True ):\n",
        "        super(MultiTaskNet, self).__init__()\n",
        "        self.net = net\n",
        "        self.net.fc = nn.Identity()\n",
        "\n",
        "        if backbone == \"resnet18\":\n",
        "          self.net.fc1 = nn.Sequential(OrderedDict([('linear', nn.Linear(512,512)),('relu1', nn.ReLU()),('dropout1', nn.Dropout(0.5)),('final', nn.Linear(512, 4))]))\n",
        "          self.net.fc2 = nn.Sequential(OrderedDict([('linear', nn.Linear(512,512)),('relu1', nn.ReLU()),('dropout1', nn.Dropout(0.5)),('final', nn.Linear(512, 65))]))\n",
        "        elif backbone == \"resnet50\":\n",
        "          self.net.fc1 = nn.Sequential(OrderedDict([('linear', nn.Linear(2048,2048)),('relu1', nn.ReLU()),('dropout1', nn.Dropout(0.5)),('final', nn.Linear(2048, 4))]))\n",
        "          self.net.fc2 = nn.Sequential(OrderedDict([('linear', nn.Linear(2048,2048)),('relu1', nn.ReLU()),('dropout1', nn.Dropout(0.5)),('final', nn.Linear(2048, 65))]))\n",
        "\n",
        "\n",
        "    def forward(self, x, perm_1, perm_2, perm_3):\n",
        "        x = self.net(x, perm_1, perm_2, perm_3)\n",
        "        style_head = self.net.fc1(x)\n",
        "        item_head = self.net.fc2(x)\n",
        "        #print(self.net.lmda_1)\n",
        "        #print(self.net.perm_1)\n",
        "\n",
        "        #print(self.net.mixstyle.lmda)\n",
        "        #print(self.net.mixstyle.perm)\n",
        "\n",
        "        return style_head, item_head\n",
        "\n",
        "def init_pretrained_weights(model, model_url):\n",
        "    \"\"\"Initializes model with pretrained weights.\n",
        "\n",
        "    Layers that don't match with pretrained layers in name or size are kept unchanged.\n",
        "    \"\"\"\n",
        "    pretrain_dict = model_zoo.load_url(model_url)\n",
        "    model_dict = model.state_dict()\n",
        "    pretrain_dict = {\n",
        "        k: v\n",
        "        for k, v in pretrain_dict.items()\n",
        "        if k in model_dict and model_dict[k].size() == v.size()\n",
        "    }\n",
        "    model_dict.update(pretrain_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "\n",
        "\n",
        "def resnet18(num_classes=1000, loss='softmax', pretrained=True, **kwargs):\n",
        "    model = ResNet(\n",
        "        num_classes=num_classes,\n",
        "        loss=loss,\n",
        "        block=BasicBlock,\n",
        "        layers=[2, 2, 2, 2],\n",
        "        last_stride=2,\n",
        "        fc_dims=None,\n",
        "        dropout_p=None,\n",
        "        mixstyle_layers=['layer1', 'layer2', 'layer3'],\n",
        "        mixstyle_alpha=0.1,\n",
        "        **kwargs\n",
        "    )\n",
        "    if pretrained:\n",
        "        init_pretrained_weights(model, model_urls['resnet18'])\n",
        "    return model\n",
        "\n",
        "def resnet50(num_classes=1000, loss='softmax', pretrained=True, **kwargs):\n",
        "    model = ResNet(\n",
        "        num_classes=num_classes,\n",
        "        loss=loss,\n",
        "        block=Bottleneck,\n",
        "        layers=[3, 4, 6, 3],\n",
        "        last_stride=2,\n",
        "        fc_dims=None,\n",
        "        dropout_p=None,\n",
        "        mixstyle_layers=['layer1', 'layer2'],\n",
        "        mixstyle_alpha=0.1,\n",
        "        **kwargs\n",
        "    )\n",
        "    if pretrained:\n",
        "        init_pretrained_weights(model, model_urls['resnet50'])\n",
        "    return model\n",
        "\n",
        "backbone = [\"resnet18\" , \"resnet50\"]\n",
        "# 특정 레이어를 freeze\n",
        "freeze_layers = ['layer1', 'layer2']  # freeze하고 싶은 레이어 명세\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "for name, child in model_pretrain.named_children():\n",
        "    if name in freeze_layers:\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "# 특정 레이어의 원하는 파라미터를 선택적으로 freeze\n",
        "for name, child in model_pretrain.named_children():\n",
        "    if name == 'layer1':\n",
        "        for param_name, param in child.named_parameters():\n",
        "            # 특정 조건에 따라 파라미터 freeze\n",
        "            if 'bn' in param_name:  # 'bn'이 포함된 파라미터만 선택\n",
        "                param.requires_grad = False\n",
        "'''\n",
        "\n",
        "model_pretrained_ = resnet50()\n",
        "model_mixstyle_ = MultiTaskNet(model_pretrained_, backbone = \"resnet50\")\n",
        "model_to_train = model_mixstyle_.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model_to_train.parameters(), lr = 0.0004, weight_decay = 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ONebUU1yg0YL",
        "outputId": "168bbeb7-f7c4-476b-b4c3-d31a7e259590"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n    data = data.to(device)\\n    labels = labels.to(device)\\n    output = model(data)\\n    _, pred_labels = output.max(-1)\\n    correct = (pred_labels == labels)\\n    return correct.cpu().detach().numpy().tolist()\\n'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "Train and Test function\n",
        "'''\n",
        "def training_batch(train_loader, model,criterion, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    training_loss = []\n",
        "    training_style_loss = []\n",
        "    training_item_loss = []\n",
        "\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "      input = data[\"image\"].to(device)\n",
        "      style_label_ = data[\"style\"]\n",
        "      style_label = style_label_.to(device)\n",
        "      item_label = data[\"item\"].to(device)\n",
        "\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      rand_index_1 = torch.randperm(input.size()[0]).cuda()\n",
        "      rand_index_2 = torch.randperm(input.size()[0]).cuda()\n",
        "      rand_index_3 = torch.randperm(input.size()[0]).cuda()\n",
        "\n",
        "      style_output, item_output = model(input, rand_index_1, rand_index_2, rand_index_3)#, rand_index_3)\n",
        "\n",
        "      #####labelling#####\n",
        "\n",
        "      lmda_1 = float(model.net.lmda_1[0])\n",
        "      lmda_2 = float(model.net.lmda_2[0])\n",
        "      #lmda_3 = float(model.net.lmda_3[0])\n",
        "\n",
        "\n",
        "      style_label_a = style_label[rand_index_1]\n",
        "      style_label_b = style_label[rand_index_2]\n",
        "      #style_label_c = style_label[rand_index_3]\n",
        "\n",
        "\n",
        "      loss_style_temp1 = criterion(style_output, style_label_a)\n",
        "      loss_style_temp2 = criterion(style_output, style_label_b)\n",
        "      #loss_style_temp3 = criterion(style_output, style_label_c)\n",
        "\n",
        "      loss_style_original = criterion(style_output, style_label)\n",
        "\n",
        "      loss_style = lmda_2 * (lmda_1 * loss_style_original + (1 - lmda_1) * loss_style_temp1) + (1 - lmda_2) * loss_style_temp2\n",
        "      #loss_style = lmda_3 * (lmda_2 * (lmda_1 * loss_style_original + (1 - lmda_1) * loss_style_temp1) + (1 - lmda_2) * loss_style_temp2) + (1 - lmda_3) * loss_style_temp3\n",
        "\n",
        "\n",
        "      loss_item = criterion(item_output, item_label)\n",
        "\n",
        "      loss = loss_style + loss_item\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      training_loss.append(loss.detach().item())\n",
        "      training_style_loss.append(loss_style_original.detach().item())\n",
        "      training_item_loss.append(loss_item.detach().item())\n",
        "\n",
        "      # print(f\"lmda_1 : {lmda_1}\")\n",
        "      # print(f\"lmda_2 : {lmda_2}\")\n",
        "      # print(f\"lmda_3 : {lmda_3}\")\n",
        "\n",
        "\n",
        "    return training_style_loss, training_item_loss\n",
        "\n",
        "\n",
        "\n",
        "def test_batch(data, model, criterion):\n",
        "    model.eval()\n",
        "    input = data[\"image\"].to(device)\n",
        "    style_label = data[\"style\"].to(device)\n",
        "    item_label = data[\"item\"].to(device)\n",
        "\n",
        "    style_output, item_output = model(input, None, None, None)\n",
        "    loss_1 = criterion(style_output, style_label)\n",
        "    loss_2 = criterion(item_output, item_label)\n",
        "\n",
        "    total_loss = loss_1 + loss_2\n",
        "    return  loss_1.item(), loss_2.item()\n",
        "\n",
        "def accuracy(data, model):\n",
        "    model.eval()\n",
        "\n",
        "    input = data[\"image\"].to(device)\n",
        "    style_label = data[\"style\"].to(device)\n",
        "    item_label = data[\"item\"].to(device)\n",
        "\n",
        "    style_output, item_output = model(input, None, None, None)\n",
        "\n",
        "    _, pred_label_style = style_output.max(-1)\n",
        "    _, pred_label_item = item_output.max(-1)\n",
        "\n",
        "    correct_style = (pred_label_style == style_label)\n",
        "    correct_item = (pred_label_item == item_label)\n",
        "\n",
        "    return correct_style.cpu().detach().numpy().tolist(), correct_item.cpu().detach().numpy().tolist()\n",
        "\n",
        "\n",
        "'''\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    _, pred_labels = output.max(-1)\n",
        "    correct = (pred_labels == labels)\n",
        "    return correct.cpu().detach().numpy().tolist()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA2K32r5g2rs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchmetrics.classification import MulticlassAveragePrecision\n",
        "\n",
        "def classification_map(data_loader, model):\n",
        "    # MulticlassAveragePrecision 메트릭스 초기화\n",
        "    map_metric_style = MulticlassAveragePrecision(num_classes=4, average=\"macro\")\n",
        "    map_metric_item = MulticlassAveragePrecision(num_classes=65, average=\"macro\")\n",
        "\n",
        "    device = next(model.parameters()).device  # 모델의 디바이스 확인\n",
        "\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "\n",
        "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
        "        for data in data_loader:\n",
        "            input = data[\"image\"].to(device)\n",
        "            style_true = data[\"style\"].to(device)\n",
        "            item_true = data[\"item\"].to(device)\n",
        "\n",
        "            style_pred, item_pred = model(input,None, None, None)\n",
        "\n",
        "            # MulticlassAveragePrecision 클래스는 확률을 입력으로 받으므로 softmax를 적용하여 확률로 변환\n",
        "            style_pred_prob = torch.softmax(style_pred, dim=1)\n",
        "            item_pred_prob = torch.softmax(item_pred, dim=1)\n",
        "\n",
        "            # 메트릭스 업데이트\n",
        "            map_metric_style.update(style_pred_prob, style_true)\n",
        "            map_metric_item.update(item_pred_prob, item_true)\n",
        "\n",
        "    # 평균 정밀도 계산\n",
        "    mAP_style = map_metric_style.compute()\n",
        "    mAP_item = map_metric_item.compute()\n",
        "\n",
        "    return mAP_style, mAP_item\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SEtjLO8Ag45C",
        "outputId": "d2a11e87-a9b3-414b-f3f6-b815e222d785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Respective Execution time : 501.2269687652588sec    1 epochs\n",
            "Epoch: 1/20\t| Training loss style: 0.6807 | Training loss item : 1.6287\n",
            " Training accuracy style: 0.8267 |Training accuracy item: 0.8177 Test loss style: 0.5192 | Test loss item: 0.9019 |  Test accuracy style: 0.7955 Test accuracy item: 0.7579\n",
            "Test Mean Average Precision (Style): 0.850688099861145 | Test Mean Average Precision (Item): 0.8350029587745667\n",
            "\n",
            "Respective Execution time : 491.0164384841919sec    2 epochs\n",
            "Epoch: 2/20\t| Training loss style: 0.4637 | Training loss item : 0.6750\n",
            " Training accuracy style: 0.8506 |Training accuracy item: 0.8817 Test loss style: 0.4957 | Test loss item: 0.7870 |  Test accuracy style: 0.8086 Test accuracy item: 0.7834\n",
            "Test Mean Average Precision (Style): 0.8752895593643188 | Test Mean Average Precision (Item): 0.8682410717010498\n",
            "\n",
            "Respective Execution time : 494.1997892856598sec    3 epochs\n",
            "Epoch: 3/20\t| Training loss style: 0.3642 | Training loss item : 0.4499\n",
            " Training accuracy style: 0.8976 |Training accuracy item: 0.9301 Test loss style: 0.4790 | Test loss item: 0.7486 |  Test accuracy style: 0.8145 Test accuracy item: 0.7999\n",
            "Test Mean Average Precision (Style): 0.882710337638855 | Test Mean Average Precision (Item): 0.881194531917572\n",
            "\n",
            "Respective Execution time : 497.5623688697815sec    4 epochs\n",
            "Epoch: 4/20\t| Training loss style: 0.3097 | Training loss item : 0.3301\n",
            " Training accuracy style: 0.9188 |Training accuracy item: 0.9550 Test loss style: 0.5109 | Test loss item: 0.6723 |  Test accuracy style: 0.8291 Test accuracy item: 0.8207\n",
            "Test Mean Average Precision (Style): 0.8835462331771851 | Test Mean Average Precision (Item): 0.89410001039505\n",
            "\n",
            "Respective Execution time : 491.50318336486816sec    5 epochs\n",
            "Epoch: 5/20\t| Training loss style: 0.2619 | Training loss item : 0.2291\n",
            " Training accuracy style: 0.9492 |Training accuracy item: 0.9603 Test loss style: 0.3992 | Test loss item: 0.7135 |  Test accuracy style: 0.8519 Test accuracy item: 0.8136\n",
            "Test Mean Average Precision (Style): 0.9010100364685059 | Test Mean Average Precision (Item): 0.8935070633888245\n",
            "\n",
            "Respective Execution time : 492.5192754268646sec    6 epochs\n",
            "Epoch: 6/20\t| Training loss style: 0.2218 | Training loss item : 0.2263\n",
            " Training accuracy style: 0.9598 |Training accuracy item: 0.9661 Test loss style: 0.4158 | Test loss item: 0.7357 |  Test accuracy style: 0.8562 Test accuracy item: 0.8179\n",
            "Test Mean Average Precision (Style): 0.9043945670127869 | Test Mean Average Precision (Item): 0.8902438879013062\n",
            "\n",
            "Respective Execution time : 490.34984254837036sec    7 epochs\n",
            "Epoch: 7/20\t| Training loss style: 0.1977 | Training loss item : 0.1715\n",
            " Training accuracy style: 0.9467 |Training accuracy item: 0.9737 Test loss style: 0.4866 | Test loss item: 0.7725 |  Test accuracy style: 0.8341 Test accuracy item: 0.8170\n",
            "Test Mean Average Precision (Style): 0.8913588523864746 | Test Mean Average Precision (Item): 0.889916181564331\n",
            "\n",
            "Respective Execution time : 490.5528540611267sec    8 epochs\n",
            "Epoch: 8/20\t| Training loss style: 0.1400 | Training loss item : 0.0691\n",
            " Training accuracy style: 0.9922 |Training accuracy item: 0.9917 Test loss style: 0.3656 | Test loss item: 0.5856 |  Test accuracy style: 0.8674 Test accuracy item: 0.8593\n",
            "Test Mean Average Precision (Style): 0.91705322265625 | Test Mean Average Precision (Item): 0.9184128046035767\n",
            "\n",
            "Respective Execution time : 487.4441683292389sec    9 epochs\n",
            "Epoch: 9/20\t| Training loss style: 0.0989 | Training loss item : 0.0444\n",
            " Training accuracy style: 0.9951 |Training accuracy item: 0.9926 Test loss style: 0.3781 | Test loss item: 0.5907 |  Test accuracy style: 0.8711 Test accuracy item: 0.8615\n",
            "Test Mean Average Precision (Style): 0.9140263795852661 | Test Mean Average Precision (Item): 0.9187812805175781\n",
            "\n",
            "Respective Execution time : 489.52190256118774sec    10 epochs\n",
            "Epoch: 10/20\t| Training loss style: 0.0871 | Training loss item : 0.0352\n",
            " Training accuracy style: 0.9964 |Training accuracy item: 0.9931 Test loss style: 0.3968 | Test loss item: 0.6389 |  Test accuracy style: 0.8749 Test accuracy item: 0.8596\n",
            "Test Mean Average Precision (Style): 0.9191380739212036 | Test Mean Average Precision (Item): 0.917689859867096\n",
            "\n",
            "Respective Execution time : 491.4185528755188sec    11 epochs\n",
            "Epoch: 11/20\t| Training loss style: 0.0878 | Training loss item : 0.0358\n",
            " Training accuracy style: 0.9977 |Training accuracy item: 0.9934 Test loss style: 0.3836 | Test loss item: 0.5951 |  Test accuracy style: 0.8721 Test accuracy item: 0.8631\n",
            "Test Mean Average Precision (Style): 0.9164997339248657 | Test Mean Average Precision (Item): 0.9195314645767212\n",
            "\n",
            "Respective Execution time : 486.8222551345825sec    12 epochs\n",
            "Epoch: 12/20\t| Training loss style: 0.0852 | Training loss item : 0.0353\n",
            " Training accuracy style: 0.9971 |Training accuracy item: 0.9933 Test loss style: 0.3759 | Test loss item: 0.6241 |  Test accuracy style: 0.8764 Test accuracy item: 0.8631\n",
            "Test Mean Average Precision (Style): 0.9183622598648071 | Test Mean Average Precision (Item): 0.9180113673210144\n",
            "\n",
            "Respective Execution time : 489.87779784202576sec    13 epochs\n",
            "Epoch: 13/20\t| Training loss style: 0.0785 | Training loss item : 0.0321\n",
            " Training accuracy style: 0.9990 |Training accuracy item: 0.9943 Test loss style: 0.3687 | Test loss item: 0.6469 |  Test accuracy style: 0.8786 Test accuracy item: 0.8674\n",
            "Test Mean Average Precision (Style): 0.9176380634307861 | Test Mean Average Precision (Item): 0.9180463552474976\n",
            "\n",
            "Respective Execution time : 492.2764024734497sec    14 epochs\n",
            "Epoch: 14/20\t| Training loss style: 0.0795 | Training loss item : 0.0331\n",
            " Training accuracy style: 0.9989 |Training accuracy item: 0.9935 Test loss style: 0.3493 | Test loss item: 0.6143 |  Test accuracy style: 0.8808 Test accuracy item: 0.8606\n",
            "Test Mean Average Precision (Style): 0.9220355749130249 | Test Mean Average Precision (Item): 0.9171358346939087\n",
            "\n",
            "Respective Execution time : 489.0278036594391sec    15 epochs\n",
            "Epoch: 15/20\t| Training loss style: 0.0699 | Training loss item : 0.0252\n",
            " Training accuracy style: 0.9988 |Training accuracy item: 0.9948 Test loss style: 0.3483 | Test loss item: 0.6007 |  Test accuracy style: 0.8796 Test accuracy item: 0.8643\n",
            "Test Mean Average Precision (Style): 0.9258527755737305 | Test Mean Average Precision (Item): 0.9200786352157593\n",
            "\n",
            "Respective Execution time : 494.2630112171173sec    16 epochs\n",
            "Epoch: 16/20\t| Training loss style: 0.0718 | Training loss item : 0.0205\n",
            " Training accuracy style: 0.9990 |Training accuracy item: 0.9945 Test loss style: 0.3524 | Test loss item: 0.6089 |  Test accuracy style: 0.8820 Test accuracy item: 0.8674\n",
            "Test Mean Average Precision (Style): 0.9241020679473877 | Test Mean Average Precision (Item): 0.9209194779396057\n",
            "\n",
            "Respective Execution time : 496.24282813072205sec    17 epochs\n",
            "Epoch: 17/20\t| Training loss style: 0.0442 | Training loss item : 0.0178\n",
            " Training accuracy style: 0.9988 |Training accuracy item: 0.9949 Test loss style: 0.3688 | Test loss item: 0.6758 |  Test accuracy style: 0.8830 Test accuracy item: 0.8665\n",
            "Test Mean Average Precision (Style): 0.923525333404541 | Test Mean Average Precision (Item): 0.9196972846984863\n",
            "\n",
            "Respective Execution time : 496.8513009548187sec    18 epochs\n",
            "Epoch: 18/20\t| Training loss style: 0.0571 | Training loss item : 0.0175\n",
            " Training accuracy style: 0.9992 |Training accuracy item: 0.9953 Test loss style: 0.3574 | Test loss item: 0.6346 |  Test accuracy style: 0.8805 Test accuracy item: 0.8683\n",
            "Test Mean Average Precision (Style): 0.9231641292572021 | Test Mean Average Precision (Item): 0.921006441116333\n",
            "\n",
            "Respective Execution time : 492.28396701812744sec    19 epochs\n",
            "Epoch: 19/20\t| Training loss style: 0.0576 | Training loss item : 0.0175\n",
            " Training accuracy style: 0.9988 |Training accuracy item: 0.9942 Test loss style: 0.3474 | Test loss item: 0.6095 |  Test accuracy style: 0.8827 Test accuracy item: 0.8702\n",
            "Test Mean Average Precision (Style): 0.9234826564788818 | Test Mean Average Precision (Item): 0.9229506850242615\n",
            "\n",
            "Respective Execution time : 497.7924063205719sec    20 epochs\n",
            "Epoch: 20/20\t| Training loss style: 0.0619 | Training loss item : 0.0158\n",
            " Training accuracy style: 0.9991 |Training accuracy item: 0.9935 Test loss style: 0.3540 | Test loss item: 0.6444 |  Test accuracy style: 0.8814 Test accuracy item: 0.8690\n",
            "Test Mean Average Precision (Style): 0.9240859746932983 | Test Mean Average Precision (Item): 0.9217082262039185\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nmean_precision_style = np.mean(all_precisions_style)\\nmean_recall_style = np.mean(all_recalls_style)\\nmean_precision_item =  np.mean(all_precisions_item)\\nmean_recall_item = np.mean(all_recalls_item)\\n\\n  # 평균 성능 결과 출력\\nprint(\"Mean Precision Style:\", mean_precision_style)\\nprint(\"Mean Recall Style:\", mean_recall_style)\\nprint(\"Mean Precision Style:\", mean_precision_item)\\nprint(\"Mean Recall Style:\", mean_recall_item)\\n'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "Actual Training\n",
        "'''\n",
        "\n",
        "n_epochs = 20\n",
        "scheduler = StepLR(optimizer, step_size=7, gamma=0.3)\n",
        "\n",
        "train_loss_style, test_loss_style = [], []\n",
        "train_loss_item, test_loss_item = [], []\n",
        "train_accuracy_style, test_accuracy_style = [], []\n",
        "train_accuracy_item, test_accuracy_item = [], []\n",
        "all_precisions_style = []\n",
        "all_recalls_style = []\n",
        "all_precisions_item = []\n",
        "all_recalls_item = []\n",
        "\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_losses_style, train_losses_item = [], []\n",
        "    test_losses_style,test_losses_item = [],[]\n",
        "\n",
        "    train_accuracies_style, train_accuracies_item = [],[]\n",
        "    test_accuracies_style, test_accuracies_item = [], []\n",
        "\n",
        "\n",
        "    time1 = time.time()\n",
        "    train_losses_style, train_losses_item = training_batch(train_dataloader, model_to_train, criterion, optimizer)\n",
        "\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        train_batch_correct_style, train_batch_correct_item = accuracy(data, model_to_train)\n",
        "        train_accuracies_style.extend(train_batch_correct_style)\n",
        "        train_accuracies_item.extend(train_batch_correct_item)\n",
        "\n",
        "\n",
        "    train_per_epoch_loss_style = np.array(train_losses_style).mean()\n",
        "    train_per_epoch_loss_item = np.array(train_losses_item).mean()\n",
        "    train_per_epoch_accuracy_style = np.mean(train_accuracies_style)\n",
        "    train_per_epoch_accuracy_item = np.mean(train_accuracies_item)\n",
        "\n",
        "    #train_mAP_style, train_mAP_item = mean_average_precision(train_dataloader, model_to_train)\n",
        "    #print(\"Test Mean Average Precision (Style):\", train_mAP_style)\n",
        "    #print(\"Test Mean Average Precision (Item):\", train_mAP_item)\n",
        "\n",
        "    time2 = time.time()\n",
        "    print(\"Respective Execution time : \" + str(time2 - time1) + \"sec    \" + str(epoch+1) + \" epochs\")\n",
        "\n",
        "    learning_rate = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #   for i, data in enumerate(val_dataloader):\n",
        "    #       val_batch_correct_style,val_batch_correct_item = accuracy(data, model_to_train)\n",
        "    #       val_accuracies_style.extend(val_batch_correct_style)\n",
        "    #       val_accuracies_item.extend(val_batch_correct_item)\n",
        "\n",
        "    #   val_per_epoch_accuracy_style = np.mean(val_accuracies_style)\n",
        "    #   val_per_epoch_accuracy_item = np.mean(val_accuracies_item)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for i, data in enumerate(test_dataloader):\n",
        "        test_batch_loss_style, test_batch_loss_item = test_batch(data, model_to_train, criterion)\n",
        "        test_losses_style.append(test_batch_loss_style)\n",
        "        test_losses_item.append(test_batch_loss_item)\n",
        "\n",
        "        test_batch_correct_style, test_batch_correct_item = accuracy(data , model_to_train)\n",
        "        test_accuracies_style.extend(test_batch_correct_style)\n",
        "        test_accuracies_item.extend(test_batch_correct_item)\n",
        "\n",
        "    test_mAP_style, test_mAP_item  = classification_map(test_dataloader, model_to_train)\n",
        "    #average_precision_style, average_recall_style, average_precision_item, average_recall_item = test_precision_recall( model_to_train, test_dataloader)\n",
        "\n",
        "    #all_precisions_style.append(average_precision_style)\n",
        "    #all_recalls_style.append(average_recall_style)\n",
        "\n",
        "    #all_precisions_item.append(average_precision_item)\n",
        "    #all_recalls_item.append(average_recall_item)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    test_per_epoch_loss_style = np.array(test_losses_style).mean()\n",
        "    test_per_epoch_loss_item = np.array(test_losses_item).mean()\n",
        "    test_per_epoch_accuracy_style = np.mean(test_accuracies_style)\n",
        "    test_per_epoch_accuracy_item = np.mean(test_accuracies_item)\n",
        "\n",
        "\n",
        "\n",
        "    # train, test\n",
        "    # train_loss_style.append(train_per_epoch_loss_style)\n",
        "    # train_accuracy_style.append(train_per_epoch_accuracy_style)\n",
        "\n",
        "    # train_loss_item.append(train_per_epoch_loss_item)\n",
        "    # train_accuracy_item.append(train_per_epoch_accuracy_item)\n",
        "\n",
        "    # test_loss_style.append(test_per_epoch_loss_style)\n",
        "    # test_accuracy_style.append(test_per_epoch_accuracy_style)\n",
        "\n",
        "    # test_loss_item.append(test_per_epoch_loss_item)\n",
        "    # test_accuracy_item.append(test_per_epoch_accuracy_item)\n",
        "\n",
        "\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}/{n_epochs}\\t| Training loss style: {train_per_epoch_loss_style:.4f} | Training loss item : {train_per_epoch_loss_item:.4f}')\n",
        "    print(f' Training accuracy style: {train_per_epoch_accuracy_style:.4f} |Training accuracy item: {train_per_epoch_accuracy_item:.4f} ', end='')\n",
        "    print(f'Test loss style: {test_per_epoch_loss_style:.4f} | Test loss item: {test_per_epoch_loss_item:.4f} |  Test accuracy style: {test_per_epoch_accuracy_style:.4f} Test accuracy item: {test_per_epoch_accuracy_item:.4f}')\n",
        "    print(f\"Test Mean Average Precision (Style): {test_mAP_style} | Test Mean Average Precision (Item): {test_mAP_item}\" + '\\n')\n",
        "    #print(f\"Test Mean Average Precision (Style): {average_precision_style} | Test Mean Average Precision (Item): {average_precision_item}\")\n",
        "    #print(f\"Test Mean Average Recall (Style): {average_recall_style} | Test Mean Average Recall (Item): {average_recall_item}\")\n",
        "\n",
        "\n",
        "'''\n",
        "mean_precision_style = np.mean(all_precisions_style)\n",
        "mean_recall_style = np.mean(all_recalls_style)\n",
        "mean_precision_item =  np.mean(all_precisions_item)\n",
        "mean_recall_item = np.mean(all_recalls_item)\n",
        "\n",
        "  # 평균 성능 결과 출력\n",
        "print(\"Mean Precision Style:\", mean_precision_style)\n",
        "print(\"Mean Recall Style:\", mean_recall_style)\n",
        "print(\"Mean Precision Style:\", mean_precision_item)\n",
        "print(\"Mean Recall Style:\", mean_recall_item)\n",
        "'''\n",
        "\n",
        "    #print(\"Test Mean Average Precision (Style):\", test_mAP_style)\n",
        "    #print(\"Test Mean Average Precision (Item):\", test_mAP_item)\n",
        "    # print(f'Test loss style: {test_per_epoch_loss_style:.4f} | Test loss item: {test_per_epoch_loss_item:.4f} |  Test accuracy style: {test_per_epoch_accuracy_style:.4f} Test accuracy item: {test_per_epoch_accuracy_item:.4f}' + '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0ymWcnBgfct"
      },
      "source": [
        "##VERSION 5 EfficientNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d41lu9mRKiTI"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "from torchvision.models.resnet import BasicBlock\n",
        "from torch.utils import model_zoo\n",
        "\n",
        "\n",
        "class MixStyle(nn.Module):\n",
        "    \"\"\"MixStyle.\n",
        "\n",
        "    Reference:\n",
        "      Zhou et al. Domain Generalization with MixStyle. ICLR 2021.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5, alpha=0.1, eps=1e-6, mix='random'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          p (float): probability of using MixStyle.\n",
        "          alpha (float): parameter of the Beta distribution.\n",
        "          eps (float): scaling parameter to avoid numerical issues.\n",
        "          mix (str): how to mix.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        self.beta = torch.distributions.Beta(alpha, alpha)\n",
        "        self.eps = eps\n",
        "        self.alpha = alpha\n",
        "        self.mix = mix\n",
        "        self._activated = True\n",
        "        self.lmda = None\n",
        "        self.perm = None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'MixStyle(p={self.p}, alpha={self.alpha}, eps={self.eps}, mix={self.mix})'\n",
        "\n",
        "    def set_activation_status(self, status=True):\n",
        "        self._activated = status\n",
        "\n",
        "    def update_mix_method(self, mix='random'):\n",
        "        self.mix = mix\n",
        "\n",
        "    def forward(self, x , perm):\n",
        "\n",
        "        if not self.training or not self._activated:\n",
        "            return x\n",
        "\n",
        "        if random.random() > self.p:\n",
        "            self.lmda = torch.full((x.size()[0], 1, 1, 1), 1)\n",
        "            #self.perm = torch.arange(1, 65)\n",
        "            return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        B = x.size(0)\n",
        "\n",
        "        mu = x.mean(dim=[2, 3], keepdim=True)\n",
        "        var = x.var(dim=[2, 3], keepdim=True)\n",
        "        sig = (var + self.eps).sqrt()\n",
        "        mu, sig = mu.detach(), sig.detach()\n",
        "        x_normed = (x-mu) / sig\n",
        "\n",
        "\n",
        "        sampled = self.beta.sample()\n",
        "\n",
        "        if sampled < 0.5 :\n",
        "          sampled = 1- sampled\n",
        "        lmda = torch.full((x.size()[0], 1, 1, 1), sampled)\n",
        "\n",
        "        self.lmda = lmda.to(x.device)\n",
        "\n",
        "        #lmda = self.beta.sample((B, 1, 1, 1))\n",
        "        #lmda = torch.tensor(lmda)\n",
        "        # for i in range(B):\n",
        "        #   if lmda[i].item() < 0.5:\n",
        "        #     lmda[i] = 1 - lmda[i]\n",
        "\n",
        "        #print(f\"lmda : {lmda}\")\n",
        "\n",
        "\n",
        "        mu2, sig2 = mu[perm], sig[perm]\n",
        "        mu_mix = mu*self.lmda + mu2 * (1-self.lmda)\n",
        "        sig_mix = sig*self.lmda + sig2 * (1-self.lmda)\n",
        "\n",
        "        return x_normed*sig_mix + mu_mix\n",
        "\n",
        "\n",
        "from __future__ import division, absolute_import\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    'resnet18',\n",
        "    'resnet50'\n",
        "]\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18':\n",
        "    'https://download.pytorch.org/models/resnet18-f37072fd.pth',\n",
        "    'resnet50':\n",
        "    'https://download.pytorch.org/models/resnet50-11ad3fa6.pth'\n",
        "}\n",
        "\n",
        "'''\n",
        "Reference : https://pytorch.org/vision/main/_modules/torchvision/models/resnet.html\n",
        "'''\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation\n",
        "    )\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes, out_planes, kernel_size=1, stride=stride, bias=False\n",
        "    )\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride=1,\n",
        "        downsample=None,\n",
        "        groups=1,\n",
        "        base_width=64,\n",
        "        dilation=1,\n",
        "        norm_layer=None\n",
        "    ):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError(\n",
        "                'BasicBlock only supports groups=1 and base_width=64'\n",
        "            )\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\n",
        "                \"Dilation > 1 not supported in BasicBlock\"\n",
        "            )\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\" https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride = 1,\n",
        "        downsample = None,\n",
        "        groups = 1,\n",
        "        base_width = 64,\n",
        "        dilation = 1,\n",
        "        norm_layer = None,\n",
        "    ):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.0)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        loss,\n",
        "        block,\n",
        "        layers,\n",
        "        zero_init_residual=False,\n",
        "        groups=1,\n",
        "        width_per_group=64,\n",
        "        replace_stride_with_dilation=None,\n",
        "        norm_layer=None,\n",
        "        last_stride=2,\n",
        "        fc_dims=None,\n",
        "        dropout_p=None,\n",
        "        mixstyle_layers=[],\n",
        "        mixstyle_p=0.5,\n",
        "        mixstyle_alpha=0.1,\n",
        "    ):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "        self.loss = loss\n",
        "        self.feature_dim = 512 * block.expansion\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\n",
        "\n",
        "                format(replace_stride_with_dilation)\n",
        "            )\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False\n",
        "        )\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(\n",
        "            block,\n",
        "            128,\n",
        "            layers[1],\n",
        "            stride=2,\n",
        "            dilate=replace_stride_with_dilation[0]\n",
        "        )\n",
        "        self.layer3 = self._make_layer(\n",
        "            block,\n",
        "            256,\n",
        "            layers[2],\n",
        "            stride=2,\n",
        "            dilate=replace_stride_with_dilation[1]\n",
        "        )\n",
        "        self.layer4 = self._make_layer(\n",
        "            block,\n",
        "            512,\n",
        "            layers[3],\n",
        "            stride=last_stride,\n",
        "            dilate=replace_stride_with_dilation[2]\n",
        "        )\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "\n",
        "        self.mixstyle = MixStyle(p=mixstyle_p, alpha=mixstyle_alpha, mix='random')\n",
        "        self.mixstyle_layers = mixstyle_layers\n",
        "\n",
        "        self.lmda_1 = None\n",
        "\n",
        "        self.lmda_2 = None\n",
        "\n",
        "        self.lmda_3 = None\n",
        "\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n",
        "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
        "                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes, planes, stride, downsample, self.groups,\n",
        "                self.base_width, previous_dilation, norm_layer\n",
        "            )\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, perm_1, perm_2, perm_3):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        if 'layer1' in self.mixstyle_layers:\n",
        "            x = self.mixstyle(x, perm_1)\n",
        "            #print(self.mixstyle.lmda_1)\n",
        "            #print(self.mixstyle.perm_1)\n",
        "            self.lmda_1 = self.mixstyle.lmda\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        if 'layer2' in self.mixstyle_layers:\n",
        "            x = self.mixstyle(x, perm_2)\n",
        "            self.lmda_2 = self.mixstyle.lmda\n",
        "\n",
        "        x = self.layer3(x)\n",
        "        if 'layer3' in self.mixstyle_layers:\n",
        "            x = self.mixstyle(x, perm_3)\n",
        "            self.lmda_3 = self.mixstyle.lmda\n",
        "\n",
        "        f = self.layer4(x)\n",
        "        v = self.global_avgpool(f)\n",
        "        v = v.view(v.size(0), -1)\n",
        "\n",
        "        y = self.fc(v)\n",
        "\n",
        "        return y\n",
        "\n",
        "class MultiTaskNet(nn.Module):\n",
        "    def __init__(self, net, backbone, num_classes_style = 4, num_classes_item = 65, pretrained=True ):\n",
        "        super(MultiTaskNet, self).__init__()\n",
        "        self.net = net\n",
        "        self.net.fc = nn.Identity()\n",
        "\n",
        "        if backbone == \"resnet18\":\n",
        "          self.net.fc1 = nn.Sequential(OrderedDict([('linear', nn.Linear(512,512)),('relu1', nn.ReLU()),('final', nn.Linear(512, 4))]))\n",
        "          self.net.fc2 = nn.Sequential(OrderedDict([('linear', nn.Linear(512,512)),('relu1', nn.ReLU()),('final', nn.Linear(512, 65))]))\n",
        "        elif backbone == \"resnet50\":\n",
        "          self.net.fc1 = nn.Sequential(OrderedDict([('linear', nn.Linear(2048,2048)),('relu1', nn.ReLU()),('final', nn.Linear(2048, 4))]))\n",
        "          self.net.fc2 = nn.Sequential(OrderedDict([('linear', nn.Linear(2048,2048)),('relu1', nn.ReLU()),('final', nn.Linear(2048, 65))]))\n",
        "\n",
        "\n",
        "    def forward(self, x, perm_1, perm_2, perm_3):\n",
        "        x = self.net(x, perm_1, perm_2, perm_3)\n",
        "        style_head = self.net.fc1(x)\n",
        "        item_head = self.net.fc2(x)\n",
        "        #print(self.net.lmda_1)\n",
        "        #print(self.net.perm_1)\n",
        "\n",
        "        #print(self.net.mixstyle.lmda)\n",
        "        #print(self.net.mixstyle.perm)\n",
        "\n",
        "        return style_head, item_head\n",
        "\n",
        "def init_pretrained_weights(model, model_url):\n",
        "    \"\"\"Initializes model with pretrained weights.\n",
        "\n",
        "    Layers that don't match with pretrained layers in name or size are kept unchanged.\n",
        "    \"\"\"\n",
        "    pretrain_dict = model_zoo.load_url(model_url)\n",
        "    model_dict = model.state_dict()\n",
        "    pretrain_dict = {\n",
        "        k: v\n",
        "        for k, v in pretrain_dict.items()\n",
        "        if k in model_dict and model_dict[k].size() == v.size()\n",
        "    }\n",
        "    model_dict.update(pretrain_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "\n",
        "\n",
        "def resnet18(num_classes=1000, loss='softmax', pretrained=True, **kwargs):\n",
        "    model = ResNet(\n",
        "        num_classes=num_classes,\n",
        "        loss=loss,\n",
        "        block=BasicBlock,\n",
        "        layers=[2, 2, 2, 2],\n",
        "        last_stride=2,\n",
        "        fc_dims=None,\n",
        "        dropout_p=None,\n",
        "        mixstyle_layers=['layer1', 'layer2', 'layer3'],\n",
        "        mixstyle_alpha=0.1,\n",
        "        **kwargs\n",
        "    )\n",
        "    if pretrained:\n",
        "        init_pretrained_weights(model, model_urls['resnet18'])\n",
        "    return model\n",
        "\n",
        "def resnet50(num_classes=1000, loss='softmax', pretrained=True, **kwargs):\n",
        "    model = ResNet(\n",
        "        num_classes=num_classes,\n",
        "        loss=loss,\n",
        "        block=Bottleneck,\n",
        "        layers=[3, 4, 6, 3],\n",
        "        last_stride=2,\n",
        "        fc_dims=None,\n",
        "        dropout_p=None,\n",
        "        mixstyle_layers=['layer1', 'layer2'],\n",
        "        mixstyle_alpha=0.1,\n",
        "        **kwargs\n",
        "    )\n",
        "    if pretrained:\n",
        "        init_pretrained_weights(model, model_urls['resnet50'])\n",
        "    return model\n",
        "\n",
        "backbone = [\"resnet18\" , \"resnet50\"]\n",
        "# 특정 레이어를 freeze\n",
        "freeze_layers = ['layer1', 'layer2']  # freeze하고 싶은 레이어 명세\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "for name, child in model_pretrain.named_children():\n",
        "    if name in freeze_layers:\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "# 특정 레이어의 원하는 파라미터를 선택적으로 freeze\n",
        "for name, child in model_pretrain.named_children():\n",
        "    if name == 'layer1':\n",
        "        for param_name, param in child.named_parameters():\n",
        "            # 특정 조건에 따라 파라미터 freeze\n",
        "            if 'bn' in param_name:  # 'bn'이 포함된 파라미터만 선택\n",
        "                param.requires_grad = False\n",
        "'''\n",
        "\n",
        "model_pretrained_ = resnet50()\n",
        "model_mixstyle_ = MultiTaskNet(model_pretrained_, backbone = \"resnet50\")\n",
        "model_to_train = model_mixstyle_.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model_to_train.parameters(), lr = 0.0004, weight_decay = 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M54-TvvJKps7"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Train and Test function\n",
        "'''\n",
        "def training_batch(train_loader, model,criterion, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    training_loss = []\n",
        "    training_style_loss = []\n",
        "    training_item_loss = []\n",
        "\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "      input = data[\"image\"].to(device)\n",
        "      style_label_ = data[\"style\"]\n",
        "      style_label = style_label_.to(device)\n",
        "      item_label = data[\"item\"].to(device)\n",
        "\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      rand_index_1 = torch.randperm(input.size()[0]).cuda()\n",
        "      rand_index_2 = torch.randperm(input.size()[0]).cuda()\n",
        "      rand_index_3 = torch.randperm(input.size()[0]).cuda()\n",
        "\n",
        "      style_output, item_output = model(input, rand_index_1, rand_index_2)#, rand_index_3)\n",
        "\n",
        "      #####labelling#####\n",
        "\n",
        "      lmda_1 = float(model.net.lmda_1[0])\n",
        "      lmda_2 = float(model.net.lmda_2[0])\n",
        "      lmda_3 = float(model.net.lmda_3[0])\n",
        "\n",
        "\n",
        "      style_label_a = style_label[rand_index_1]\n",
        "      style_label_b = style_label[rand_index_2]\n",
        "      style_label_c = style_label[rand_index_3]\n",
        "\n",
        "\n",
        "      loss_style_temp1 = criterion(style_output, style_label_a)\n",
        "      loss_style_temp2 = criterion(style_output, style_label_b)\n",
        "      #loss_style_temp3 = criterion(style_output, style_label_c)\n",
        "\n",
        "      loss_style_original = criterion(style_output, style_label)\n",
        "\n",
        "      loss_style = lmda_2 * (lmda_1 * loss_style_original + (1 - lmda_1) * loss_style_temp1) + (1 - lmda_2) * loss_style_temp2\n",
        "      #loss_style = lmda_3 * (lmda_2 * (lmda_1 * loss_style_original + (1 - lmda_1) * loss_style_temp1) + (1 - lmda_2) * loss_style_temp2) + (1 - lmda_3) * loss_style_temp3\n",
        "\n",
        "\n",
        "      loss_item = criterion(item_output, item_label)\n",
        "\n",
        "      loss = loss_style + loss_item\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      training_loss.append(loss.detach().item())\n",
        "      training_style_loss.append(loss_style_original.detach().item())\n",
        "      training_item_loss.append(loss_item.detach().item())\n",
        "\n",
        "      # print(f\"lmda_1 : {lmda_1}\")\n",
        "      # print(f\"lmda_2 : {lmda_2}\")\n",
        "      # print(f\"lmda_3 : {lmda_3}\")\n",
        "\n",
        "\n",
        "    return training_style_loss, training_item_loss\n",
        "\n",
        "\n",
        "\n",
        "def test_batch(data, model, criterion):\n",
        "    model.eval()\n",
        "    input = data[\"image\"].to(device)\n",
        "    style_label = data[\"style\"].to(device)\n",
        "    item_label = data[\"item\"].to(device)\n",
        "\n",
        "    style_output, item_output = model(input, None, None, None)\n",
        "    loss_1 = criterion(style_output, style_label)\n",
        "    loss_2 = criterion(item_output, item_label)\n",
        "\n",
        "    total_loss = loss_1 + loss_2\n",
        "    return  loss_1.item(), loss_2.item()\n",
        "\n",
        "def accuracy(data, model):\n",
        "    model.eval()\n",
        "\n",
        "    input = data[\"image\"].to(device)\n",
        "    style_label = data[\"style\"].to(device)\n",
        "    item_label = data[\"item\"].to(device)\n",
        "\n",
        "    style_output, item_output = model(input, None, None, None)\n",
        "\n",
        "    _, pred_label_style = style_output.max(-1)\n",
        "    _, pred_label_item = item_output.max(-1)\n",
        "\n",
        "    correct_style = (pred_label_style == style_label)\n",
        "    correct_item = (pred_label_item == item_label)\n",
        "\n",
        "    return correct_style.cpu().detach().numpy().tolist(), correct_item.cpu().detach().numpy().tolist()\n",
        "\n",
        "\n",
        "'''\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    _, pred_labels = output.max(-1)\n",
        "    correct = (pred_labels == labels)\n",
        "    return correct.cpu().detach().numpy().tolist()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgeMrm8FK0bI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchmetrics.classification import MulticlassAveragePrecision\n",
        "\n",
        "def classification_map(data_loader, model):\n",
        "    # MulticlassAveragePrecision 메트릭스 초기화\n",
        "    map_metric_style = MulticlassAveragePrecision(num_classes=4, average=\"macro\")\n",
        "    map_metric_item = MulticlassAveragePrecision(num_classes=65, average=\"macro\")\n",
        "\n",
        "    device = next(model.parameters()).device  # 모델의 디바이스 확인\n",
        "\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "\n",
        "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
        "        for data in data_loader:\n",
        "            input = data[\"image\"].to(device)\n",
        "            style_true = data[\"style\"].to(device)\n",
        "            item_true = data[\"item\"].to(device)\n",
        "\n",
        "            style_pred, item_pred = model(input,None, None, None)\n",
        "\n",
        "            # MulticlassAveragePrecision 클래스는 확률을 입력으로 받으므로 softmax를 적용하여 확률로 변환\n",
        "            style_pred_prob = torch.softmax(style_pred, dim=1)\n",
        "            item_pred_prob = torch.softmax(item_pred, dim=1)\n",
        "\n",
        "            # 메트릭스 업데이트\n",
        "            map_metric_style.update(style_pred_prob, style_true)\n",
        "            map_metric_item.update(item_pred_prob, item_true)\n",
        "\n",
        "    # 평균 정밀도 계산\n",
        "    mAP_style = map_metric_style.compute()\n",
        "    mAP_item = map_metric_item.compute()\n",
        "\n",
        "    return mAP_style, mAP_item\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXrBJZi2K3Sp"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Actual Training\n",
        "'''\n",
        "\n",
        "n_epochs = 10\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.9)\n",
        "\n",
        "train_loss_style, test_loss_style = [], []\n",
        "train_loss_item, test_loss_item = [], []\n",
        "train_accuracy_style, test_accuracy_style = [], []\n",
        "train_accuracy_item, test_accuracy_item = [], []\n",
        "all_precisions_style = []\n",
        "all_recalls_style = []\n",
        "all_precisions_item = []\n",
        "all_recalls_item = []\n",
        "\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_losses_style, train_losses_item = [], []\n",
        "    test_losses_style,test_losses_item = [],[]\n",
        "\n",
        "    train_accuracies_style, train_accuracies_item = [],[]\n",
        "    test_accuracies_style, test_accuracies_item = [], []\n",
        "\n",
        "\n",
        "    time1 = time.time()\n",
        "    train_losses_style, train_losses_item = training_batch(train_dataloader, model_to_train, criterion, optimizer)\n",
        "\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        train_batch_correct_style, train_batch_correct_item = accuracy(data, model_to_train)\n",
        "        train_accuracies_style.extend(train_batch_correct_style)\n",
        "        train_accuracies_item.extend(train_batch_correct_item)\n",
        "\n",
        "\n",
        "    train_per_epoch_loss_style = np.array(train_losses_style).mean()\n",
        "    train_per_epoch_loss_item = np.array(train_losses_item).mean()\n",
        "    train_per_epoch_accuracy_style = np.mean(train_accuracies_style)\n",
        "    train_per_epoch_accuracy_item = np.mean(train_accuracies_item)\n",
        "\n",
        "    #train_mAP_style, train_mAP_item = mean_average_precision(train_dataloader, model_to_train)\n",
        "    #print(\"Test Mean Average Precision (Style):\", train_mAP_style)\n",
        "    #print(\"Test Mean Average Precision (Item):\", train_mAP_item)\n",
        "\n",
        "    time2 = time.time()\n",
        "    print(\"Respective Execution time : \" + str(time2 - time1) + \"sec    \" + str(epoch+1) + \" epochs\")\n",
        "\n",
        "    learning_rate = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    if epoch == 7:\n",
        "      for param in model_to_train.conv1.parameters():\n",
        "        param.requires_grad = False\n",
        "      for param in model_to_train.bn1.parameters():\n",
        "        param.requires_grad = False\n",
        "      for param in model_to_train.relu.parameters():\n",
        "        param.requires_grad = False\n",
        "      for param in model_to_train.maxpool.parameters():\n",
        "        param.requires_grad = False\n",
        "      for param in model_to_train.layer1.parameters():\n",
        "        param.requires_grad = False\n",
        "    # with torch.no_grad():\n",
        "    #   for i, data in enumerate(val_dataloader):\n",
        "    #       val_batch_correct_style,val_batch_correct_item = accuracy(data, model_to_train)\n",
        "    #       val_accuracies_style.extend(val_batch_correct_style)\n",
        "    #       val_accuracies_item.extend(val_batch_correct_item)\n",
        "\n",
        "    #   val_per_epoch_accuracy_style = np.mean(val_accuracies_style)\n",
        "    #   val_per_epoch_accuracy_item = np.mean(val_accuracies_item)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for i, data in enumerate(test_dataloader):\n",
        "        test_batch_loss_style, test_batch_loss_item = test_batch(data, model_to_train, criterion)\n",
        "        test_losses_style.append(test_batch_loss_style)\n",
        "        test_losses_item.append(test_batch_loss_item)\n",
        "\n",
        "        test_batch_correct_style, test_batch_correct_item = accuracy(data , model_to_train)\n",
        "        test_accuracies_style.extend(test_batch_correct_style)\n",
        "        test_accuracies_item.extend(test_batch_correct_item)\n",
        "\n",
        "    test_mAP_style, test_mAP_item  = classification_map(test_dataloader, model_to_train)\n",
        "    #average_precision_style, average_recall_style, average_precision_item, average_recall_item = test_precision_recall( model_to_train, test_dataloader)\n",
        "\n",
        "    #all_precisions_style.append(average_precision_style)\n",
        "    #all_recalls_style.append(average_recall_style)\n",
        "\n",
        "    #all_precisions_item.append(average_precision_item)\n",
        "    #all_recalls_item.append(average_recall_item)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    test_per_epoch_loss_style = np.array(test_losses_style).mean()\n",
        "    test_per_epoch_loss_item = np.array(test_losses_item).mean()\n",
        "    test_per_epoch_accuracy_style = np.mean(test_accuracies_style)\n",
        "    test_per_epoch_accuracy_item = np.mean(test_accuracies_item)\n",
        "\n",
        "\n",
        "\n",
        "    # train, test\n",
        "    # train_loss_style.append(train_per_epoch_loss_style)\n",
        "    # train_accuracy_style.append(train_per_epoch_accuracy_style)\n",
        "\n",
        "    # train_loss_item.append(train_per_epoch_loss_item)\n",
        "    # train_accuracy_item.append(train_per_epoch_accuracy_item)\n",
        "\n",
        "    # test_loss_style.append(test_per_epoch_loss_style)\n",
        "    # test_accuracy_style.append(test_per_epoch_accuracy_style)\n",
        "\n",
        "    # test_loss_item.append(test_per_epoch_loss_item)\n",
        "    # test_accuracy_item.append(test_per_epoch_accuracy_item)\n",
        "\n",
        "\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}/{n_epochs}\\t| Training loss style: {train_per_epoch_loss_style:.4f} | Training loss item : {train_per_epoch_loss_item:.4f}')\n",
        "    print(f' Training accuracy style: {train_per_epoch_accuracy_style:.4f} |Training accuracy item: {train_per_epoch_accuracy_item:.4f} ', end='')\n",
        "    print(f'Test loss style: {test_per_epoch_loss_style:.4f} | Test loss item: {test_per_epoch_loss_item:.4f} |  Test accuracy style: {test_per_epoch_accuracy_style:.4f} Test accuracy item: {test_per_epoch_accuracy_item:.4f}')\n",
        "    print(f\"Test Mean Average Precision (Style): {test_mAP_style} | Test Mean Average Precision (Item): {test_mAP_item}\" + '\\n')\n",
        "    #print(f\"Test Mean Average Precision (Style): {average_precision_style} | Test Mean Average Precision (Item): {average_precision_item}\")\n",
        "    #print(f\"Test Mean Average Recall (Style): {average_recall_style} | Test Mean Average Recall (Item): {average_recall_item}\")\n",
        "\n",
        "\n",
        "'''\n",
        "mean_precision_style = np.mean(all_precisions_style)\n",
        "mean_recall_style = np.mean(all_recalls_style)\n",
        "mean_precision_item =  np.mean(all_precisions_item)\n",
        "mean_recall_item = np.mean(all_recalls_item)\n",
        "\n",
        "  # 평균 성능 결과 출력\n",
        "print(\"Mean Precision Style:\", mean_precision_style)\n",
        "print(\"Mean Recall Style:\", mean_recall_style)\n",
        "print(\"Mean Precision Style:\", mean_precision_item)\n",
        "print(\"Mean Recall Style:\", mean_recall_item)\n",
        "'''\n",
        "\n",
        "    #print(\"Test Mean Average Precision (Style):\", test_mAP_style)\n",
        "    #print(\"Test Mean Average Precision (Item):\", test_mAP_item)\n",
        "    # print(f'Test loss style: {test_per_epoch_loss_style:.4f} | Test loss item: {test_per_epoch_loss_item:.4f} |  Test accuracy style: {test_per_epoch_accuracy_style:.4f} Test accuracy item: {test_per_epoch_accuracy_item:.4f}' + '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy8IcqSXerzx"
      },
      "source": [
        "##VERSION 6 Freeze upto many stages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpEu2Vdxe-Zi",
        "outputId": "521d3b05-f8ca-4ef6-845c-8a8b6c78aff7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 130MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "from torchvision.models.resnet import BasicBlock\n",
        "from torch.utils import model_zoo\n",
        "\n",
        "\n",
        "class MixStyle(nn.Module):\n",
        "    \"\"\"MixStyle.\n",
        "\n",
        "    Reference:\n",
        "      Zhou et al. Domain Generalization with MixStyle. ICLR 2021.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5, alpha=0.1, eps=1e-6, mix='random'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          p (float): probability of using MixStyle.\n",
        "          alpha (float): parameter of the Beta distribution.\n",
        "          eps (float): scaling parameter to avoid numerical issues.\n",
        "          mix (str): how to mix.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        self.beta = torch.distributions.Beta(alpha, alpha)\n",
        "        self.eps = eps\n",
        "        self.alpha = alpha\n",
        "        self.mix = mix\n",
        "        self._activated = True\n",
        "        self.lmda = None\n",
        "        self.perm = None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'MixStyle(p={self.p}, alpha={self.alpha}, eps={self.eps}, mix={self.mix})'\n",
        "\n",
        "    def set_activation_status(self, status=True):\n",
        "        self._activated = status\n",
        "\n",
        "    def update_mix_method(self, mix='random'):\n",
        "        self.mix = mix\n",
        "\n",
        "    def forward(self, x , perm):\n",
        "\n",
        "        if not self.training or not self._activated:\n",
        "            return x\n",
        "\n",
        "        if random.random() > self.p:\n",
        "            self.lmda = torch.full((x.size()[0], 1, 1, 1), 1)\n",
        "            #self.perm = torch.arange(1, 65)\n",
        "            return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        B = x.size(0)\n",
        "\n",
        "        mu = x.mean(dim=[2, 3], keepdim=True)\n",
        "        var = x.var(dim=[2, 3], keepdim=True)\n",
        "        sig = (var + self.eps).sqrt()\n",
        "        mu, sig = mu.detach(), sig.detach()\n",
        "        x_normed = (x-mu) / sig\n",
        "\n",
        "\n",
        "        sampled = self.beta.sample()\n",
        "\n",
        "        if sampled < 0.5 :\n",
        "          sampled = 1- sampled\n",
        "        lmda = torch.full((x.size()[0], 1, 1, 1), sampled)\n",
        "\n",
        "        self.lmda = lmda.to(x.device)\n",
        "\n",
        "        #lmda = self.beta.sample((B, 1, 1, 1))\n",
        "        #lmda = torch.tensor(lmda)\n",
        "        # for i in range(B):\n",
        "        #   if lmda[i].item() < 0.5:\n",
        "        #     lmda[i] = 1 - lmda[i]\n",
        "\n",
        "        #print(f\"lmda : {lmda}\")\n",
        "\n",
        "\n",
        "        mu2, sig2 = mu[perm], sig[perm]\n",
        "        mu_mix = mu*self.lmda + mu2 * (1-self.lmda)\n",
        "        sig_mix = sig*self.lmda + sig2 * (1-self.lmda)\n",
        "\n",
        "        return x_normed*sig_mix + mu_mix\n",
        "\n",
        "\n",
        "from __future__ import division, absolute_import\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    'resnet18',\n",
        "    'resnet50'\n",
        "]\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18':\n",
        "    'https://download.pytorch.org/models/resnet18-f37072fd.pth',\n",
        "    'resnet50':\n",
        "    'https://download.pytorch.org/models/resnet50-11ad3fa6.pth'\n",
        "}\n",
        "\n",
        "'''\n",
        "Reference : https://pytorch.org/vision/main/_modules/torchvision/models/resnet.html\n",
        "'''\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation\n",
        "    )\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes, out_planes, kernel_size=1, stride=stride, bias=False\n",
        "    )\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride=1,\n",
        "        downsample=None,\n",
        "        groups=1,\n",
        "        base_width=64,\n",
        "        dilation=1,\n",
        "        norm_layer=None\n",
        "    ):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError(\n",
        "                'BasicBlock only supports groups=1 and base_width=64'\n",
        "            )\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\n",
        "                \"Dilation > 1 not supported in BasicBlock\"\n",
        "            )\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\" https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride = 1,\n",
        "        downsample = None,\n",
        "        groups = 1,\n",
        "        base_width = 64,\n",
        "        dilation = 1,\n",
        "        norm_layer = None,\n",
        "    ):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.0)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        loss,\n",
        "        block,\n",
        "        layers,\n",
        "        zero_init_residual=False,\n",
        "        groups=1,\n",
        "        width_per_group=64,\n",
        "        replace_stride_with_dilation=None,\n",
        "        norm_layer=None,\n",
        "        last_stride=2,\n",
        "        fc_dims=None,\n",
        "        dropout_p=None,\n",
        "        mixstyle_layers=[],\n",
        "        mixstyle_p=0.5,\n",
        "        mixstyle_alpha=0.1,\n",
        "    ):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "        self.loss = loss\n",
        "        self.feature_dim = 512 * block.expansion\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\n",
        "\n",
        "                format(replace_stride_with_dilation)\n",
        "            )\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False\n",
        "        )\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(\n",
        "            block,\n",
        "            128,\n",
        "            layers[1],\n",
        "            stride=2,\n",
        "            dilate=replace_stride_with_dilation[0]\n",
        "        )\n",
        "        self.layer3 = self._make_layer(\n",
        "            block,\n",
        "            256,\n",
        "            layers[2],\n",
        "            stride=2,\n",
        "            dilate=replace_stride_with_dilation[1]\n",
        "        )\n",
        "        self.layer4 = self._make_layer(\n",
        "            block,\n",
        "            512,\n",
        "            layers[3],\n",
        "            stride=last_stride,\n",
        "            dilate=replace_stride_with_dilation[2]\n",
        "        )\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "\n",
        "        self.mixstyle = MixStyle(p=mixstyle_p, alpha=mixstyle_alpha, mix='random')\n",
        "        self.mixstyle_layers = mixstyle_layers\n",
        "\n",
        "        self.lmda_1 = None\n",
        "\n",
        "        self.lmda_2 = None\n",
        "\n",
        "        self.lmda_3 = None\n",
        "\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n",
        "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
        "                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes, planes, stride, downsample, self.groups,\n",
        "                self.base_width, previous_dilation, norm_layer\n",
        "            )\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, perm_1, perm_2, perm_3):\n",
        "        for param in self.conv1.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.bn1.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.relu.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.maxpool.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.layer1.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.layer1.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        if 'layer1' in self.mixstyle_layers:\n",
        "            x = self.mixstyle(x, perm_1)\n",
        "            #print(self.mixstyle.lmda_1)\n",
        "            #print(self.mixstyle.perm_1)\n",
        "            self.lmda_1 = self.mixstyle.lmda\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        if 'layer2' in self.mixstyle_layers:\n",
        "            x = self.mixstyle(x, perm_2)\n",
        "            self.lmda_2 = self.mixstyle.lmda\n",
        "\n",
        "        x = self.layer3(x)\n",
        "        if 'layer3' in self.mixstyle_layers:\n",
        "            x = self.mixstyle(x, perm_3)\n",
        "            self.lmda_3 = self.mixstyle.lmda\n",
        "\n",
        "        f = self.layer4(x)\n",
        "        v = self.global_avgpool(f)\n",
        "        v = v.view(v.size(0), -1)\n",
        "\n",
        "        y = self.fc(v)\n",
        "\n",
        "        return y\n",
        "\n",
        "class MultiTaskNet(nn.Module):\n",
        "    def __init__(self, net, backbone, num_classes_style = 4, num_classes_item = 65, pretrained=True ):\n",
        "        super(MultiTaskNet, self).__init__()\n",
        "        self.net = net\n",
        "        self.net.fc = nn.Identity()\n",
        "\n",
        "        if backbone == \"resnet18\":\n",
        "          self.net.fc1 = nn.Sequential(OrderedDict([('linear', nn.Linear(512,512)),('relu1', nn.ReLU()),('final', nn.Linear(512, 4))]))\n",
        "          self.net.fc2 = nn.Sequential(OrderedDict([('linear', nn.Linear(512,512)),('relu1', nn.ReLU()),('final', nn.Linear(512, 65))]))\n",
        "        elif backbone == \"resnet50\":\n",
        "          self.net.fc1 = nn.Sequential(OrderedDict([('linear', nn.Linear(2048,2048)),('relu1', nn.ReLU()),('final', nn.Linear(2048, 4))]))\n",
        "          self.net.fc2 = nn.Sequential(OrderedDict([('linear', nn.Linear(2048,2048)),('relu1', nn.ReLU()),('final', nn.Linear(2048, 65))]))\n",
        "\n",
        "\n",
        "    def forward(self, x, perm_1, perm_2, perm_3):\n",
        "        x = self.net(x, perm_1, perm_2, perm_3)\n",
        "        style_head = self.net.fc1(x)\n",
        "        item_head = self.net.fc2(x)\n",
        "        #print(self.net.lmda_1)\n",
        "        #print(self.net.perm_1)\n",
        "\n",
        "        #print(self.net.mixstyle.lmda)\n",
        "        #print(self.net.mixstyle.perm)\n",
        "\n",
        "        return style_head, item_head\n",
        "\n",
        "def init_pretrained_weights(model, model_url):\n",
        "    \"\"\"Initializes model with pretrained weights.\n",
        "\n",
        "    Layers that don't match with pretrained layers in name or size are kept unchanged.\n",
        "    \"\"\"\n",
        "    pretrain_dict = model_zoo.load_url(model_url)\n",
        "    model_dict = model.state_dict()\n",
        "    pretrain_dict = {\n",
        "        k: v\n",
        "        for k, v in pretrain_dict.items()\n",
        "        if k in model_dict and model_dict[k].size() == v.size()\n",
        "    }\n",
        "    model_dict.update(pretrain_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "\n",
        "\n",
        "def resnet18(num_classes=1000, loss='softmax', pretrained=True, **kwargs):\n",
        "    model = ResNet(\n",
        "        num_classes=num_classes,\n",
        "        loss=loss,\n",
        "        block=BasicBlock,\n",
        "        layers=[2, 2, 2, 2],\n",
        "        last_stride=2,\n",
        "        fc_dims=None,\n",
        "        dropout_p=None,\n",
        "        mixstyle_layers=['layer1', 'layer2', 'layer3'],\n",
        "        mixstyle_alpha=0.1,\n",
        "        **kwargs\n",
        "    )\n",
        "    if pretrained:\n",
        "        init_pretrained_weights(model, model_urls['resnet18'])\n",
        "    return model\n",
        "\n",
        "def resnet50(num_classes=1000, loss='softmax', pretrained=True, **kwargs):\n",
        "    model = ResNet(\n",
        "        num_classes=num_classes,\n",
        "        loss=loss,\n",
        "        block=Bottleneck,\n",
        "        layers=[3, 4, 6, 3],\n",
        "        last_stride=2,\n",
        "        fc_dims=None,\n",
        "        dropout_p=None,\n",
        "        mixstyle_layers=['layer1', 'layer2'],\n",
        "        mixstyle_alpha=0.1,\n",
        "        **kwargs\n",
        "    )\n",
        "    if pretrained:\n",
        "        init_pretrained_weights(model, model_urls['resnet50'])\n",
        "    return model\n",
        "\n",
        "backbone = [\"resnet18\" , \"resnet50\"]\n",
        "# 특정 레이어를 freeze\n",
        "freeze_layers = ['layer1', 'layer2']  # freeze하고 싶은 레이어 명세\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "for name, child in model_pretrain.named_children():\n",
        "    if name in freeze_layers:\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "# 특정 레이어의 원하는 파라미터를 선택적으로 freeze\n",
        "for name, child in model_pretrain.named_children():\n",
        "    if name == 'layer1':\n",
        "        for param_name, param in child.named_parameters():\n",
        "            # 특정 조건에 따라 파라미터 freeze\n",
        "            if 'bn' in param_name:  # 'bn'이 포함된 파라미터만 선택\n",
        "                param.requires_grad = False\n",
        "'''\n",
        "\n",
        "model_pretrained_ = resnet50()\n",
        "model_mixstyle_ = MultiTaskNet(model_pretrained_, backbone = \"resnet50\")\n",
        "model_to_train = model_mixstyle_.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model_to_train.parameters(), lr = 0.0004, weight_decay = 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "D0KSntPjfOLe",
        "outputId": "cce079b5-79e2-48a9-b9fa-b8389b9e880e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n    data = data.to(device)\\n    labels = labels.to(device)\\n    output = model(data)\\n    _, pred_labels = output.max(-1)\\n    correct = (pred_labels == labels)\\n    return correct.cpu().detach().numpy().tolist()\\n'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "Train and Test function\n",
        "'''\n",
        "def training_batch(train_loader, model,criterion, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    training_loss = []\n",
        "    training_style_loss = []\n",
        "    training_item_loss = []\n",
        "\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "      input = data[\"image\"].to(device)\n",
        "      style_label_ = data[\"style\"]\n",
        "      style_label = style_label_.to(device)\n",
        "      item_label = data[\"item\"].to(device)\n",
        "\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      rand_index_1 = torch.randperm(input.size()[0]).cuda()\n",
        "      rand_index_2 = torch.randperm(input.size()[0]).cuda()\n",
        "      rand_index_3 = torch.randperm(input.size()[0]).cuda()\n",
        "\n",
        "      style_output, item_output = model(input, rand_index_1, rand_index_2, rand_index_3)\n",
        "\n",
        "      #####labelling#####\n",
        "      if(model.net.lmda_1[0] != None):\n",
        "        lmda_1 = float(model.net.lmda_1[0])\n",
        "      if(model.net.lmda_2[0] != None):\n",
        "        lmda_2 = float(model.net.lmda_2[0])\n",
        "      # if(model.net.lmda_3[0] != None):\n",
        "      #   lmda_3 = float(model.net.lmda_3[0])\n",
        "\n",
        "\n",
        "      style_label_a = style_label[rand_index_1]\n",
        "      style_label_b = style_label[rand_index_2]\n",
        "      #style_label_c = style_label[rand_index_3]\n",
        "\n",
        "\n",
        "      loss_style_temp1 = criterion(style_output, style_label_a)\n",
        "      loss_style_temp2 = criterion(style_output, style_label_b)\n",
        "      #loss_style_temp3 = criterion(style_output, style_label_c)\n",
        "\n",
        "      loss_style_original = criterion(style_output, style_label)\n",
        "\n",
        "      #loss_style = lmda_3 * (lmda_2 * (lmda_1 * loss_style_original + (1 - lmda_1) * loss_style_temp1) + (1 - lmda_2) * loss_style_temp2) + (1 - lmda_3) * loss_style_temp3\n",
        "      loss_style = lmda_2 * (lmda_1 * loss_style_original + (1 - lmda_1) * loss_style_temp1) + (1 - lmda_2) * loss_style_temp2\n",
        "\n",
        "\n",
        "      loss_item = criterion(item_output, item_label)\n",
        "\n",
        "      loss = loss_style + loss_item\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      training_loss.append(loss.detach().item())\n",
        "      training_style_loss.append(loss_style_original.detach().item())\n",
        "      training_item_loss.append(loss_item.detach().item())\n",
        "\n",
        "      # print(f\"lmda_1 : {lmda_1}\")\n",
        "      # print(f\"lmda_2 : {lmda_2}\")\n",
        "      # print(f\"lmda_3 : {lmda_3}\")\n",
        "\n",
        "\n",
        "    return training_style_loss, training_item_loss\n",
        "\n",
        "'''\n",
        "def training_batch_with_cutmix(train_loader, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    beta = 1.0\n",
        "    cutmix_prob = 0.5\n",
        "\n",
        "    for _, (input, target) in enumerate(train_loader):\n",
        "        input = input.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        r = np.random.rand(1)\n",
        "\n",
        "        if beta > int(0.0) and r < cutmix_prob:\n",
        "            lam = np.random.beta(beta, beta)\n",
        "            rand_index = torch.randperm(input.size()[0]).cuda()\n",
        "            target_a = target # target이 label\n",
        "            target_b = target[rand_index]\n",
        "            bbx1, bby1, bbx2, bby2 = rand_bbox(input.size(), lam)\n",
        "            input[:, :, bbx1:bbx2, bby1:bby2] = input[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "                # adjust lambda to exactly match pixel ratio\n",
        "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (input.size()[-1] * input.size()[-2]))\n",
        "                # compute output\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target_a) * lam + criterion(output, target_b) * (1. - lam)\n",
        "        else:\n",
        "            # compute output\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        training_losses.append(loss.item())\n",
        "\n",
        "    for data, labels in train_loader:\n",
        "        training_accuracies.extend(accuracy(data, labels, model))\n",
        "'''\n",
        "\n",
        "def test_batch(data, model, criterion):\n",
        "    model.eval()\n",
        "    input = data[\"image\"].to(device)\n",
        "    style_label = data[\"style\"].to(device)\n",
        "    item_label = data[\"item\"].to(device)\n",
        "\n",
        "    style_output, item_output = model(input, None, None, None)\n",
        "    loss_1 = criterion(style_output, style_label)\n",
        "    loss_2 = criterion(item_output, item_label)\n",
        "\n",
        "    total_loss = loss_1 + loss_2\n",
        "    return  loss_1.item(), loss_2.item()\n",
        "\n",
        "def accuracy(data, model):\n",
        "    model.eval()\n",
        "\n",
        "    input = data[\"image\"].to(device)\n",
        "    style_label = data[\"style\"].to(device)\n",
        "    item_label = data[\"item\"].to(device)\n",
        "\n",
        "    style_output, item_output = model(input, None, None, None)\n",
        "\n",
        "    _, pred_label_style = style_output.max(-1)\n",
        "    _, pred_label_item = item_output.max(-1)\n",
        "\n",
        "    correct_style = (pred_label_style == style_label)\n",
        "    correct_item = (pred_label_item == item_label)\n",
        "\n",
        "    return correct_style.cpu().detach().numpy().tolist(), correct_item.cpu().detach().numpy().tolist()\n",
        "\n",
        "\n",
        "'''\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    _, pred_labels = output.max(-1)\n",
        "    correct = (pred_labels == labels)\n",
        "    return correct.cpu().detach().numpy().tolist()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syuDTcLSfR-V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchmetrics.classification import MulticlassAveragePrecision\n",
        "\n",
        "def classification_map(data_loader, model):\n",
        "    # MulticlassAveragePrecision 메트릭스 초기화\n",
        "    map_metric_style = MulticlassAveragePrecision(num_classes=4, average=\"macro\")\n",
        "    map_metric_item = MulticlassAveragePrecision(num_classes=65, average=\"macro\")\n",
        "\n",
        "    device = next(model.parameters()).device  # 모델의 디바이스 확인\n",
        "\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "\n",
        "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
        "        for data in data_loader:\n",
        "            input = data[\"image\"].to(device)\n",
        "            style_true = data[\"style\"].to(device)\n",
        "            item_true = data[\"item\"].to(device)\n",
        "\n",
        "            style_pred, item_pred = model(input, None, None, None)\n",
        "\n",
        "            # MulticlassAveragePrecision 클래스는 확률을 입력으로 받으므로 softmax를 적용하여 확률로 변환\n",
        "            style_pred_prob = torch.softmax(style_pred, dim=1)\n",
        "            item_pred_prob = torch.softmax(item_pred, dim=1)\n",
        "\n",
        "            # 메트릭스 업데이트\n",
        "            map_metric_style.update(style_pred_prob, style_true)\n",
        "            map_metric_item.update(item_pred_prob, item_true)\n",
        "\n",
        "    # 평균 정밀도 계산\n",
        "    mAP_style = map_metric_style.compute()\n",
        "    mAP_item = map_metric_item.compute()\n",
        "\n",
        "    return mAP_style, mAP_item\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Xu4SDTCwfavl",
        "outputId": "f5ea22b8-26a6-4be1-f132-fff04d8db783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Respective Execution time : 546.8022589683533sec    1 epochs\n",
            "Epoch: 1/10\t| Training loss style: 0.6729 | Training loss item : 1.4539\n",
            " Training accuracy style: 0.8071 |Training accuracy item: 0.8334 Test loss style: 0.5678 | Test loss item: 0.8508 |  Test accuracy style: 0.7828 Test accuracy item: 0.7725\n",
            "Test Mean Average Precision (Style): 0.8376710414886475 | Test Mean Average Precision (Item): 0.8576977849006653\n",
            "\n",
            "Respective Execution time : 535.0997807979584sec    2 epochs\n",
            "Epoch: 2/10\t| Training loss style: 0.4638 | Training loss item : 0.5970\n",
            " Training accuracy style: 0.8532 |Training accuracy item: 0.9001 Test loss style: 0.5319 | Test loss item: 0.7421 |  Test accuracy style: 0.8014 Test accuracy item: 0.7924\n",
            "Test Mean Average Precision (Style): 0.8601208329200745 | Test Mean Average Precision (Item): 0.8810068964958191\n",
            "\n",
            "Respective Execution time : 540.2143046855927sec    3 epochs\n",
            "Epoch: 3/10\t| Training loss style: 0.3750 | Training loss item : 0.3759\n",
            " Training accuracy style: 0.9199 |Training accuracy item: 0.9290 Test loss style: 0.4155 | Test loss item: 0.7497 |  Test accuracy style: 0.8441 Test accuracy item: 0.8064\n",
            "Test Mean Average Precision (Style): 0.8992377519607544 | Test Mean Average Precision (Item): 0.8867228627204895\n",
            "\n",
            "Respective Execution time : 536.3867802619934sec    4 epochs\n",
            "Epoch: 4/10\t| Training loss style: 0.2966 | Training loss item : 0.2904\n",
            " Training accuracy style: 0.9368 |Training accuracy item: 0.9532 Test loss style: 0.4289 | Test loss item: 0.6990 |  Test accuracy style: 0.8459 Test accuracy item: 0.8198\n",
            "Test Mean Average Precision (Style): 0.8950754404067993 | Test Mean Average Precision (Item): 0.8917859196662903\n",
            "\n",
            "Respective Execution time : 544.4086043834686sec    5 epochs\n",
            "Epoch: 5/10\t| Training loss style: 0.2516 | Training loss item : 0.2117\n",
            " Training accuracy style: 0.9343 |Training accuracy item: 0.9488 Test loss style: 0.4562 | Test loss item: 0.8163 |  Test accuracy style: 0.8369 Test accuracy item: 0.8027\n",
            "Test Mean Average Precision (Style): 0.891238272190094 | Test Mean Average Precision (Item): 0.880781352519989\n",
            "\n",
            "Respective Execution time : 542.8269166946411sec    6 epochs\n",
            "Epoch: 6/10\t| Training loss style: 0.2138 | Training loss item : 0.1781\n",
            " Training accuracy style: 0.9627 |Training accuracy item: 0.9673 Test loss style: 0.4372 | Test loss item: 0.7355 |  Test accuracy style: 0.8459 Test accuracy item: 0.8142\n",
            "Test Mean Average Precision (Style): 0.8945350646972656 | Test Mean Average Precision (Item): 0.8880754113197327\n",
            "\n",
            "Respective Execution time : 545.5094780921936sec    7 epochs\n",
            "Epoch: 7/10\t| Training loss style: 0.1853 | Training loss item : 0.1551\n",
            " Training accuracy style: 0.9622 |Training accuracy item: 0.9763 Test loss style: 0.4553 | Test loss item: 0.7061 |  Test accuracy style: 0.8481 Test accuracy item: 0.8276\n",
            "Test Mean Average Precision (Style): 0.9019712209701538 | Test Mean Average Precision (Item): 0.8934245109558105\n",
            "\n",
            "Respective Execution time : 539.2760021686554sec    8 epochs\n",
            "Epoch: 8/10\t| Training loss style: 0.1555 | Training loss item : 0.1389\n",
            " Training accuracy style: 0.9742 |Training accuracy item: 0.9498 Test loss style: 0.4519 | Test loss item: 1.0665 |  Test accuracy style: 0.8547 Test accuracy item: 0.7828\n",
            "Test Mean Average Precision (Style): 0.8991499543190002 | Test Mean Average Precision (Item): 0.8657069206237793\n",
            "\n",
            "Respective Execution time : 546.7452352046967sec    9 epochs\n",
            "Epoch: 9/10\t| Training loss style: 0.1535 | Training loss item : 0.1300\n",
            " Training accuracy style: 0.9700 |Training accuracy item: 0.9676 Test loss style: 0.4509 | Test loss item: 0.9064 |  Test accuracy style: 0.8453 Test accuracy item: 0.7980\n",
            "Test Mean Average Precision (Style): 0.8942570686340332 | Test Mean Average Precision (Item): 0.8772526979446411\n",
            "\n",
            "Respective Execution time : 544.629976272583sec    10 epochs\n",
            "Epoch: 10/10\t| Training loss style: 0.1561 | Training loss item : 0.1180\n",
            " Training accuracy style: 0.9809 |Training accuracy item: 0.9681 Test loss style: 0.4149 | Test loss item: 0.8702 |  Test accuracy style: 0.8587 Test accuracy item: 0.8002\n",
            "Test Mean Average Precision (Style): 0.9009828567504883 | Test Mean Average Precision (Item): 0.8804684281349182\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nmean_precision_style = np.mean(all_precisions_style)\\nmean_recall_style = np.mean(all_recalls_style)\\nmean_precision_item =  np.mean(all_precisions_item)\\nmean_recall_item = np.mean(all_recalls_item)\\n\\n  # 평균 성능 결과 출력\\nprint(\"Mean Precision Style:\", mean_precision_style)\\nprint(\"Mean Recall Style:\", mean_recall_style)\\nprint(\"Mean Precision Style:\", mean_precision_item)\\nprint(\"Mean Recall Style:\", mean_recall_item)\\n'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "Actual Training\n",
        "'''\n",
        "\n",
        "n_epochs = 10\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.9)\n",
        "\n",
        "train_loss_style, test_loss_style = [], []\n",
        "train_loss_item, test_loss_item = [], []\n",
        "train_accuracy_style, test_accuracy_style = [], []\n",
        "train_accuracy_item, test_accuracy_item = [], []\n",
        "all_precisions_style = []\n",
        "all_recalls_style = []\n",
        "all_precisions_item = []\n",
        "all_recalls_item = []\n",
        "\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_losses_style, train_losses_item = [], []\n",
        "    test_losses_style,test_losses_item = [],[]\n",
        "\n",
        "    train_accuracies_style, train_accuracies_item = [],[]\n",
        "    test_accuracies_style, test_accuracies_item = [], []\n",
        "\n",
        "\n",
        "    time1 = time.time()\n",
        "    train_losses_style, train_losses_item = training_batch(train_dataloader, model_to_train, criterion, optimizer)\n",
        "\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        train_batch_correct_style, train_batch_correct_item = accuracy(data, model_to_train)\n",
        "        train_accuracies_style.extend(train_batch_correct_style)\n",
        "        train_accuracies_item.extend(train_batch_correct_item)\n",
        "\n",
        "\n",
        "    train_per_epoch_loss_style = np.array(train_losses_style).mean()\n",
        "    train_per_epoch_loss_item = np.array(train_losses_item).mean()\n",
        "    train_per_epoch_accuracy_style = np.mean(train_accuracies_style)\n",
        "    train_per_epoch_accuracy_item = np.mean(train_accuracies_item)\n",
        "\n",
        "    #train_mAP_style, train_mAP_item = mean_average_precision(train_dataloader, model_to_train)\n",
        "    #print(\"Test Mean Average Precision (Style):\", train_mAP_style)\n",
        "    #print(\"Test Mean Average Precision (Item):\", train_mAP_item)\n",
        "\n",
        "    time2 = time.time()\n",
        "    print(\"Respective Execution time : \" + str(time2 - time1) + \"sec    \" + str(epoch+1) + \" epochs\")\n",
        "\n",
        "    learning_rate = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #   for i, data in enumerate(val_dataloader):\n",
        "    #       val_batch_correct_style,val_batch_correct_item = accuracy(data, model_to_train)\n",
        "    #       val_accuracies_style.extend(val_batch_correct_style)\n",
        "    #       val_accuracies_item.extend(val_batch_correct_item)\n",
        "\n",
        "    #   val_per_epoch_accuracy_style = np.mean(val_accuracies_style)\n",
        "    #   val_per_epoch_accuracy_item = np.mean(val_accuracies_item)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for i, data in enumerate(test_dataloader):\n",
        "        test_batch_loss_style, test_batch_loss_item = test_batch(data, model_to_train, criterion)\n",
        "        test_losses_style.append(test_batch_loss_style)\n",
        "        test_losses_item.append(test_batch_loss_item)\n",
        "\n",
        "        test_batch_correct_style, test_batch_correct_item = accuracy(data , model_to_train)\n",
        "        test_accuracies_style.extend(test_batch_correct_style)\n",
        "        test_accuracies_item.extend(test_batch_correct_item)\n",
        "\n",
        "    test_mAP_style, test_mAP_item  = classification_map(test_dataloader, model_to_train)\n",
        "    #average_precision_style, average_recall_style, average_precision_item, average_recall_item = test_precision_recall( model_to_train, test_dataloader)\n",
        "\n",
        "    #all_precisions_style.append(average_precision_style)\n",
        "    #all_recalls_style.append(average_recall_style)\n",
        "\n",
        "    #all_precisions_item.append(average_precision_item)\n",
        "    #all_recalls_item.append(average_recall_item)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    test_per_epoch_loss_style = np.array(test_losses_style).mean()\n",
        "    test_per_epoch_loss_item = np.array(test_losses_item).mean()\n",
        "    test_per_epoch_accuracy_style = np.mean(test_accuracies_style)\n",
        "    test_per_epoch_accuracy_item = np.mean(test_accuracies_item)\n",
        "\n",
        "\n",
        "\n",
        "    # train, test\n",
        "    # train_loss_style.append(train_per_epoch_loss_style)\n",
        "    # train_accuracy_style.append(train_per_epoch_accuracy_style)\n",
        "\n",
        "    # train_loss_item.append(train_per_epoch_loss_item)\n",
        "    # train_accuracy_item.append(train_per_epoch_accuracy_item)\n",
        "\n",
        "    # test_loss_style.append(test_per_epoch_loss_style)\n",
        "    # test_accuracy_style.append(test_per_epoch_accuracy_style)\n",
        "\n",
        "    # test_loss_item.append(test_per_epoch_loss_item)\n",
        "    # test_accuracy_item.append(test_per_epoch_accuracy_item)\n",
        "\n",
        "\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}/{n_epochs}\\t| Training loss style: {train_per_epoch_loss_style:.4f} | Training loss item : {train_per_epoch_loss_item:.4f}')\n",
        "    print(f' Training accuracy style: {train_per_epoch_accuracy_style:.4f} |Training accuracy item: {train_per_epoch_accuracy_item:.4f} ', end='')\n",
        "    print(f'Test loss style: {test_per_epoch_loss_style:.4f} | Test loss item: {test_per_epoch_loss_item:.4f} |  Test accuracy style: {test_per_epoch_accuracy_style:.4f} Test accuracy item: {test_per_epoch_accuracy_item:.4f}')\n",
        "    print(f\"Test Mean Average Precision (Style): {test_mAP_style} | Test Mean Average Precision (Item): {test_mAP_item}\" + '\\n')\n",
        "    #print(f\"Test Mean Average Precision (Style): {average_precision_style} | Test Mean Average Precision (Item): {average_precision_item}\")\n",
        "    #print(f\"Test Mean Average Recall (Style): {average_recall_style} | Test Mean Average Recall (Item): {average_recall_item}\")\n",
        "\n",
        "\n",
        "'''\n",
        "mean_precision_style = np.mean(all_precisions_style)\n",
        "mean_recall_style = np.mean(all_recalls_style)\n",
        "mean_precision_item =  np.mean(all_precisions_item)\n",
        "mean_recall_item = np.mean(all_recalls_item)\n",
        "\n",
        "  # 평균 성능 결과 출력\n",
        "print(\"Mean Precision Style:\", mean_precision_style)\n",
        "print(\"Mean Recall Style:\", mean_recall_style)\n",
        "print(\"Mean Precision Style:\", mean_precision_item)\n",
        "print(\"Mean Recall Style:\", mean_recall_item)\n",
        "'''\n",
        "\n",
        "    #print(\"Test Mean Average Precision (Style):\", test_mAP_style)\n",
        "    #print(\"Test Mean Average Precision (Item):\", test_mAP_item)\n",
        "    # print(f'Test loss style: {test_per_epoch_loss_style:.4f} | Test loss item: {test_per_epoch_loss_item:.4f} |  Test accuracy style: {test_per_epoch_accuracy_style:.4f} Test accuracy item: {test_per_epoch_accuracy_item:.4f}' + '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCc2vsxaFj5m"
      },
      "source": [
        "# **Task 4**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 동진 코드"
      ],
      "metadata": {
        "id": "c6Mamv2t6aDa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ogs0da8Fexk"
      },
      "source": [
        "#Semantic Segmentation Practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrwMtVd2N9px"
      },
      "source": [
        "##DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtY--xw3FjQX",
        "outputId": "df0cc6d6-dabf-43f6-ab9a-31d4d09ce4f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urtvLsET9pRx",
        "outputId": "b3476e2b-4808-4712-daf8-e6e0da83a5d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFTuy0JDN8Nj"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/VOCdevkit.zip\"\n",
        "extract_path = \"/content\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "  zip_ref.extractall(extract_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7M2UwCgOHBs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms import RandomHorizontalFlip, RandomCrop, RandomRotation\n",
        "from torchvision.transforms import functional as F\n",
        "import torch.nn.functional as Fu\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import torch.optim as optim\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4w3GFjuOMi9"
      },
      "outputs": [],
      "source": [
        "class CustomTransform:\n",
        "    def __init__(self):\n",
        "        self.resize = transforms.Resize((224, 224))\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "    def __call__(self, img, label):\n",
        "        # Random horizontal flip\n",
        "        if random.random() > 0.5:\n",
        "            img = F.hflip(img)\n",
        "            label = F.hflip(label)\n",
        "\n",
        "        # Random vertical flip\n",
        "        if random.random() > 1:\n",
        "            img = F.vflip(img)\n",
        "            label = F.vflip(label)\n",
        "\n",
        "        # Random rotation\n",
        "        angle = random.uniform(-10, 10)\n",
        "        img = F.rotate(img, angle)\n",
        "        label = F.rotate(label, angle)\n",
        "\n",
        "\n",
        "\n",
        "        # Resize and to tensor\n",
        "        img = self.to_tensor(self.resize(img))\n",
        "        label = self.to_tensor(self.resize(label))\n",
        "\n",
        "        return img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNNSNq6QOQl6"
      },
      "outputs": [],
      "source": [
        "class VOC2012(Dataset):\n",
        "  def __init__(self, data_path, label_path, check, text_file, transform ):\n",
        "    self.images = []\n",
        "    self.labels = []\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "    self.transform = transform\n",
        "    self.labels_ = []\n",
        "    with open(text_file, 'r') as file:\n",
        "      lines = file.read().splitlines()\n",
        "      for file_name in lines:\n",
        "        dataname_image = file_name + '.jpg'\n",
        "        dataname_label =  file_name + '.png'\n",
        "\n",
        "        image_file_path = os.path.join(data_path, dataname_image)\n",
        "        label_file_path = os.path.join(label_path, dataname_label)\n",
        "        #print(image_file_path)\n",
        "        #print(label_file_path)\n",
        "        if (os.path.isfile(image_file_path) and os.path.isfile(label_file_path)):\n",
        "          self.images.append(image_file_path)\n",
        "          self.labels.append(label_file_path)\n",
        "          self.labels_.append(label_file_path)\n",
        "\n",
        "\n",
        "\n",
        "        #print(self.labels)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img = Image.open(self.images[index]).convert(\"RGB\")\n",
        "\n",
        "    #img = transform_custom(img)\n",
        "    #print(\"execute\")\n",
        "\n",
        "    label_path = self.labels[index]\n",
        "\n",
        "    label = Image.open(label_path)\n",
        "\n",
        "\n",
        "    #label = transform_custom(label)\n",
        "    img, label = self.transform(img, label)\n",
        "    if(img == None or label == None):\n",
        "      print(\"error\")\n",
        "\n",
        "    #label_ = Image.open(self.labels[index]).convert(\"RGB\")\n",
        "    label = torch.tensor(np.array(label)*255, dtype = torch.long)\n",
        "    #, \"label_\": label_\n",
        "    sample = {\"image\" : img , \"label\" : label}\n",
        "\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EVBXES_OSYk"
      },
      "outputs": [],
      "source": [
        "#JPEGImages_path = \"/content/VOCdevkit/VOC2012/JPEGImages\"\n",
        "JPEGImages_path = \"/content/VOC2012/JPEGImages\"\n",
        "SegmentationClass_path = \"/content/VOC2012/SegmentationClass\"\n",
        "#SegmentationClass_path = \"/content/VOCdeckit/VOC2012/SegmentationClass\"\n",
        "pbl_train = \"/content/drive/MyDrive/pbl_train.txt\"\n",
        "pbl_val = \"/content/drive/MyDrive/pbl_val.txt\"\n",
        "\n",
        "Batch_size = 8\n",
        "custom_transform = CustomTransform()\n",
        "train_dataloader = DataLoader( VOC2012(data_path = JPEGImages_path, label_path = SegmentationClass_path, check = True, text_file= pbl_train, transform = custom_transform) , shuffle = True, batch_size = Batch_size)\n",
        "test_dataloader = DataLoader( VOC2012(data_path = JPEGImages_path, label_path = SegmentationClass_path, check = False, text_file= pbl_val, transform = custom_transform) , shuffle = False, batch_size = Batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "gRvEfzB7OUPW",
        "outputId": "309757e9-71a4-49cf-9a93-29b00d67be1b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'imshow' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f942a7bdf58a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"index : 0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'imshow' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "sample = next(iter(train_dataloader))\n",
        "imshow(sample['image'][2])\n",
        "print(\"index : 0\")\n",
        "print()\n",
        "imshow(sample['label'][2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef50uBoUOBPt"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmwBTPb_OWn_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class FCNResNet50(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(FCNResNet50, self).__init__()\n",
        "\n",
        "        # ResNet-50 불러오기 (pretrained 가중치 사용)\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "\n",
        "        # ResNet-50의 특징 추출 부분만 가져오기 (마지막 fully connected layer 제거)\n",
        "        self.features = nn.Sequential(*list(resnet.children())[:-2])\n",
        "\n",
        "        # 1x1 convolutional layer를 사용하여 클래스에 대한 score map 생성\n",
        "        self.score_conv = nn.Conv2d(2048, num_classes, kernel_size=1)\n",
        "\n",
        "        # Up-sampling을 사용하여 최종 segmentation map 생성\n",
        "        self.upsample = nn.Upsample(scale_factor=32, mode='bilinear', align_corners=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ResNet-50의 특징 추출\n",
        "        x = self.features(x)\n",
        "\n",
        "        # 1x1 convolutional layer를 통해 클래스에 대한 score map 생성\n",
        "        x = self.score_conv(x)\n",
        "\n",
        "        # Up-sampling을 통해 최종 segmentation map 생성\n",
        "        x = self.upsample(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# 클래스 수에 따라 모델 초기화\n",
        "num_classes = 21  # Pascal VOC 데이터셋의 클래스 수는 21\n",
        "fcn_resnet50_model = FCNResNet50(num_classes)\n",
        "# input_tensor = torch.randn(2, 3, 224 ,224)\n",
        "# semantic = fcn_resnet50_model(input_tensor)\n",
        "# print(semantic.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7QUnWEdObN5"
      },
      "outputs": [],
      "source": [
        " # (batch_size, num_classes, 224, 224)\n",
        " class SegmentationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SegmentationModel, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 21, kernel_size=2, stride=2)  # 21 classes for VOC2012\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 10\n",
        "#model_ = SegmentationModel()\n",
        "model = fcn_resnet50_model.to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 255)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozh0-Z1wOgg-"
      },
      "source": [
        "## Train and Test Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgNWJrXNOc_q"
      },
      "outputs": [],
      "source": [
        "def calculate_pixel_accuracy(pred, target):\n",
        "    correct_pixels = (pred == target).sum().item()\n",
        "    total_pixels = pred.size(0) * pred.size(1) * pred.size(2)\n",
        "    pixel_accuracy = correct_pixels / total_pixels\n",
        "    return pixel_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIUxcf4NOk54"
      },
      "outputs": [],
      "source": [
        "def calculate_iou(pred, target, num_classes):\n",
        "    iou_list = []\n",
        "    pred = pred.cpu().numpy()\n",
        "    target = target.cpu().numpy()\n",
        "\n",
        "    for cls in range(num_classes):\n",
        "        pred_inds = (pred == cls)\n",
        "        target_inds = (target == cls)\n",
        "\n",
        "        intersection = np.logical_and(pred_inds, target_inds).sum()\n",
        "        union = np.logical_or(pred_inds, target_inds).sum()\n",
        "\n",
        "        if union == 0:\n",
        "            iou_list.append(float('nan'))  # 예측과 라벨이 모두 없는 경우 NaN으로 처리\n",
        "        else:\n",
        "            iou_list.append(intersection / union)\n",
        "\n",
        "    return np.nanmean(iou_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vPe2bWVOmu-"
      },
      "outputs": [],
      "source": [
        "def calculate_mIoU(outputs, labels, num_classes):\n",
        "    iou_list = []\n",
        "    for i in range(outputs.size(0)):\n",
        "        pred = torch.argmax(outputs[i], dim=0)\n",
        "        iou = calculate_iou(pred, labels[i], num_classes)\n",
        "        iou_list.append(iou)\n",
        "    return np.mean(iou_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Thk07xkOoHQ"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_pixel_accuracy = 0.0\n",
        "    num_classes = 21\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        inputs = data[\"image\"].to(device)\n",
        "        labels_ = data[\"label\"]\n",
        "        labels = labels_.squeeze().to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.long())\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        pixel_acc = calculate_pixel_accuracy(torch.argmax(outputs, dim=1), labels)\n",
        "        running_pixel_accuracy += pixel_acc\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_dataloader)}, Pixel Accuracy: {running_pixel_accuracy / len(train_dataloader)}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_pixel_accuracy = 0.0\n",
        "    val_mIoU = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data in test_dataloader:\n",
        "            inputs = data[\"image\"].to(device)\n",
        "            labels_ = data[\"label\"]\n",
        "            labels = labels_.squeeze().to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            pixel_acc = calculate_pixel_accuracy(torch.argmax(outputs, dim=1), labels)\n",
        "            val_pixel_accuracy += pixel_acc\n",
        "\n",
        "            mIoU = calculate_mIoU(outputs, labels, num_classes)\n",
        "            val_mIoU += mIoU\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss / len(test_dataloader)}, Validation Pixel Accuracy: {val_pixel_accuracy / len(test_dataloader)}, Validation mIoU: {val_mIoU / len(test_dataloader)}\")\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSWq3k7wWdOx"
      },
      "source": [
        "## Panoptic Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf52koNSWcvF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ce-MJy4Wiip"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/VOCdevkit.zip\"\n",
        "extract_path = \"/content\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "  zip_ref.extractall(extract_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHQMqyC_Wn_k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms import RandomHorizontalFlip, RandomCrop, RandomRotation\n",
        "from torchvision.transforms import functional as F\n",
        "import torch.nn.functional as Fu\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import torch.optim as optim\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvVnbm1sWxTa"
      },
      "outputs": [],
      "source": [
        "class VOC2012(Dataset):\n",
        "  def __init__(self, data_path, label_semantic_path, label_instance_path,check, text_file, transform ):\n",
        "    self.images = []\n",
        "    self.labels_semantic = []\n",
        "    self.labels_instance = []\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "    self.transform = transform\n",
        "\n",
        "    with open(text_file, 'r') as file:\n",
        "      lines = file.read().splitlines()\n",
        "      for file_name in lines:\n",
        "        dataname_image = file_name + '.jpg'\n",
        "        dataname_label =  file_name + '.png'\n",
        "\n",
        "        image_file_path = os.path.join(data_path, dataname_image)\n",
        "        label_file_path_semantic = os.path.join(label_semantic_path, dataname_label)\n",
        "        label_file_path_instance =os.path.join(label_semantic_path, dataname_label)\n",
        "        #print(image_file_path)\n",
        "        #print(label_file_path)\n",
        "        if (os.path.isfile(image_file_path) and os.path.isfile(label_file_path)):\n",
        "          self.images.append(image_file_path)\n",
        "          self.labels_semantic.append(label_file_path_semantic)\n",
        "          self.labels_instance.append(label_file_path_instance)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    print(len(self.images))\n",
        "    print(len(self.labels_instance))\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img = Image.open(self.images[index]).convert(\"RGB\")\n",
        "\n",
        "    #img = transform_custom(img)\n",
        "    #print(\"execute\")\n",
        "\n",
        "    label_semantic = Image.open(self.labels_semantic[index])\n",
        "    label_instance = Image.open(self.labels_instance[index])\n",
        "\n",
        "    #label = transform_custom(label)\n",
        "    img, label_semantic, label_instance = self.transform(img, label_semantic, label_instance)\n",
        "    if(img == None or label_semantic == None or label_instance == None):\n",
        "      print(\"error\")\n",
        "\n",
        "    #label_ = Image.open(self.labels[index]).convert(\"RGB\")\n",
        "    label_semantic = torch.tensor(np.array(label)*255, dtype = torch.long)\n",
        "    label_instance = torch.tensor()\n",
        "    #, \"label_\": label_\n",
        "    sample = {\"image\" : img , \"label_semantic\" : label_semantic, \"label_instance\" : label_instance}\n",
        "\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOn76NTpW2Ym"
      },
      "outputs": [],
      "source": [
        "JPEGImages_path = \"/content/VOCdevkit/VOC2012/JPEGImages\"\n",
        "Semantic_path = \"/content/VOCdevkit/VOC2012/SegmentationClass\"\n",
        "Instance_path = \"/content/VOCdevkit/VOC2012/SegmentationObject\"\n",
        "\n",
        "pbl_train = \"/content/drive/MyDrive/pbl_train.txt\"\n",
        "pbl_val = \"/content/drive/MyDrive/pbl_val.txt\"\n",
        "\n",
        "Batch_size = 8\n",
        "custom_transform = CustomTransform()\n",
        "train_dataloader = DataLoader( VOC2012(data_path = JPEGImages_path, label_semantic_path = SegmentationClass_path, label_instance_path = Instance_path, check = True, text_file= pbl_train, transform = custom_transform) , shuffle = True, batch_size = Batch_size)\n",
        "test_dataloader = DataLoader( VOC2012(data_path = JPEGImages_path, label_semantic_path = SegmentationClass_path, label_instance_path = Instance_path, check = False, text_file= pbl_val, transform = custom_transform) , shuffle = False, batch_size = Batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJhrzDbUaFy6"
      },
      "source": [
        "##BackBone Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdEWnWHOaIr0"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"2 Layer No Expansion Block\n",
        "    \"\"\"\n",
        "    expansion: int = 1\n",
        "    def __init__(self, c1, c2, s=1, downsample= None) -> None:\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(c1, c2, 3, s, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(c2)\n",
        "        self.conv2 = nn.Conv2d(c2, c2, 3, 1, 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(c2)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        if self.downsample is not None: identity = self.downsample(x)\n",
        "        out += identity\n",
        "        return F.relu(out)\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    \"\"\"3 Layer 4x Expansion Block\n",
        "    \"\"\"\n",
        "    expansion: int = 4\n",
        "    def __init__(self, c1, c2, s=1, downsample=None) -> None:\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(c1, c2, 1, 1, 0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(c2)\n",
        "        self.conv2 = nn.Conv2d(c2, c2, 3, s, 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(c2)\n",
        "        self.conv3 = nn.Conv2d(c2, c2 * self.expansion, 1, 1, 0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(c2 * self.expansion)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        if self.downsample is not None: identity = self.downsample(x)\n",
        "        out += identity\n",
        "        return F.relu(out)\n",
        "\n",
        "\n",
        "resnet_settings = {\n",
        "    '18': [BasicBlock, [2, 2, 2, 2]],\n",
        "    '34': [BasicBlock, [3, 4, 6, 3]],\n",
        "    '50': [Bottleneck, [3, 4, 6, 3]],\n",
        "    '101': [Bottleneck, [3, 4, 23, 3]],\n",
        "    '152': [Bottleneck, [3, 8, 36, 3]]\n",
        "}\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, model_name: str = '50') -> None:\n",
        "        super().__init__()\n",
        "        assert model_name in resnet_settings.keys(), f\"ResNet model name should be in {list(resnet_settings.keys())}\"\n",
        "        block, depths = resnet_settings[model_name]\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, 7, 2, 3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.inplanes)\n",
        "        self.maxpool = nn.MaxPool2d(3, 2, 1)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, depths[0], s=1)\n",
        "        self.layer2 = self._make_layer(block, 128, depths[1], s=2)\n",
        "        self.layer3 = self._make_layer(block, 256, depths[2], s=2)\n",
        "        self.layer4 = self._make_layer(block, 512, depths[3], s=2)\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, planes, depth, s=1) -> nn.Sequential:\n",
        "        downsample = None\n",
        "        if s != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion, 1, s, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion)\n",
        "            )\n",
        "        layers = nn.Sequential(\n",
        "            block(self.inplanes, planes, s, downsample),\n",
        "            *[block(planes * block.expansion, planes) for _ in range(1, depth)]\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        return layers\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.maxpool(F.relu(self.bn1(self.conv1(x))))   # [1, 64, H/4, W/4]\n",
        "        x1 = self.layer1(x)  # [1, 64/256, H/4, W/4]\n",
        "        x2 = self.layer2(x1)  # [1, 128/512, H/8, W/8]\n",
        "        x3 = self.layer3(x2)  # [1, 256/1024, H/16, W/16]\n",
        "        x4 = self.layer4(x3)  # [1, 512/2048, H/32, W/32]\n",
        "        return x1, x2, x3, x4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwOPdg_MaJPg"
      },
      "source": [
        "##Panoptic Deeplab Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qW6H4hxLaPPD"
      },
      "outputs": [],
      "source": [
        "class ASPP(nn.Module):\n",
        "    def __init__(self, c1, c2, drop_rate=0.1):\n",
        "        super().__init__()\n",
        "        ratios = [1, 6, 12, 18]\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Conv(c1, c2, 1 if ratio==1 else 3, 1, 0 if ratio==1 else ratio, ratio)\n",
        "        for ratio in ratios])\n",
        "\n",
        "        self.blocks.append(nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            Conv(c1, c2, 1)\n",
        "        ))\n",
        "        self.conv = Conv(c2 * (len(ratios) + 1), c2, 1)\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        contexts = []\n",
        "        for blk in self.blocks:\n",
        "            contexts.append(F.interpolate(blk(x), x.shape[2:], mode='bilinear', align_corners=False))\n",
        "\n",
        "        x = self.conv(torch.cat(contexts, dim=1))\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, backbone_channels, aspp_out_channel=256, decoder_channel=256, low_level_channels=[64, 32]):\n",
        "        super().__init__()\n",
        "        self.aspp = ASPP(backbone_channels[-1], aspp_out_channel)\n",
        "        self.conv = Conv(aspp_out_channel, aspp_out_channel, 1)\n",
        "\n",
        "        self.project8 = Conv(backbone_channels[1], low_level_channels[0], 1)\n",
        "        self.fuse8 = SeparableConv(aspp_out_channel + low_level_channels[0], decoder_channel, 5, 1, 2)\n",
        "\n",
        "        self.project4 = Conv(backbone_channels[0], low_level_channels[1], 1)\n",
        "        self.fuse4 = SeparableConv(decoder_channel + low_level_channels[1], decoder_channel, 5, 1, 2)\n",
        "\n",
        "    def forward(self, features: list) -> Tensor:\n",
        "        x = self.aspp(features[-1])\n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        feat8 = self.project8(features[1])\n",
        "        x = self.fuse8(torch.cat([x, feat8], dim=1))\n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        feat4 = self.project4(features[0])\n",
        "        x = self.fuse4(torch.cat([x, feat4], dim=1))\n",
        "        return x\n",
        "\n",
        "\n",
        "class SemanticHead(nn.Module):\n",
        "    def __init__(self, decoder_channel, head_channel, num_classes):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            SeparableConv(decoder_channel, head_channel, 5, 1, 2),\n",
        "            nn.Conv2d(head_channel, num_classes, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class InstanceHead(nn.Module):\n",
        "    def __init__(self, decoder_channel, head_channel):\n",
        "        super().__init__()\n",
        "        self.center_conv = nn.Sequential(\n",
        "            SeparableConv(decoder_channel, head_channel, 5, 1, 2),\n",
        "            nn.Conv2d(head_channel, 1, 1)\n",
        "        )\n",
        "        self.offset_conv = nn.Sequential(\n",
        "            SeparableConv(decoder_channel, head_channel, 5, 1, 2),\n",
        "            nn.Conv2d(head_channel, 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.center_conv(x), self.offset_conv(x)\n",
        "\n",
        "\n",
        "class PanopticDeepLab(nn.Module):\n",
        "    def __init__(self, variant: str = '50', num_classes: int = 21):\n",
        "        super().__init__()\n",
        "        self.backbone = ResNet(variant)\n",
        "        backbone_channels = [256, 512, 1024]\n",
        "\n",
        "        self.semantic_decoder = Decoder(backbone_channels, 256, 256, [64, 32])\n",
        "        self.instance_decoder = Decoder(backbone_channels, 256, 128, [32, 16])\n",
        "\n",
        "        self.semantic_head = SemanticHead(256, 256, num_classes)\n",
        "        self.instance_head = InstanceHead(128, 32)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        features = self.backbone(x)[:-1]\n",
        "        semantic = self.semantic_decoder(features)\n",
        "        instance = self.instance_decoder(features)\n",
        "\n",
        "        semantic = self.semantic_head(semantic)\n",
        "        center, offset = self.instance_head(instance)\n",
        "\n",
        "        semantic = F.interpolate(semantic, x.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        center = F.interpolate(center, x.shape[-2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        scale = x.shape[-2] // offset.shape[-2]\n",
        "        offset = F.interpolate(offset, x.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        offset *= scale\n",
        "\n",
        "        return semantic, center, offset\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 10\n",
        "model = PanopticDeepLab(variant = '50')\n",
        "model = fcn_resnet50_model.to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 255)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkEKvT0qcm-8"
      },
      "source": [
        "##Train and Test Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyyUKWbacmoM"
      },
      "outputs": [],
      "source": [
        "def calculate_pixel_accuracy(pred, target):\n",
        "    correct_pixels = (pred == target).sum().item()\n",
        "    total_pixels = pred.size(0) * pred.size(1) * pred.size(2)\n",
        "    pixel_accuracy = correct_pixels / total_pixels\n",
        "    return pixel_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO8X0C63c_Hq"
      },
      "outputs": [],
      "source": [
        "def calculate_iou(pred, target, num_classes):\n",
        "    iou_list = []\n",
        "    pred = pred.cpu().numpy()\n",
        "    target = target.cpu().numpy()\n",
        "\n",
        "    for cls in range(num_classes):\n",
        "        pred_inds = (pred == cls)\n",
        "        target_inds = (target == cls)\n",
        "\n",
        "        intersection = np.logical_and(pred_inds, target_inds).sum()\n",
        "        union = np.logical_or(pred_inds, target_inds).sum()\n",
        "\n",
        "        if union == 0:\n",
        "            iou_list.append(float('nan'))  # 예측과 라벨이 모두 없는 경우 NaN으로 처리\n",
        "        else:\n",
        "            iou_list.append(intersection / union)\n",
        "\n",
        "    return np.nanmean(iou_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzwztbiFdGEo"
      },
      "outputs": [],
      "source": [
        "def calculate_mIoU(outputs, labels, num_classes):\n",
        "    iou_list = []\n",
        "    for i in range(outputs.size(0)):\n",
        "        pred = torch.argmax(outputs[i], dim=0)\n",
        "        iou = calculate_iou(pred, labels[i], num_classes)\n",
        "        iou_list.append(iou)\n",
        "    return np.mean(iou_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7iEDfcNdJNi"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_pixel_accuracy = 0.0\n",
        "    num_classes = 21\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        inputs = data[\"image\"].to(device)\n",
        "        labels_semantic_ = data[\"label_semantic\"]\n",
        "        labels_semantic = labels_semantic_.squeeze().to(device)\n",
        "\n",
        "        labels_instance_ = data[\"label_instance\"]\n",
        "        labels_instance = labels_instance_.squeeze().to(device)\n",
        "\n",
        "\n",
        "        # Forward pass\n",
        "        outputs_semantic, outputs_center, outputs_offset = model(inputs)\n",
        "        loss = criterion(outputs_semantic, labels_semantic.long())\n",
        "\n",
        "\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        pixel_acc = calculate_pixel_accuracy(torch.argmax(outputs, dim=1), labels)\n",
        "        running_pixel_accuracy += pixel_acc\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_dataloader)}, Pixel Accuracy: {running_pixel_accuracy / len(train_dataloader)}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_pixel_accuracy = 0.0\n",
        "    val_mIoU = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data in test_dataloader:\n",
        "            inputs = data[\"image\"].to(device)\n",
        "            labels_ = data[\"label\"]\n",
        "            labels = labels_.squeeze().to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            pixel_acc = calculate_pixel_accuracy(torch.argmax(outputs, dim=1), labels)\n",
        "            val_pixel_accuracy += pixel_acc\n",
        "\n",
        "            mIoU = calculate_mIoU(outputs, labels, num_classes)\n",
        "            val_mIoU += mIoU\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss / len(test_dataloader)}, Validation Pixel Accuracy: {val_pixel_accuracy / len(test_dataloader)}, Validation mIoU: {val_mIoU / len(test_dataloader)}\")\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QQ2ozLlj7OJ"
      },
      "source": [
        "## Semi Supervised Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laklWGXMj9un"
      },
      "outputs": [],
      "source": [
        "#합치기 수정\n",
        "import os\n",
        "import torch\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms import functional as F\n",
        "import torch.nn.functional as Fu\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import torch.optim as optim\n",
        "from typing import Optional\n",
        "import copy\n",
        "\n",
        "class CustomTransform:\n",
        "    def __init__(self):\n",
        "        self.resize = transforms.Resize((224, 224))\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "    def __call__(self, img, label):\n",
        "        # Random horizontal flip\n",
        "        if random.random() > 0.5:\n",
        "            img = F.hflip(img)\n",
        "            label = F.hflip(label)\n",
        "\n",
        "        # Random rotation\n",
        "        angle = random.uniform(-10, 10)\n",
        "        img = F.rotate(img, angle)\n",
        "        label = F.rotate(label, angle)\n",
        "\n",
        "        # Resize and to tensor\n",
        "        img = self.to_tensor(self.resize(img))\n",
        "        label = self.to_tensor(self.resize(label))\n",
        "\n",
        "        return img, label\n",
        "\n",
        "class CustomTransform_test:\n",
        "  def __init__(self):\n",
        "        self.resize = transforms.Resize((224, 224))\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "  def __call__(self, img, label):\n",
        "        img = self.to_tensor(self.resize(img))\n",
        "        label = self.to_tensor(self.resize(label))\n",
        "        return img, label\n",
        "\n",
        "\n",
        "class VOC2012(Dataset):\n",
        "    def __init__(self, data_path, label_path, text_file, transform):\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.transform = transform\n",
        "\n",
        "        with open(text_file, 'r') as file:\n",
        "            lines = file.read().splitlines()\n",
        "            for file_name in lines:\n",
        "                dataname_image = file_name + '.jpg'\n",
        "                dataname_label = file_name + '.png'\n",
        "\n",
        "                image_file_path = os.path.join(data_path, dataname_image)\n",
        "                label_file_path = os.path.join(label_path, dataname_label)\n",
        "\n",
        "                if os.path.isfile(image_file_path) and os.path.isfile(label_file_path):\n",
        "                    self.images.append(image_file_path)\n",
        "                    self.labels.append(label_file_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.images[index]).convert(\"RGB\")\n",
        "        label = Image.open(self.labels[index])\n",
        "\n",
        "        img, label = self.transform(img, label)\n",
        "\n",
        "        label = torch.tensor(np.array(label) * 255, dtype=torch.long)\n",
        "        sample = {\"image\": img, \"label\": label}\n",
        "\n",
        "        return sample\n",
        "\n",
        "JPEGImages_path = \"/content/VOC2012/JPEGImages\"\n",
        "SegmentationClass_path = \"/content/VOC2012/SegmentationClass\"\n",
        "pbl_train = \"/content/drive/MyDrive/pbl_train.txt\"\n",
        "pbl_val = \"/content/drive/MyDrive/pbl_val.txt\"\n",
        "\n",
        "Batch_size = 16\n",
        "custom_transform = CustomTransform()\n",
        "custom_transform_test = CustomTransform_test()\n",
        "train_dataloader = DataLoader(VOC2012(data_path=JPEGImages_path, label_path=SegmentationClass_path, text_file=pbl_train, transform=custom_transform_test), shuffle=True, batch_size=Batch_size)\n",
        "test_dataloader = DataLoader(VOC2012(data_path=JPEGImages_path, label_path=SegmentationClass_path, text_file=pbl_val, transform=custom_transform_test), shuffle=False, batch_size=Batch_size)\n",
        "\n",
        "class FCNResNet50(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(FCNResNet50, self).__init__()\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        self.features = nn.Sequential(*list(resnet.children())[:-2])\n",
        "        self.score_conv = nn.Conv2d(2048, num_classes, kernel_size=1)\n",
        "        self.upsample = nn.Upsample(scale_factor=32, mode='bilinear', align_corners=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.score_conv(x)\n",
        "        x = self.upsample(x)\n",
        "        return x\n",
        "\n",
        "num_classes = 21\n",
        "fcn_resnet50_model = FCNResNet50(num_classes)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "learning_rate = 0.00001\n",
        "num_epochs = 10\n",
        "model = fcn_resnet50_model.to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def calculate_pixel_accuracy(pred, target):\n",
        "    correct_pixels = (pred == target).sum().item()\n",
        "    total_pixels = pred.numel()\n",
        "    pixel_accuracy = correct_pixels / total_pixels\n",
        "    return pixel_accuracy\n",
        "\n",
        "def calculate_iou(pred, target, num_classes):\n",
        "    pred = pred.cpu().numpy()\n",
        "    target = target.cpu().numpy()\n",
        "    iou_list = []\n",
        "\n",
        "    for cls in range(num_classes):\n",
        "        pred_inds = (pred == cls)\n",
        "        target_inds = (target == cls)\n",
        "        intersection = np.logical_and(pred_inds, target_inds).sum()\n",
        "        union = np.logical_or(pred_inds, target_inds).sum()\n",
        "        if union == 0:\n",
        "            iou_list.append(float('nan'))\n",
        "        else:\n",
        "            iou_list.append(intersection / union)\n",
        "\n",
        "    return np.nanmean(iou_list)\n",
        "\n",
        "def calculate_mIoU(outputs, labels, num_classes):\n",
        "    iou_list = []\n",
        "    for i in range(outputs.size(0)):\n",
        "        pred = torch.argmax(outputs[i], dim=0)\n",
        "        iou = calculate_iou(pred, labels[i], num_classes)\n",
        "        iou_list.append(iou)\n",
        "    return np.mean(iou_list)\n",
        "\n",
        "def consistency_loss(outputs_list):\n",
        "    consistency_reg = 0.0\n",
        "    n = len(outputs_list)\n",
        "    for j in range(n - 1):\n",
        "        for k in range(j + 1, n):\n",
        "            consistency_reg += torch.mean((outputs_list[j] - outputs_list[k]) ** 2)\n",
        "    #return consistency_reg\n",
        "    return consistency_reg / (n * (n - 1) / 2)\n",
        "\n",
        "# Define the minimum entropy loss function\n",
        "def minimum_entropy_loss(outputs_list):\n",
        "    loss = 0.0\n",
        "    N = len(outputs_list)  # Number of samples\n",
        "    C = outputs_list[0].size(1)  # Number of classes\n",
        "\n",
        "    for outputs in outputs_list:\n",
        "        outputs = outputs.permute(0, 2, 3, 1).reshape(-1, C)  # Reshape for easier processing\n",
        "        for i in range(N):\n",
        "            sum_k = 0.0\n",
        "            for k in range(C):\n",
        "                product_l = 1.0\n",
        "                for l in range(C):\n",
        "                    if l != k:\n",
        "                        product_l *= (1 - outputs[i, l])\n",
        "                sum_k += outputs[i, k] * product_l\n",
        "            loss += -sum_k\n",
        "    return loss\n",
        "\n",
        "to_pil = transforms.ToPILImage()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_pixel_accuracy = 0.0\n",
        "\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        inputs = data[\"image\"].to(device)\n",
        "        labels_ = data[\"label\"]\n",
        "        labels = labels_.squeeze().to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.long())\n",
        "\n",
        "        num_augmentations = 3\n",
        "        augmented_images = [custom_transform(to_pil(copy.deepcopy(inputs[j].cpu())), Image.fromarray(copy.deepcopy(labels[j].cpu().numpy()).astype(np.uint8)))[0].to(device) for j in range(inputs.size(0)) for _ in range(num_augmentations)]\n",
        "        augmented_outputs = [model(aug_img.unsqueeze(0)) for aug_img in augmented_images]\n",
        "\n",
        "        consistency_reg = consistency_loss(augmented_outputs)\n",
        "        #me_loss = minimum_entropy_loss(augmented_outputs)\n",
        "        loss += 0.0001 * (consistency_reg)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        pixel_acc = calculate_pixel_accuracy(torch.argmax(outputs, dim=1), labels)\n",
        "        running_pixel_accuracy += pixel_acc\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_dataloader)}, Pixel Accuracy: {running_pixel_accuracy / len(train_dataloader)}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_pixel_accuracy = 0.0\n",
        "    val_mIoU = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data in test_dataloader:\n",
        "            inputs = data[\"image\"].to(device)\n",
        "            labels_ = data[\"label\"]\n",
        "            labels = labels_.squeeze().to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            pixel_acc = calculate_pixel_accuracy(torch.argmax(outputs, dim=1), labels)\n",
        "            val_pixel_accuracy += pixel_acc\n",
        "\n",
        "            mIoU = calculate_mIoU(outputs, labels, num_classes)\n",
        "            val_mIoU += mIoU\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss / len(test_dataloader)}, Validation Pixel Accuracy: {val_pixel_accuracy / len(test_dataloader)}, Validation mIoU: {val_mIoU / len(test_dataloader)}\")\n",
        "\n",
        "print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTGYTAHnPG2F"
      },
      "source": [
        "#Final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QH5nUcuPQuV"
      },
      "source": [
        "##DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ylokwik1PJvU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f13a7c88-0a61-4d85-a2f0-eaf0c85bc3fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlG2-av1PqOg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "3f1331be-2c42-45a9-d23f-23c2e6fb6f7f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[Errno 107] Transport endpoint is not connected",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1c2b1e053800>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1646\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1701\u001b[0m              \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsrc_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 995\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    996\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_read2\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 107] Transport endpoint is not connected",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1c2b1e053800>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mextract_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1843\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fpclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_write_end_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_fpclose\u001b[0;34m(self, fp)\u001b[0m\n\u001b[1;32m   1941\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileRefCnt\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileRefCnt\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filePassed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1943\u001b[0;31m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 107] Transport endpoint is not connected"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/VOCdevkit.zip\"\n",
        "extract_path = \"/content\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "  zip_ref.extractall(extract_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMGg6ajHPrb-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms import RandomHorizontalFlip, RandomCrop, RandomRotation\n",
        "from torchvision.transforms import functional as F\n",
        "import torch.nn.functional as Fu\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import random\n",
        "import torch.optim as optim\n",
        "from typing import Optional\n",
        "import torchvision.models as models\n",
        "import torchvision.models.segmentation as segmentation\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ochMXXPmPskM"
      },
      "outputs": [],
      "source": [
        "class CustomTransform:\n",
        "    def __init__(self):\n",
        "        self.resize = transforms.Resize((224, 224))\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "    def __call__(self, img, label_semantic, label_instance):\n",
        "        # Random horizontal flip\n",
        "        if random.random() > 0.5:\n",
        "            img = F.hflip(img)\n",
        "            label_semantic = F.hflip(label_semantic)\n",
        "            label_instance = F.hflip(label_instance)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Random vertical flip\n",
        "        if random.random() > 1:\n",
        "            img = F.vflip(img)\n",
        "            label_semantic = F.vflip(label_semantic)\n",
        "            label_instance = F.vflip(label_instance)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Random rotation\n",
        "        #angle = random.uniform(-10, 10)\n",
        "        angle = 0\n",
        "        img = F.rotate(img, angle)\n",
        "        label_semantic = F.rotate(label_semantic, angle)\n",
        "        label_instance = F.rotate(label_instance, angle)\n",
        "\n",
        "\n",
        "\n",
        "        # Resize and to tensor\n",
        "        img = self.to_tensor(self.resize(img))\n",
        "        label_semantic = transforms.PILToTensor()(self.resize(label_semantic))\n",
        "        label_instance = self.to_tensor(self.resize(label_instance))\n",
        "\n",
        "\n",
        "        return img, label_semantic, label_instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qV2dHHNcPt9x"
      },
      "outputs": [],
      "source": [
        "class VOC2012(Dataset):\n",
        "  def __init__(self, data_path, label_semantic_path, label_instance_path,check, text_file, transform ):\n",
        "    self.images = []\n",
        "    self.labels_semantic = []\n",
        "    self.labels_instance = []\n",
        "    self.inst_height = 224\n",
        "    self.inst_width = 224\n",
        "    self.sigma = 4 #for gaussian filter\n",
        "    sigma = self.sigma\n",
        "    size = 6 * sigma + 3\n",
        "    x = np.arange(0, size, 1, float)\n",
        "    y = x[:, np.newaxis]\n",
        "    x0, y0 = 3 * sigma + 1, 3 * sigma + 1\n",
        "    self.g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n",
        "\n",
        "    mean = [0.485, 0.456, 0.406]# [123.675, 116.280, 103.530]\n",
        "    std = [0.229, 0.224, 0.225]#[58.395, 57.120, 57.375]\n",
        "    self.transform = transform\n",
        "\n",
        "    with open(text_file, 'r') as file:\n",
        "      lines = file.read().splitlines()\n",
        "      for file_name in lines:\n",
        "        dataname_image = file_name + '.jpg'\n",
        "        dataname_label =  file_name + '.png'\n",
        "\n",
        "        image_file_path = os.path.join(data_path, dataname_image)\n",
        "        label_file_path_semantic = os.path.join(label_semantic_path, dataname_label)\n",
        "        label_file_path_instance =os.path.join(label_instance_path, dataname_label)\n",
        "        #print(image_file_path)\n",
        "        #print(label_file_path)\n",
        "        if (os.path.isfile(image_file_path) and os.path.isfile(label_file_path_semantic)):\n",
        "          self.images.append(image_file_path)\n",
        "          self.labels_semantic.append(label_file_path_semantic)\n",
        "          self.labels_instance.append(label_file_path_instance)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def get_instances(self, inst_seg):\n",
        "    center = np.zeros((1, self.inst_height, self.inst_width), dtype=np.float32)\n",
        "    offset = np.zeros((2, self.inst_height, self.inst_width), dtype=np.float32)\n",
        "    y_coord = np.ones_like(inst_seg, dtype=np.float32)\n",
        "    x_coord = np.ones_like(inst_seg, dtype=np.float32)\n",
        "    y_coord = np.cumsum(y_coord, axis=0) - 1\n",
        "    x_coord = np.cumsum(x_coord, axis=1) - 1\n",
        "\n",
        "    #print(np.unique(inst_seg)[1])\n",
        "    i = 0\n",
        "    #list = []\n",
        "\n",
        "    for seg_id in np.unique(inst_seg)[1:]:\n",
        "      if seg_id == 1:\n",
        "        break\n",
        "      mask_index = np.where(inst_seg == seg_id)\n",
        "      center_y, center_x = np.mean(mask_index[1]), np.mean(mask_index[2])\n",
        "\n",
        "      i = i+1\n",
        "      #print(seg_id)\n",
        "\n",
        "\n",
        "      y, x = int(center_y), int(center_x)\n",
        "      #print(x,y)\n",
        "      ul = int(np.round(x - 3 * self.sigma - 1)), int(np.round(y - 3 * self.sigma - 1))\n",
        "      br = int(np.round(x + 3 * self.sigma + 2)), int(np.round(y + 3 * self.sigma + 2))\n",
        "      c, d = max(0, -ul[0]), min(br[0], self.inst_width) - ul[0]\n",
        "      a, b = max(0, -ul[1]), min(br[1], self.inst_height) - ul[1]\n",
        "\n",
        "      cc, dd = max(0, ul[0]), min(br[0], self.inst_width)\n",
        "      aa, bb = max(0, ul[1]), min(br[1], self.inst_height)\n",
        "      center[0, aa:bb, cc:dd] = np.maximum(center[0, aa:bb, cc:dd], self.g[a:b, c:d])\n",
        "\n",
        "      offset_y_index = (np.zeros_like(mask_index[1]), mask_index[1], mask_index[2])\n",
        "      offset_x_index = (np.ones_like(mask_index[1]), mask_index[1], mask_index[2])\n",
        "      offset[offset_y_index] = center_y - y_coord[mask_index]\n",
        "      offset[offset_x_index] = center_x - x_coord[mask_index]\n",
        "\n",
        "\n",
        "    return center, offset\n",
        "\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img = Image.open(self.images[index]).convert(\"RGB\")\n",
        "\n",
        "    #img = transform_custom(img)\n",
        "    #print(\"execute\")\n",
        "\n",
        "    label_semantic = Image.open(self.labels_semantic[index])\n",
        "    label_instance = Image.open(self.labels_instance[index])\n",
        "\n",
        "    #label = transform_custom(label)\n",
        "    img, label_semantic, label_instance = self.transform(img, label_semantic, label_instance)\n",
        "    if(img == None or label_semantic == None or label_instance == None):\n",
        "      print(\"error\")\n",
        "\n",
        "\n",
        "    instance_center, instance_offset = self.get_instances(label_instance)\n",
        "\n",
        "    label_semantic = torch.tensor(np.array(label_semantic), dtype = torch.long)\n",
        "    label_instance = torch.tensor(np.array(label_instance)*255, dtype = torch.long)\n",
        "    #, \"label_\": label_\n",
        "    sample = {\"image\" : img , \"label_semantic\" : label_semantic, \"label_instance\" : label_instance, \"label_offset\" : instance_offset,\"label_center\" : instance_center}\n",
        "\n",
        "    return sample\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56OWyHA8Pwy6"
      },
      "outputs": [],
      "source": [
        "JPEGImages_path = \"/content/VOCdevkit/VOC2012/JPEGImages\"\n",
        "Semantic_path = \"/content/VOCdevkit/VOC2012/SegmentationClass\"\n",
        "Instance_path = \"/content/VOCdevkit/VOC2012/SegmentationObject\"\n",
        "\n",
        "pbl_train = \"/content/drive/MyDrive/pbl_train.txt\"\n",
        "pbl_val = \"/content/drive/MyDrive/pbl_val.txt\"\n",
        "\n",
        "Batch_size = 16\n",
        "custom_transform = CustomTransform()\n",
        "train_dataloader = DataLoader( VOC2012(data_path = JPEGImages_path, label_semantic_path = Semantic_path, label_instance_path = Instance_path, check = True, text_file= pbl_train, transform = custom_transform) , shuffle = True, batch_size = Batch_size)\n",
        "test_dataloader = DataLoader( VOC2012(data_path = JPEGImages_path, label_semantic_path = Semantic_path, label_instance_path = Instance_path, check = False, text_file= pbl_val, transform = custom_transform) , shuffle = False, batch_size = Batch_size)\n",
        "\n",
        "def imshow(img):\n",
        "    # Unnormalize\n",
        "    img = img / 2 + 0.5\n",
        "\t# convert to Numpy\n",
        "    npimg = img.numpy()\n",
        "\t# Transpose to get the correct color\n",
        "    npimg=np.transpose(npimg, (1, 2, 0))\n",
        "    plt.imshow(npimg)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sb-FuD13PyF1"
      },
      "outputs": [],
      "source": [
        "sample = next(iter(train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLyIwD-sP6ng"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "batch_index = 1\n",
        "\n",
        "img = sample['image'][batch_index].permute(1, 2, 0).numpy()\n",
        "label_instance = sample['label_instance'][batch_index].squeeze().numpy()\n",
        "\n",
        "center = sample['label_center'][batch_index, 0]\n",
        "offset = sample['label_offset'][batch_index]\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "axes[0].imshow(img)\n",
        "axes[0].set_title(\"Image\")\n",
        "\n",
        "axes[1].imshow(label_instance, cmap='gray')\n",
        "axes[1].set_title(\"Instance Label\")\n",
        "\n",
        "axes[2].imshow(center, cmap='hot')\n",
        "axes[2].set_title(\"Instance Center\")\n",
        "\n",
        "offset_x = offset[0]\n",
        "offset_y = offset[1]\n",
        "X, Y = np.meshgrid(np.arange(offset_x.shape[1]), np.arange(offset_x.shape[0]))\n",
        "axes[3].quiver(X, Y, offset_x, offset_y)\n",
        "axes[3].set_title(\"Instance Offset\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6dhrK2iQBwx"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0OvaaZdQDVd"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "learning_rate = 0.000009\n",
        "num_epochs = 30\n",
        "#DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1\n",
        "deeplabv3_model = segmentation.deeplabv3_resnet101(pretrained =True)\n",
        "\n",
        "\n",
        "classifier_layers = list(deeplabv3_model.classifier.children())\n",
        "\n",
        "if len(classifier_layers) >= 2:\n",
        "    # Create a new sequential block with dropout inserted\n",
        "    to_dropout = nn.Sequential(\n",
        "        classifier_layers[0],\n",
        "        nn.Dropout(0.5),\n",
        "        classifier_layers[1]\n",
        "    )\n",
        "    # If there are more layers, append them\n",
        "    if len(classifier_layers) > 2:\n",
        "        to_dropout = nn.Sequential(\n",
        "            to_dropout,\n",
        "            *classifier_layers[2:]\n",
        "        )\n",
        "else:\n",
        "    raise ValueError(\"The classifier does not have enough layers to insert dropout.\")\n",
        "\n",
        "deeplabv3_model.classifier = to_dropout\n",
        "\n",
        "model = deeplabv3_model.to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 255)\n",
        "\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay = 0.0001)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pms7tqFGQRVI"
      },
      "source": [
        "##Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fnFmBvKQZDL"
      },
      "outputs": [],
      "source": [
        "def calculate_pixel_accuracy(pred, target):\n",
        "    correct_pixels = (pred == target).sum().item()\n",
        "    total_pixels = pred.size(0) * pred.size(1) * pred.size(2)\n",
        "    pixel_accuracy = correct_pixels / total_pixels\n",
        "    return pixel_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbpOtPAKQQir"
      },
      "outputs": [],
      "source": [
        "def calculate_iou(pred, target, num_classes, ignore_index=255):\n",
        "    iou_list = []\n",
        "    pred = pred.cpu().numpy()\n",
        "    target = target.cpu().numpy()\n",
        "\n",
        "    for cls in range(num_classes):\n",
        "        if cls == ignore_index:\n",
        "            continue\n",
        "\n",
        "        pred_inds = (pred == cls)\n",
        "        target_inds = (target == cls)\n",
        "\n",
        "        intersection = np.logical_and(pred_inds, target_inds).sum()\n",
        "        union = np.logical_or(pred_inds, target_inds).sum()\n",
        "\n",
        "        if union == 0:\n",
        "            iou_list.append(float('nan'))\n",
        "        else:\n",
        "            iou_list.append(intersection / union)\n",
        "\n",
        "    return np.nanmean(iou_list)\n",
        "\n",
        "def calculate_mIoU(outputs, labels, num_classes, ignore_index=255):\n",
        "    iou_list = []\n",
        "    for i in range(outputs.size(0)):\n",
        "        pred = torch.argmax(outputs[i], dim=0)\n",
        "        iou = calculate_iou(pred, labels[i], num_classes, ignore_index)\n",
        "        iou_list.append(iou)\n",
        "    return np.mean(iou_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CvRRjRiQZsl"
      },
      "outputs": [],
      "source": [
        "class SegMetrics:\n",
        "    def __init__(self, num_classes):\n",
        "        self.num_classes = num_classes\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.confusion_matrix = np.zeros((self.num_classes, self.num_classes))\n",
        "\n",
        "    def update(self, preds, labels):\n",
        "        preds = torch.argmax(preds, dim=1).cpu().numpy()\n",
        "        labels = labels.cpu().numpy()\n",
        "        self.confusion_matrix += self._confusion_matrix(labels, preds)\n",
        "\n",
        "    def _confusion_matrix(self, labels, preds):\n",
        "        mask = (labels >= 0) & (labels < self.num_classes)\n",
        "        cm = np.bincount(\n",
        "            self.num_classes * labels[mask].astype(int) + preds[mask],\n",
        "            minlength=self.num_classes ** 2,\n",
        "        ).reshape(self.num_classes, self.num_classes)\n",
        "        return cm\n",
        "\n",
        "    def get_result(self):\n",
        "        pa = np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\n",
        "        miou = np.mean(np.diag(self.confusion_matrix) / (self.confusion_matrix.sum(1) + self.confusion_matrix.sum(0) - np.diag(self.confusion_matrix)))\n",
        "        return pa, miou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lih1VfXqQbFc"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_pixel_accuracy = 0.0\n",
        "    num_classes = 21\n",
        "    metric = SegMetrics(num_classes=num_classes)\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        inputs = data[\"image\"].to(device)\n",
        "        labels_semantic_ = data[\"label_semantic\"]\n",
        "        labels_semantic = labels_semantic_.squeeze().to(device)\n",
        "        labels_offset = data[\"label_offset\"].to(device)\n",
        "        labels_center = data[\"label_center\"].to(device)\n",
        "\n",
        "        # labels_instance_ = data[\"label_instance\"]\n",
        "        # labels_instance = labels_instance_.squeeze().to(device)\n",
        "\n",
        "\n",
        "        # Forward pass\n",
        "        outputs_semantic = model(inputs)['out']\n",
        "        loss_semantic = criterion(outputs_semantic, labels_semantic.long()) ## torch.argmax(outputs_semantic, dim =1)??\n",
        "        # loss_offset = L1Loss(outputs_center, labels_offset)\n",
        "        # loss_center = MSELoss(outputs_offset, labels_center)\n",
        "\n",
        "        loss = loss_semantic# + 0*loss_offset + 0*loss_center\n",
        "\n",
        "        ### Center prediction은 히트맵 형태로 표현됨 [N, 1, H, W]\n",
        "        ### Offset prediction은 offset 벡터는 중심점까지의 x,y 방향의 거리 [N, 2, H, W]\n",
        "        ### grouping pixels into instances\n",
        "        ### instance segmentation map\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        #pixel_acc = calculate_pixel_accuracy(torch.argmax(outputs_semantic, dim=1), labels_semantic)\n",
        "        #running_pixel_accuracy += pixel_acc\n",
        "        metric.update(outputs_semantic, labels_semantic)\n",
        "\n",
        "    train_pa, train_miou = metric.get_result()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_dataloader)}, Pixel Accuracy: {train_pa}, mIoU: {train_miou}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_pixel_accuracy = 0.0\n",
        "    val_mIoU = 0.0\n",
        "\n",
        "    scheduler.step()\n",
        "    metric.reset()\n",
        "    with torch.no_grad():\n",
        "        for data in test_dataloader:\n",
        "            inputs = data[\"image\"].to(device)\n",
        "            labels_semantic = data[\"label_semantic\"]\n",
        "            labels = labels_semantic.squeeze().to(device)\n",
        "\n",
        "            outputs = model(inputs)['out']\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            pixel_acc = calculate_pixel_accuracy(torch.argmax(outputs, dim=1), labels)\n",
        "            val_pixel_accuracy += pixel_acc\n",
        "\n",
        "            mIoU = calculate_mIoU(outputs, labels, num_classes)\n",
        "            val_mIoU += mIoU\n",
        "\n",
        "            metric.update(outputs, labels)\n",
        "\n",
        "    val_pa, val_miou = metric.get_result()\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss / len(test_dataloader)}, Validation Pixel Accuracy: {val_pa}, Validation mIoU: {val_miou}\")\n",
        "\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs-l2CThQgEA"
      },
      "source": [
        "##Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwzFvNPlQtuM"
      },
      "source": [
        "###Panoptic Deeplab + Xception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TIdQX2DQjXX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "class ConvBnRelu(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, ksize, stride, pad, dilation=1,\n",
        "                 groups=1, has_bn=True, norm_layer=nn.BatchNorm2d, bn_eps=1e-5,\n",
        "                 has_relu=True, inplace=True, has_bias=False):\n",
        "        super(ConvBnRelu, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=ksize,\n",
        "                              stride=stride, padding=pad,\n",
        "                              dilation=dilation, groups=groups, bias=has_bias)\n",
        "        self.has_bn = has_bn\n",
        "        if self.has_bn:\n",
        "            self.bn = norm_layer(out_planes, eps=bn_eps)\n",
        "        self.has_relu = has_relu\n",
        "        if self.has_relu:\n",
        "            self.relu = nn.ReLU(inplace=inplace)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.has_bn:\n",
        "            x = self.bn(x)\n",
        "        if self.has_relu:\n",
        "            x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class _ASPPModule(nn.Module):\n",
        "    def __init__(self, inplanes, planes, kernel_size, padding, dilation, BatchNorm):\n",
        "        super(_ASPPModule, self).__init__()\n",
        "        self.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n",
        "                                     stride=1, padding=padding, dilation=dilation, bias=False)\n",
        "        self.bn = BatchNorm(planes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.atrous_conv(x)\n",
        "        x = self.bn(x)\n",
        "        return self.relu(x)\n",
        "\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self, backbone, output_stride, BatchNorm):\n",
        "        super(ASPP, self).__init__()\n",
        "        if backbone == 'resnet':\n",
        "            inplanes = 2048\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        if output_stride == 16:\n",
        "            dilations = [1, 3, 6, 12]\n",
        "        elif output_stride == 8:\n",
        "            dilations = [1, 6, 12, 18]\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.aspp1 = _ASPPModule(inplanes, 256, 1, padding=0, dilation=dilations[0], BatchNorm=BatchNorm)\n",
        "        self.aspp2 = _ASPPModule(inplanes, 256, 3, padding=dilations[1], dilation=dilations[1], BatchNorm=BatchNorm)\n",
        "        self.aspp3 = _ASPPModule(inplanes, 256, 3, padding=dilations[2], dilation=dilations[2], BatchNorm=BatchNorm)\n",
        "        self.aspp4 = _ASPPModule(inplanes, 256, 3, padding=dilations[3], dilation=dilations[3], BatchNorm=BatchNorm)\n",
        "\n",
        "        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n",
        "                                             nn.Conv2d(inplanes, 256, 1, stride=1, bias=False),\n",
        "                                             BatchNorm(256),\n",
        "                                             nn.ReLU())\n",
        "        self.conv1 = nn.Conv2d(1280, 256, 1, bias=False)\n",
        "        self.bn1 = BatchNorm(256)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.aspp1(x)\n",
        "        x2 = self.aspp2(x)\n",
        "        x3 = self.aspp3(x)\n",
        "        x4 = self.aspp4(x)\n",
        "        x5 = self.global_avg_pool(x)\n",
        "        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)\n",
        "        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        return self.relu(x)\n",
        "\n",
        "class space_to_dense(nn.Module):\n",
        "    def __init__(self, stride):\n",
        "        super(space_to_dense, self).__init__()\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, input):\n",
        "        assert len(input.shape) == 4, \"input tensor must be 4 dimensions\"\n",
        "        stride = self.stride\n",
        "        B, C, W, H = input.shape\n",
        "        assert (W % stride == 0 and H % stride == 0), \"the W = {} or H = {} must be divided by {}\".format(W, H, stride)\n",
        "        ws = W // stride\n",
        "        hs = H // stride\n",
        "        x = input.view(B, C, hs, stride, ws, stride).transpose(3, 4).contiguous()\n",
        "        x = x.view(B, C, hs * ws, stride * stride).transpose(2, 3).contiguous()\n",
        "        x = x.view(B, C, stride * stride, hs, ws).transpose(1, 2).contiguous()\n",
        "        x = x.view(B, stride * stride * C, hs, ws)\n",
        "        return x\n",
        "\n",
        "class dense_to_space(nn.Module):\n",
        "    def __init__(self, stride):\n",
        "        super(dense_to_space, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.ps = torch.nn.PixelShuffle(stride)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.ps(input)\n",
        "\n",
        "class deeperlab(nn.Module):\n",
        "    def __init__(self, inplane, outplane, criterion=None, aux_criterion=None, area_alpa=None,\n",
        "                 pretrained_model=None,\n",
        "                 norm_layer=nn.BatchNorm2d, detection=False):\n",
        "        super(deeperlab, self).__init__()\n",
        "        self.backbone = models.resnet101(pretrained=True)\n",
        "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
        "        self.business_layer = []\n",
        "        self.s2d = space_to_dense(4)\n",
        "        self.d2s = torch.nn.PixelShuffle(upscale_factor=4)\n",
        "        self.aspp = ASPP(\"resnet\", 8, norm_layer)\n",
        "        self.conv1 = ConvBnRelu(256, 32, 1, 1, 0, norm_layer=norm_layer, bn_eps=1e-5)\n",
        "        self.conv2 = ConvBnRelu(256 + 2048, 4096, 3, 1, 1, norm_layer=norm_layer, bn_eps=1e-5)\n",
        "        self.conv3 = ConvBnRelu(4096, 4096, 3, 1, 1, norm_layer=norm_layer, bn_eps=1e-5)\n",
        "        self.seg_conv = deeperlab_seg_head(256, outplane, 4, norm_layer=norm_layer)\n",
        "        self.business_layer.append(self.s2d)\n",
        "        self.business_layer.append(self.d2s)\n",
        "        self.business_layer.append(self.aspp)\n",
        "        self.business_layer.append(self.conv1)\n",
        "        self.business_layer.append(self.conv2)\n",
        "        self.business_layer.append(self.conv3)\n",
        "        self.business_layer.append(self.seg_conv)\n",
        "        self.criterion = criterion\n",
        "\n",
        "    def forward(self, input, label=None, aux_label=None):\n",
        "        low_level = self.backbone[:6](input)\n",
        "        high_level = self.backbone[6:](low_level)\n",
        "        high_level = self.aspp(high_level)\n",
        "        low_level = self.conv1(low_level)\n",
        "        low_level = self.s2d(low_level)\n",
        "        decode = torch.cat((high_level, low_level), dim=1)\n",
        "        decode = self.conv2(decode)\n",
        "        decode = self.conv3(decode)\n",
        "        decode = self.d2s(decode)\n",
        "        pre = self.seg_conv(decode)\n",
        "        if label is not None:\n",
        "            loss = self.criterion(pre, label)\n",
        "            return loss\n",
        "        return F.log_softmax(pre, dim=1)\n",
        "\n",
        "class deeperlab_seg_head(nn.Module):\n",
        "    def __init__(self, inplane, outplane, scale=4, norm_layer=nn.BatchNorm2d):\n",
        "        super(deeperlab_seg_head, self).__init__()\n",
        "        self.conv = ConvBnRelu(inplane, 256, 7, 1, 3, norm_layer=norm_layer, bn_eps=1e-5)\n",
        "        self.conv_seg = nn.Conv2d(256, outplane, kernel_size=1, stride=1, padding=0)\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.conv_seg(x)\n",
        "        x = F.interpolate(x, scale_factor=self.scale, mode='bilinear', align_corners=True)\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = deeperlab(inplane=3, outplane=21).to(device)  # Adjust outplane as needed\n",
        "x = torch.randn(1, 3, 256, 256).to(device)  # Example input, moved to the appropriate device\n",
        "label = torch.randint(0, 21, (1, 256, 256)).to(device)  # Example label, moved to the appropriate device\n",
        "\n",
        "\n",
        "print(x.shape)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "output = model(x)\n",
        "loss = criterion(output, label)\n",
        "print(\"Loss:\", loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WA-kPmGRA8d"
      },
      "source": [
        "###Panoptic Deeplab + Resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn-JAzJ-RGyC"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"2 Layer No Expansion Block\n",
        "    \"\"\"\n",
        "    expansion: int = 1\n",
        "    def __init__(self, c1, c2, s=1, downsample= None) -> None:\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(c1, c2, 3, s, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(c2)\n",
        "        self.conv2 = nn.Conv2d(c2, c2, 3, 1, 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(c2)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "        out = Fu.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        if self.downsample is not None: identity = self.downsample(x)\n",
        "        out += identity\n",
        "        return Fu.relu(out)\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    \"\"\"3 Layer 4x Expansion Block\n",
        "    \"\"\"\n",
        "    expansion: int = 4\n",
        "    def __init__(self, c1, c2, s=1, downsample=None) -> None:\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(c1, c2, 1, 1, 0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(c2)\n",
        "        self.conv2 = nn.Conv2d(c2, c2, 3, s, 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(c2)\n",
        "        self.conv3 = nn.Conv2d(c2, c2 * self.expansion, 1, 1, 0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(c2 * self.expansion)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "        out = Fu.relu(self.bn1(self.conv1(x)))\n",
        "        out = Fu.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        if self.downsample is not None: identity = self.downsample(x)\n",
        "        out += identity\n",
        "        return Fu.relu(out)\n",
        "\n",
        "\n",
        "resnet_settings = {\n",
        "    '18': [BasicBlock, [2, 2, 2, 2]],\n",
        "    '34': [BasicBlock, [3, 4, 6, 3]],\n",
        "    '50': [Bottleneck, [3, 4, 6, 3]],\n",
        "    '101': [Bottleneck, [3, 4, 23, 3]],\n",
        "    '152': [Bottleneck, [3, 8, 36, 3]]\n",
        "}\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, model_name: str = '101') -> None:\n",
        "        super().__init__()\n",
        "        assert model_name in resnet_settings.keys(), f\"ResNet model name should be in {list(resnet_settings.keys())}\"\n",
        "        block, depths = resnet_settings[model_name]\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, 7, 2, 3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.inplanes)\n",
        "        self.maxpool = nn.MaxPool2d(3, 2, 1)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, depths[0], s=1)\n",
        "        self.layer2 = self._make_layer(block, 128, depths[1], s=2)\n",
        "        self.layer3 = self._make_layer(block, 256, depths[2], s=2)\n",
        "        self.layer4 = self._make_layer(block, 512, depths[3], s=2)\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, planes, depth, s=1) -> nn.Sequential:\n",
        "        downsample = None\n",
        "        if s != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion, 1, s, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion)\n",
        "            )\n",
        "        layers = nn.Sequential(\n",
        "            block(self.inplanes, planes, s, downsample),\n",
        "            *[block(planes * block.expansion, planes) for _ in range(1, depth)]\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        return layers\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.maxpool(Fu.relu(self.bn1(self.conv1(x))))   # [1, 64, H/4, W/4]\n",
        "        x1 = self.layer1(x)  # [1, 64/256, H/4, W/4]\n",
        "        x2 = self.layer2(x1)  # [1, 128/512, H/8, W/8]\n",
        "        x3 = self.layer3(x2)  # [1, 256/1024, H/16, W/16]\n",
        "        x4 = self.layer4(x3)  # [1, 512/2048, H/32, W/32]\n",
        "        return x1, x2, x3, x4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iz4PmxSxRNjc"
      },
      "outputs": [],
      "source": [
        "class Conv(nn.Sequential):\n",
        "    def __init__(self, c1, c2, k, s=1, p=0, d=1, g=1):\n",
        "        super().__init__(\n",
        "            nn.Conv2d(c1, c2, k, s, p, d, g, bias=False),\n",
        "            nn.BatchNorm2d(c2),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "\n",
        "class SeparableConv(nn.Module):\n",
        "    def __init__(self, c1, c2, k, s=1, p=0, d=1, g=1):\n",
        "        super().__init__()\n",
        "        self.dw_conv = Conv(c1, c1, k, s, p, g=g)\n",
        "        self.pw_conv = Conv(c1, c2, 1)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.dw_conv(x)\n",
        "        x = self.pw_conv(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8y1zcmbRQP_"
      },
      "outputs": [],
      "source": [
        "class ASPP(nn.Module):\n",
        "    def __init__(self, c1, c2, drop_rate=0.1):\n",
        "        super().__init__()\n",
        "        ratios = [1, 6, 12, 18]\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Conv(c1, c2, 1 if ratio==1 else 3, 1, 0 if ratio==1 else ratio, ratio)\n",
        "        for ratio in ratios])\n",
        "\n",
        "        self.blocks.append(nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            Conv(c1, c2, 1)\n",
        "        ))\n",
        "        self.conv = Conv(c2 * (len(ratios) + 1), c2, 1)\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        contexts = []\n",
        "        for blk in self.blocks:\n",
        "            contexts.append(Fu.interpolate(blk(x), x.shape[2:], mode='bilinear', align_corners=False))\n",
        "\n",
        "        x = self.conv(torch.cat(contexts, dim=1))\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, backbone_channels, aspp_out_channel, decoder_channel, low_level_channels=[64, 32]):\n",
        "        super().__init__()\n",
        "        self.aspp = ASPP(backbone_channels[-1], aspp_out_channel)\n",
        "        self.conv = Conv(aspp_out_channel, aspp_out_channel, 1)\n",
        "\n",
        "        self.project8 = Conv(backbone_channels[1], low_level_channels[0], 1)\n",
        "        self.fuse8 = SeparableConv(aspp_out_channel + low_level_channels[0], decoder_channel, 5, 1, 2)\n",
        "\n",
        "        self.project4 = Conv(backbone_channels[0], low_level_channels[1], 1)\n",
        "        self.fuse4 = SeparableConv(decoder_channel + low_level_channels[1], decoder_channel, 5, 1, 2)\n",
        "\n",
        "    def forward(self, features: list) -> Tensor:\n",
        "        x = self.aspp(features[-1])\n",
        "        x = Fu.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        feat8 = self.project8(features[1])\n",
        "        x = self.fuse8(torch.cat([x, feat8], dim=1))\n",
        "        x = Fu.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        feat4 = self.project4(features[0])\n",
        "        x = self.fuse4(torch.cat([x, feat4], dim=1))\n",
        "        return x\n",
        "\n",
        "\n",
        "class SemanticHead(nn.Module):\n",
        "    def __init__(self, decoder_channel, head_channel, num_classes):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            SeparableConv(decoder_channel, head_channel, 5, 1, 2),\n",
        "            nn.Conv2d(head_channel, num_classes, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class InstanceHead(nn.Module):\n",
        "    def __init__(self, decoder_channel, head_channel):\n",
        "        super().__init__()\n",
        "        self.center_conv = nn.Sequential(\n",
        "            SeparableConv(decoder_channel, head_channel, 5, 1, 2),\n",
        "            nn.Conv2d(head_channel, 1, 1)\n",
        "        )\n",
        "        self.offset_conv = nn.Sequential(\n",
        "            SeparableConv(decoder_channel, head_channel, 5, 1, 2),\n",
        "            nn.Conv2d(head_channel, 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.center_conv(x), self.offset_conv(x)\n",
        "\n",
        "## semantic segmantation head는 typical design of semantic segmentation과 동일\n",
        "## instancehead 문제점.. class agnostic, involving a simple instance center regression\n",
        "\n",
        "class PanopticDeepLab(nn.Module):\n",
        "    def __init__(self, model, num_classes: int = 21):\n",
        "        super().__init__()\n",
        "        self.backbone = model\n",
        "        backbone_channels = [256, 512, 1024]\n",
        "\n",
        "        decoder_output_channels = 256\n",
        "\n",
        "        self.semantic_decoder = Decoder(backbone_channels, decoder_output_channels, decoder_output_channels, [64, 32])\n",
        "        self.instance_decoder = Decoder(backbone_channels, 256, 128, [32, 16])\n",
        "\n",
        "        self.semantic_head = SemanticHead(decoder_output_channels, decoder_output_channels, num_classes)\n",
        "        self.instance_head = InstanceHead(128, 32)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        features = self.backbone(x)[:-1]\n",
        "        semantic = self.semantic_decoder(features)\n",
        "        instance = self.instance_decoder(features)\n",
        "\n",
        "        semantic = self.semantic_head(semantic)\n",
        "        center, offset = self.instance_head(instance)\n",
        "\n",
        "        semantic = Fu.interpolate(semantic, x.shape[-2:], mode = 'bilinear', align_corners=False)\n",
        "        center = Fu.interpolate(center, x.shape[-2:], mode = 'bilinear', align_corners=False)\n",
        "\n",
        "        scale = x.shape[-2] // offset.shape[-2]\n",
        "        offset = Fu.interpolate(offset, x.shape[-2:], mode = 'bilinear', align_corners=False)\n",
        "        offset *= scale\n",
        "\n",
        "        return semantic, center, offset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1\n",
        "deeplabv3_model = segmentation.deeplabv3_resnet101(pretrained=True)\n",
        "backbone_state_dict = deeplabv3_model.backbone.state_dict()\n",
        "\n",
        "model_pretrained = ResNet('101')\n",
        "model_pretrained.load_state_dict(backbone_state_dict)\n",
        "\n",
        "model = PanopticDeepLab(model = model_pretrained).to(device)\n",
        "# learning_rate = 0.00004"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "2ZA6wxzbZhaf",
        "VnUEK7QtZhag",
        "KiPoiA8yZhag",
        "7hvRDMhMbRL-",
        "gS88UrI3b_-g",
        "mYUjcmUydVh3",
        "3cnwL12JaUDx",
        "iDbZpzVhfQjs",
        "s4EFwAEHftow",
        "J0ymWcnBgfct",
        "Xy8IcqSXerzx",
        "6ogs0da8Fexk",
        "IrwMtVd2N9px",
        "ef50uBoUOBPt",
        "Ozh0-Z1wOgg-",
        "XSWq3k7wWdOx",
        "WJhrzDbUaFy6",
        "QwOPdg_MaJPg",
        "OkEKvT0qcm-8",
        "4QQ2ozLlj7OJ",
        "7QH5nUcuPQuV",
        "dwzFvNPlQtuM",
        "6WA-kPmGRA8d"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}