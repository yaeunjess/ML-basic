## ML-basic

</head><body><article id="a35afb17-4dd0-47e3-bcd4-cdaf7cff76b2" class="page sans"><header><div class="page-header-icon undefined"><span class="icon">1️⃣</span></div><h1 class="page-title">MNIST Classification</h1><p class="page-description"></p></header><div class="page-body"><h3 id="f5bbf481-5b05-4256-9c3e-20e2909ec625" class="">목표</h3><p id="fee2cb39-362f-4c8a-9173-11e76b05cbc2" class="">0~9까지의 손글씨에 대한 MNIST 데이터셋을 통해 모델을 학습시킨 후 손글씨에 대한 classification 성능을 accuracy로 확인</p><h3 id="b08fb462-a5b6-4be4-8584-b81c12200bb0" class="">핵심</h3><p id="01a6c576-9472-48a0-857e-c1cf8561d449" class="">✅ 모델 구축</p><p id="83271743-e014-40aa-b942-9b05c44e8d3c" class="">✅ 모델 학습에서의 데이터 증강</p><p id="11c72ca7-cb72-49b1-9ff8-3f9c762eeeeb" class="">✅ 성능 향상을 위한 기법 도입</p><p id="b2d8b92c-7de1-47c0-9e7a-90c480b1773b" class="">✅ 튜닝을 통한 변수 결정</p><h3 id="4b588a97-6e63-431a-a26b-8d726739ae57" class="">모델 구조</h3>

![Untitled](https://github.com/yaeunjess/ML-basic/assets/138097446/30fa8a4e-cbe8-4ec5-bbd1-90d6bb3b9015)

</a></figure><pre id="df583b77-ceee-4d5c-83dc-749db13e32fe" class="code"><code class="language-Python">class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Sequential(
            nn.Conv2d(
                in_channels=1,
                out_channels=32,
                kernel_size=5,
                stride=1),               
            nn.LeakyReLU(),              
            nn.BatchNorm2d(32),          
            nn.MaxPool2d(kernel_size=2), 
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(32, 64, 5, 1),    
            nn.ReLU(),                  
            nn.BatchNorm2d(64),         
            nn.MaxPool2d(2),            
        )
        self.fc1 = nn.Linear(64 * 4 * 4, 128) 
        self.fc2 = nn.Linear(128, 10)         
        self.relu = nn.ReLU()                 
        self.dropout = nn.Dropout(0.5)        
        self.softmax = nn.LogSoftmax(dim=1)   

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)

        x = x.view(x.size(0), -1)  
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.softmax(self.fc2(x))

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)

        x = x.view(x.size(0), -1)  
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.softmax(self.fc2(x))
        
        return x</code></pre><h3 id="27f46d66-3bfe-4163-bb40-f8bbbca4295d" class="">모델 <strong>Hyperparameter </strong>설정</h3><ol type="1" id="d2ee526d-ec0d-4378-ba53-6c33eb0fc1a8" class="numbered-list" start="1"><li><strong>optimizer </strong>: <code>adam</code> &gt; <code>SGD</code></li></ol><ol type="1" id="f16e988b-a4d1-4eaa-94b0-85d48a11d379" class="numbered-list" start="2"><li><strong>loss function</strong> : <code>crossentropy</code></li></ol><ol type="1" id="cfb2905f-116f-4c18-8887-37d0259298ef" class="numbered-list" start="3"><li><strong>activation function </strong>: <code>logsoftmax</code> &gt; <code>softmax</code><figure id="e7806280-067f-43d9-a949-061a6ee7c685" class="code"><code class="language-Python"># CNN class 정의에 쓰인 nn.BatchNorm2d()와 nn.Dropout()
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        .
        .
        .
        self.conv2 = nn.Sequential(
            nn.Conv2d(32, 64, 5, 1),  
            nn.ReLU(), 
            nn.BatchNorm2d(64), # batch normalization
            nn.MaxPool2d(2), 
        )
        .
        .
        self.dropout = nn.Dropout(0.5) # dropout
        .
        .
        (생략)</code></pre></li>
<ol>
    <li>
        <strong>Dropout</strong>
        <figure id="3049bc25-45d1-4d2d-9206-2dda57890cef">
            <img src="https://github.com/yaeunjess/ML-basic/assets/138097446/0b98f37b-ac7a-4373-84d0-eedf4173ee5a" alt="Untitled 2" />
        </figure>
    </li>
    <li>
        <strong>stepLR</strong> 
        <strong>BatchNorm</strong>
        <figure id="3049bc25-45d1-4d2d-9206-2dda57890cef">
            <img src="https://github.com/yaeunjess/ML-basic/assets/138097446/744acd7d-0691-40e3-8558-c9370b624637" alt="Untitled 3" />
            <img src="https://github.com/yaeunjess/ML-basic/assets/138097446/dafb76ab-56a8-4a42-9e0e-e45180633cd0" alt="Untitled 4" />
        </figure>
    </li>
</ol>

                                       
<ol start="3">
    <li>
        <strong>stepLR</strong> <strong>스케줄러</strong>
        <figure id="3049bc25-45d1-4d2d-9206-2dda57890cef">
            <img src="https://github.com/yaeunjess/ML-basic/assets/138097446/f4ae0d6c-fe58-43b7-8635-d22e03f10d5c" alt="Untitled 5"/>
        </figure>
        <pre class="code"><code class="language-Python"># StepLR 스케줄러 설정 방법
optimizer = optim.Adam(model.parameters(), lr=0.00108)
scheduler = StepLR(optimizer, step_size=4, gamma=0.5)</code></pre>
    </li>
</ol>
<h3 id="2c4c8be2-0839-4964-808a-21f397f0a144">...</h3>

<h3 class="">모델 튜닝을 통한 변수 설정</h3><ol type="1" id="b755391f-4618-4bb4-a7fb-c005871259d9" class="numbered-list" start="1"><li><code>batch_size</code> = 100<p id="10a7be94-ed09-476d-b884-6b4d40a2a6b4" class="">64~128 사이로 튜닝을 하다가, 가운데 값인 100으로 설정</p></li></ol><ol type="1" id="dcf81a5b-f727-4b8a-9733-f170c03624ce" class="numbered-list" start="2"><li><code>epoch</code> = 25<p id="acb7b30c-e7fe-4b96-9cf1-3f78476588b3" class="">30을 기준으로 하여 최대 성능이 나오는 지점 부근으로 epoch을 수정했다.</p></li></ol><ol type="1" id="bef3c1d9-2016-4199-918c-3fe3df09d720" class="numbered-list" start="3"><li><code>learning_rate</code> = 0.00108<p id="bf346712-5db1-4562-ba12-174b0076b492" class="">0.001 기준으로 튜닝했다.</p></li></ol><ol type="1" id="050445cf-6fd4-43af-99e9-50d9d822d4fc" class="numbered-list" start="4"><li><code>step_size</code> = 4, <code>gamma</code>= 0.5</li></ol><h3 id="28cccdfe-4443-4fcc-9e23-98df9fa2efa6" class="">데이터 증강</h3><ol type="1" id="0fa2aa6f-3dd5-47c9-bfc7-3ea664873262" class="numbered-list" start="1"><li>데이터 회전 각도 : -13~13도<p id="a030269a-0235-41f3-a7df-7c20c3be4cbc" class="">회전 각에 대해서도 변화를 주고 싶어, 모델이 test했을때 틀린 이미지를 저장하여 확인해가면서 회전 각을 정했다.</p><figure id="0404998c-45d8-49f8-8e81-4d945a93a900"  class="">Randominvert 함수를 이용해 색상 반전을 해보았는데, 효과가 없었다.</p><figure id="287eb2dc-84a9-4b74-b8f5-ef0c0e517cca" class="">ColorJitter 함수를 이용해 이미지의 밝기, 채도 등 컬러 관련 속성들을 변경해봤는데, 효과가 없었다.</p><figure id="145b3da5-ebc4-4873-bfac-a011958abb91" class="">모델별 성능 비교 - optimizer(SGD vs Adam), batch normalization 유무</h3><div id="79f268e4-79b9-4997-9de7-c63acea04590" class="column-list"><div id="ca7080f1-ef00-4ed0-a8eb-61e1358a36a8" style="width:50%" class="column"><figure id="10a66bfd-c099-44e8-bbed-6d71b238c877" class="column-list"><div id="707d3305-efd6-4082-b2b9-f9d8d380a5bd" style="width:50%" class="column"><figure id="af48b0ef-a432-4379-9ce6-db3f8c8192fb"  class="column-list"><div id="0d3c5c53-b3b4-404a-8c39-085181d2a3aa" style="width:50%" class="column"><figure id="96da1668-9aa1-48be-abc5-44e7e475ec3e" class="column"><figure id="e45bfc85-b7d9-4587-a896-dec72878357f"  class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>


</head><body><article id="d3382ef4-b9e0-4ad6-83da-b70348718c90" class="page sans"><header><div class="page-header-icon undefined"><span class="icon">2️⃣</span></div><h1 class="page-title">CIFAR-100 Classification</h1><p class="page-description"></p></header><div class="page-body"><ul id="36cdaa2a-358c-4f02-bd2b-f05964e6adbb" class="toggle"><li><details open=""><summary>발표 흐름</summary><ol type="1" id="c6c7eb8a-263c-42af-8bd8-324c86485a2a" class="numbered-list" start="1"><li>모델 : EfficientNet</li></ol><ol type="1" id="2e9dc78f-c4ed-44b4-b4e4-b4151f43349a" class="numbered-list" start="2"><li>기법<ul id="cf75aa45-41cd-42cd-b0c8-6f5ca4ad90c8" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-default"><del><strong>stochastic depth</strong></del></mark><mark class="highlight-default"><strong> </strong></mark><mark class="highlight-default">: 성능 미미함</mark></li></ul><ul id="be76c8fa-0c57-4f10-bfb0-28122565da46" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-default"><del><strong>앙상블 - 보팅</strong></del></mark><mark class="highlight-default"><strong> : </strong></mark><mark class="highlight-default">과적합일어난 거를 앙상블 하면 더 좋든지 </mark></li></ul><ul id="557a8b06-fe44-4684-95f6-a98f24bc63b2" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-default"><del><strong>progressive learning</strong></del></mark><mark class="highlight-default"> : 실험 힘듦 오래걸림, 실제 논문에서 발표된 학습시간은 350 에포크 가량으로 이번 과제에 적용하기에는 학습 시간 및 총 5회 학습 진행해야된다는 점 감안하여 부적절하다고 판단. 테스트를 진행했을 때는 학습시간을 적게 하여 진행하려 했으나 정확도가 안 나온다.</mark></li></ul><ul id="e4a01f6c-1116-4251-b242-7846a0f21d65" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-default"><strong>scheduler-stepLR()</strong></mark><mark class="highlight-default"> : 성능 좋음, 값이 빠르게 수렴하는걸 도와줌</mark></li></ul><ul id="a78d768d-eab8-48ca-a048-617ddb8fef27" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-default"><strong>데이터 증강</strong></mark><ul id="1878bca2-e304-4de0-b254-c2025edda8b0" class="bulleted-list"><li style="list-style-type:circle"><mark class="highlight-default"><strong>crop</strong></mark></li></ul><ul id="71934c4c-ab6f-489c-9de8-d263c6a34967" class="bulleted-list"><li style="list-style-type:circle"><mark class="highlight-default"><strong>flip</strong></mark></li></ul><ul id="35be27af-c90e-4c2f-957e-8c1391443539" class="bulleted-list"><li style="list-style-type:circle"><mark class="highlight-default"><strong>rotation</strong></mark></li></ul><ul id="889c8d19-262c-4c94-9530-f6eaa7d6670e" class="bulleted-list"><li style="list-style-type:circle"><mark class="highlight-default"><strong>cutmix-0.5</strong></mark></li></ul><ul id="36421b1b-0912-4c8d-83a6-1d6b703694dd" class="bulleted-list"><li style="list-style-type:circle"><mark class="highlight-default"><del><strong>선형 보간법</strong></del></mark></li></ul><ul id="e24ae1a4-6280-42c7-9d40-660085dbd26c" class="bulleted-list"><li style="list-style-type:circle"><del><mark class="highlight-default"><strong>colorjitter</strong></mark></del><mark class="highlight-default"><strong> : </strong></mark><mark class="highlight-default">고유의 색이 물체의 중요한 특징일 경우 성능이 더 안 좋아질 수 있음</mark></li></ul></li></ul><ul id="90943111-6ab3-4c52-8c9e-810ea2b304ef" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-default"><strong>적절한 Hyperparameter </strong></mark><ul id="e0a96872-67c6-4c79-b34b-23c28eab9a81" class="bulleted-list"><li style="list-style-type:circle"><mark class="highlight-default">learning rate 크게 했더니 성능 감소 확인</mark></li></ul><ul id="4fd9dbc3-8270-4a1c-95f9-6c80b025ecda" class="bulleted-list"><li style="list-style-type:circle">batch size : 32, 64는 성능 차이 컸음, m모델 64 &lt; s모델 128</li></ul><ul id="8ce6e5a9-6d49-4bda-b372-9aa7d9854f58" class="bulleted-list"><li style="list-style-type:circle">weight_decay : overfitting 방지로 넣었음</li></ul><ul id="6c69094e-6b8f-43fd-9f65-e2d93f41cd39" class="bulleted-list"><li style="list-style-type:circle">seed로 nparray 랜덤으로 뽑기</li></ul><div id="102f7af7-0d83-4345-ab6e-15d52d41ee1c" class="column-list"><div id="a2a2ce9e-ac38-457b-829f-3979040a58f2" style="width:50%" class="column"><figure id="dec4aca1-62a1-4bac-9f34-dcfbaf9b5a04" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled.png"><img style="width:240px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled.png"/></a></figure></div><div id="3818b11a-26a8-443c-a85c-5cdfd8c52141" style="width:50%" class="column"><figure id="d16f5650-4db8-47e0-8ce8-94c2384b772e" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%201.png"><img style="width:240px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%201.png"/></a></figure></div></div></li></ul></li></ol><ol type="1" id="93d30ad1-594e-41f1-a355-63cbf9958a61" class="numbered-list" start="3"><li>성능</li></ol></details></li></ul><hr id="145fbdd8-7552-41f9-b3da-94b4280539e6"/><h2 id="a677c1d0-f85a-44b0-9f02-3ffa385a1812" class="">1. 모델</h2><h3 id="408d0428-db72-4868-8286-403263ddb0a2" class="block-color-blue_background"><del><mark class="highlight-default"><strong>Ensemble - Bagging</strong></mark></del></h3><p id="0f9eac82-4042-4638-9557-417b9eff00ce" class="">단일 알고리즘 기반의 모델을 다른 데이터를 기반으로 학습시켜 모델의 결과를 결합한다.</p><figure id="4e3617fd-fe0f-4b86-9eb2-73c3fa3ba2d8" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%202.png"><img style="width:384px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%202.png"/></a></figure><p id="c54729f2-3e57-44cf-96f7-50b4bf0a0fba" class="">여러개의 CNN구조  만들고 augment만 다르게 하여서 학습 후 합쳐서 결과 도출</p><figure id="7f0f3b19-552e-40c8-bbc4-a800f81bb492" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%203.png"><img style="width:624px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%203.png"/></a></figure><h3 id="395dbe4c-777c-4d32-af39-574541397116" class="block-color-blue_background">EfficientNetV2</h3><p id="d1bac9de-c3e6-4717-858e-e9db74bf0136" class=""><mark class="highlight-blue"><em><strong>모델 선정 이유</strong></em></mark></p><ul id="62fb95cc-60eb-4078-a4fb-6c78ac803855" class="bulleted-list"><li style="list-style-type:disc">기존 EfficientNet을 개선한 EfficientNetV2로 선정 </li></ul><ul id="7c0d4b02-e437-440d-985f-438847157489" class="bulleted-list"><li style="list-style-type:disc">파라미터 수가 비교적 적고 학습 속도 개선 데이터 확인하여 모델 선정.</li></ul><ul id="90ae3c3e-85a7-4a60-b87b-d6eac767be08" class="bulleted-list"><li style="list-style-type:disc">이때 EfficientNetV2는 기존 EfficientNet의 MBConv를 개선하여 fused MBConv를 1~3 stage에서 사용</li></ul><figure id="33a58b2b-07e7-4430-b237-1aa4c93360f4" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%204.png"><img style="width:384px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%204.png"/></a></figure><ul id="5d95c609-bd5c-4be4-9679-51920d411101" class="bulleted-list"><li style="list-style-type:disc">또한 기존 EfficientNet의 compound scailing으로 모든 스테이지를 동일하게 scailing up 하였지만</li></ul><ul id="f098328f-60ce-49a8-a451-a2d6ea028b50" class="bulleted-list"><li style="list-style-type:disc">이를 개선하고자 non-uniform scaling strategy를 사용하여 stage가 증가할 수록 layer가 증가하는 정도를 높인 방법을 제안함.</li></ul><p id="efc1c423-2b27-4074-a8fe-6029b1a5caf0" class=""><mark class="highlight-blue"><em><strong>모델 적용</strong></em></mark></p><ul id="0b55de44-f5e2-4483-938b-8dd9aa355bde" class="bulleted-list"><li style="list-style-type:disc">여러 자료를 통해 정확도가 더 높으며 training 시간이 적다는 것을 참고하여 테스트 모델로 사용하게 됨.</li></ul><ul id="01a238a7-f04f-4978-ae3b-32b755ee4443" class="bulleted-list"><li style="list-style-type:disc">모델은 s,m,l 사이즈 모두 테스트 진행했음.</li></ul><ul id="fa9b2426-855b-43b3-82c6-1fc22bc870e5" class="bulleted-list"><li style="list-style-type:disc">imagenet1k에 대해 pretrain된 모델 이용하여 학습 진행했음.</li></ul><hr id="a04c0a33-e222-4567-94ce-5f271587153e"/><h2 id="dc96ea9c-a1bd-48dd-b40b-38bae0ee746e" class="">2. 기법</h2><h3 id="c333eb90-4fb6-4016-b6c1-3cc18d21b717" class="block-color-blue_background">적절한 Hyperparameter가 성능에 제일 중요했다!</h3><ul id="f9fd4cff-1cf9-488b-a69f-7c8617f7eaab" class="bulleted-list"><li style="list-style-type:disc">loss function : <code>CrossEntropyLoss()</code></li></ul><ul id="30a76cde-a781-49f4-84ca-45306b2743ca" class="bulleted-list"><li style="list-style-type:disc">optimizer : <code>AdamW()</code>,  Adam 옵티마이저의 변형으로 weight decay를 적용하는 것이 특징임. </li></ul><ul id="8d95426e-61da-462c-aeb7-6b9d0c6416e6" class="bulleted-list"><li style="list-style-type:disc">n_epochs = <code>25</code></li></ul><ul id="2c82db08-35e2-4570-a70c-e641e0a6fbe9" class="bulleted-list"><li style="list-style-type:disc">batch_size = <code>128</code> </li></ul><ul id="243ec438-d31e-4b9c-b689-e2eb199895e7" class="bulleted-list"><li style="list-style-type:disc">scheduler : <code>StepLR()</code> , step size마다 gamma 비율로 lr을 감소시킴.</li></ul><ul id="e07ed212-c230-42a4-ac0f-3d0e07d1f795" class="bulleted-list"><li style="list-style-type:disc">learning_rate : 초기 <code>0.0004</code> 에서 epoch <code>10</code> 간격으로 gamma, <code>0.9</code>가 곱해짐</li></ul><div id="b7f54edc-5a9b-40e2-b72a-6044b37a50af" class="column-list"><div id="ce75b8d6-fac6-4fb2-94a6-06be3d4c6e64" style="width:50%" class="column"><figure id="44004ad2-253f-4416-a340-ff8c6416d930" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled.png"><img style="width:240px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled.png"/></a></figure></div><div id="588832d7-e54d-4bbc-829f-a39430184c66" style="width:50%" class="column"><figure id="c5fc8c13-ad82-4178-9f1b-c5e4041933a8" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%201.png"><img style="width:240px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%201.png"/></a></figure></div></div><ul id="f8327326-9af9-4fb7-87a5-9ce56d23ccb2" class="bulleted-list"><li style="list-style-type:disc">weight_decay : <code>0.001</code>, overfitting 방지로 넣었음.</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="769260f2-8b75-46e4-b68f-f5fa513ef11d" class="code"><code class="language-Python">criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model_to_train.parameters(), lr=0.0004, weight_decay= 0.001)
scheduler = StepLR(optimizer, step_size=10, gamma=0.9)</code></pre><p id="241701c1-e276-483d-bc68-9455ef0708cc" class="">
</p><h3 id="df091ee5-3098-4c48-8d53-b806a3c0f1e4" class="block-color-blue_background"><mark class="highlight-default"><strong>데이터 증강</strong></mark></h3><ol type="1" id="4ec3df62-f001-4d36-bc6f-272251bd56da" class="numbered-list" start="1"><li><strong><del>선형 보간법, Bilinear Interpolation</del></strong><br/><br/><del><code>transforms.Resize((224, 224), interpolation=Image.BILINEAR)</code></del><p id="daac8896-bcb9-4e44-a683-833706b20aa2" class=""> 보간법이란 화소 값을 할당 받지 못한 영상(예 영상의 빈 공간)의 품질은 안 좋을 수밖에 없는데, 이때 빈 화소에 값을 할당하여 좋은 품질의 영상을 만드는 방법을 의미합니다.</p><p id="1ff56c08-59cb-4e88-9ef5-858c38ca4506" class=""> 선형 보간법은 두 축에 대해 선형 보간을 수행합니다. 주변 4개의 픽셀을 사용하여 가중 평균을 계산하여 값을 결정합니다. 확대 및 축소 모두에 사용할 수 있으며, Nearest Neighbor에 비해 부드러운 이미지를 생성합니다.</p><div id="6ed6966c-2246-4e1f-8e1f-eb130c6f9f76" class="column-list"><div id="ba3b0e3e-29d2-4df9-b28d-785a2a8a78c1" style="width:50%" class="column"><figure id="c1c77783-74fc-4628-99b8-677abad05a6e" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%205.png"><img style="width:853px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%205.png"/></a><figcaption>선형 보간법 적용 전</figcaption></figure></div><div id="2321eb48-5c2e-401d-919d-9b6fcdee0c1a" style="width:50%" class="column"><figure id="1f491838-4dad-4dbe-a3aa-55a0ffaa3b07" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%206.png"><img style="width:827px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%206.png"/></a><figcaption>선형 보간법 적용 후</figcaption></figure></div></div><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="2387466f-080e-4512-bba5-b6f5cbc919bd"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">성능이 더 안좋았음.<br/>Bilinear Interpolation을 적용했을 때 기존 Default 값인 Nearest에 비해서 해상도가 더 좋아졌다고 판단하였음. 따라서 이를 적용하여 테스트를 진행하였지만 생각보다 성능 향상이 미미하며 효과가 없다고 판단하여 이를 배제.<br/></div></figure></li></ol><ol type="1" id="d5b21b3a-4fac-45b1-8e49-44b6a2a3d9ac" class="numbered-list" start="2"><li><del><code>ColorJitter()</code></del> : 이미지의 밝기, 채도 등등 컬러 관련 여러 속성들을 임의로 변경한다.<figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="24bc7b54-8c35-4bc5-a249-00b2ce5f0897"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">성능이 더 안 좋았음. <br/>개체를 구분함에 있어서 색깔이 중요한 특징일 경우, 이를 적용하는 게 모델 성능에 더 좋지 않을 거라고 생각.<br/>그러나, 파라미터 수정을 잘 한다면 효과는 있을 것이지만 파라미터 수정을 정확히 하지 않은 경우 오히려 악영향을 끼칠 것.<br/>이유는 아래와 같이 다람쥐 같은 동물의 경우는 개체 인식에 색이 중요한 영향을 끼친 게 원인이라고 판단.<br/></div></figure><figure id="68549283-1d26-4929-b512-1336fa2c7d52" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%207.png"><img style="width:684px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%207.png"/></a></figure></li></ol><ol type="1" id="ff64abaf-c13d-4f96-b025-a1270506b732" class="numbered-list" start="3"><li><code>RandomRotation()</code> : 이미지를 랜덤하게 rotation 한다.<figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="20a1182f-be92-4a88-9318-bc37f812f677"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">일반화 성능에 도움이 될 거라 예상하여 적용.적용 후 0.5% 가량 정확도 상승했으나 이는 오차 범위 이내로 효과가 올랐을 가능성이 있음.</div></figure><div id="f7ee355e-ffd5-4145-87e1-5246dd889dc5" class="column-list"><div id="ca54e44e-bc06-4c19-885c-ffc775f3181a" style="width:50%" class="column"><figure id="1671e004-5e59-4bb5-80e5-ec01dcc330d4" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%208.png"><img style="width:726px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%208.png"/></a></figure></div><div id="cfb2af4c-6b2d-49a0-a41d-6e33ec84db1b" style="width:50%" class="column"><figure id="79dd1242-dec3-4faa-a5af-b67e33511eca" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%209.png"><img style="width:104px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%209.png"/></a></figure></div></div></li></ol><ol type="1" id="6314b634-8125-4b5a-bbef-1529e713d43b" class="numbered-list" start="4"><li><code>RandomHorizontalFlip()</code> : 50프로 확률로 수평으로 뒤집는다. 즉 좌우반전이다. <figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="8bfe0207-fcc1-438e-93a3-3ae923edbed7"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">개체를 구분함에 있어 위/아래의 정보가 중요한 특징일 경우, 모델 성능에 좋지 않을것라고 생각함.<br/>상하반전은 적용하지 않고, 좌우반전을 적용함.<br/></div></figure></li></ol><ol type="1" id="4e6b1cd3-71ca-414b-b472-2216cbd9ac5c" class="numbered-list" start="5"><li><code>RandomCrop()</code> : (width, height)의 잘라낼 크기를 설정하면, 그 크기만큼 랜덤으로 잘라낸다. <p id="8b20fb0e-caf8-44a3-b844-703953846e78" class="">padding_mode = reflect으로 설정하여 아래와 같이 데이터 증강했다. 노이즈 추가를 통해 모델을 일반화시키려 했다. </p><div id="722445f9-d659-40af-9660-9029f1c03fb5" class="column-list"><div id="9e27b634-e679-4d6e-8cd7-161be4f1b36c" style="width:50%" class="column"><figure id="d55cc71d-09f3-4291-a55d-766f1a5ad1fb" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2010.png"><img style="width:834px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2010.png"/></a></figure></div><div id="4ebd5f63-9279-4073-8ce6-4ec2af3fab16" style="width:50%" class="column"><figure id="2b90f9df-9abb-48d5-882d-907a1999079e" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2011.png"><img style="width:317px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2011.png"/></a></figure></div></div></li></ol><ol type="1" id="1ee88255-1d8f-4858-8209-16f3ac5cd6b4" class="numbered-list" start="6"><li><code>cutmix</code> : 하나의 이미지 안에 두개의 레이블을 가진 이미지를 적당한 비율로 배치하여, 두개의 레이블이 하나의 이미지에 비율에 맞게 배치되는 방식이다. (mixup과 cutout의 특징을 합친 방법)<p id="8018ad88-934a-4401-9dc2-fde56a42edf1" class="">아래 제일 오른쪽 이미지를 보면, 고양이 얼굴 : 강아지 몸 = 0.4 : 0.6 비율로 구성되어 있다.</p><figure id="681a7484-49a8-4482-8a1c-a4971b906383" class="image" style="text-align:center"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2012.png"><img style="width:576px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2012.png"/></a></figure><p id="7cc21f9a-d197-409d-bb5b-ec7818bfae31" class="">
</p><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="651a319b-1024-4a66-b7bf-450e2ee8c1af"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">적용 후 2% 가량 정확도 상승.</div></figure></li></ol><p id="06113b64-d7e3-4121-92b5-2589b4baa35c" class="">
</p><h3 id="60fbad46-ac0a-4a5f-a282-b04bc6329086" class="block-color-blue_background"><del><mark class="highlight-default"><strong>stochastic depth</strong></mark></del></h3><ul id="5fd34f66-83a8-4b88-a6f9-e21d2aa69464" class="bulleted-list"><li style="list-style-type:disc">layer의 개수를 overfitting 없이 늘리는 역할 가능 </li></ul><ul id="c0999eb0-5e28-45b1-bf81-9c4041f3a056" class="bulleted-list"><li style="list-style-type:disc">drop out과 유사</li></ul><ul id="b15ead4f-36e8-47d4-ba6b-c6f8081db6d8" class="bulleted-list"><li style="list-style-type:disc">기존의 drop out이 network의 weight에 집중했다면 stochastic depth는 network의 depth를 학습단계에서 random하게 줄이는 것</li></ul><p id="8eb17b8f-f703-40bc-977b-6da4b71c8767" class="">deep network의 vanishing gradient, diminishing feature 문제 해결</p><p id="a17867e4-a9b6-4c90-ac48-2598539140f8" class="">diminishing feature: multiplication과 convolution 연산을 거치며 feature가 손실되는 것</p><figure id="275d3d9b-bf76-4828-9dc2-3e8dd695f396" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2013.png"><img style="width:737px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2013.png"/></a></figure><figure id="e92e1416-d6e5-462f-8015-a1228f38bed5" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2014.png"><img style="width:708px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2014.png"/></a></figure><p id="284ba208-2156-4f30-b5f5-33868444ec26" class="">
</p><p id="0f3e5689-4c9e-4bf5-a4cb-96613599f15b" class="">stochastic depth는CIFAR-10과 같이 비교적 작은 데이터셋에서 network를 복잡하게 만들었을 때 test 정확도의 향상을 기대할 수 있다. </p><p id="5de1ef82-190a-4d2b-87b3-153338c731ae" class="">결론 : 쉬운 데이터, deep network 사용시 효과적</p><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="8fb540bd-0aef-4915-937e-ca995c2405f9"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">성능 차이 미미함. 쉬운 데이터가 아니기 때문이라고 예측. </div></figure><p id="6ce4ff74-4a3d-4375-bc24-58d547358e6b" class="">
</p><h3 id="df44307e-d694-4d80-aee8-d96f4916caaf" class="block-color-blue_background"><del><mark class="highlight-default">Progressive learning with adaptive regularization</mark></del></h3><p id="38a42568-1906-46a3-b1da-40030a70a697" class="">🔗논문 설명 : <a href="https://rauleun.github.io/EfficientNetV2">https://rauleun.github.io/EfficientNetV2</a></p><ul id="f9b7a8fa-5ff0-4dd8-b3e5-fd1dc537d62d" class="bulleted-list"><li style="list-style-type:disc">Progressive learning은 모델의 크기를 줄이기 위함이 아닌 <strong>데이터의 크기, 특히 이미지 자체의 해상도를 줄이는 학습 방법을 제안하면서 등장한 EfficientNet V2에 쓰인 학습 방법</strong>이다. </li></ul><ul id="699a9bdb-ddf0-4699-885e-1056b1611ffe" class="bulleted-list"><li style="list-style-type:disc">기존 EfficientNet을 개선한 EfficientNet V2는 줄어든 크기의 입력 이미지로도 좋은 성능을 얻을 수 있다. </li></ul><figure id="42305a74-ba4d-4827-85bd-3da1af8bf115" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2015.png"><img style="width:432px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2015.png"/></a></figure><ul id="f60ee986-f121-4d56-b89d-19dd178fbec7" class="bulleted-list"><li style="list-style-type:disc">작은 크기의 이미지로 학습할 때는 정보량이 많지 않아 충분히 학습이 가능하기 때문에, regularization의 강도를 줄여야 한다. 반면 큰 크기의 이미지로 학습할 때는 정보량이 많아 overfitting이 발생할 수 있기 때문에 regularization의 강도를 키워서 학습해야 한다. 이를 <strong>progressive learning with adaptive regularization</strong>이라 한다.</li></ul><figure id="fa3d7dce-058d-41f1-b233-fbb81218256e" class="image" style="text-align:center"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2016.png"><img style="width:480px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2016.png"/></a></figure><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="390d66ba-e393-4c65-ae9d-b42311f3e519"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">실험 힘듦, 오래걸림, 실제 논문에서 발표된 학습시간은 350 에포크 가량으로 이번 과제에 적용하기에는 학습 시간 및 총 5회 학습 진행해야된다는 점 감안하여 부적절하다고 판단함. 테스트를 진행했을 때는 학습시간을 적게 하여 진행하려 했으나 정확도가 안 나온다.</div></figure><p id="64cdeb6a-4922-418c-ac7b-2db63a1b7ad4" class="">
</p><hr id="89657956-98ac-41b2-98cd-1fbf5db69f47"/><h2 id="374aefb0-d3fe-4265-a889-9c5ae92d12ae" class="">3. 성능</h2><figure id="1c97eb46-f294-414e-b28f-1254648b48cc" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2017.png"><img style="width:720px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2017.png"/></a></figure><figure id="65f65dbf-b694-4f9e-9e19-6835ad6793bb" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2018.png"><img style="width:720px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2018.png"/></a></figure><figure id="3c1617df-e43e-4ca7-8131-cb6171ae7e8f" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2019.png"><img style="width:720px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2019.png"/></a></figure><figure id="9c7454f8-221d-4bcf-b3f6-30eb103a221a" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2020.png"><img style="width:720px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2020.png"/></a></figure><figure id="5a6ffae0-c9f1-47e0-9d12-a804ee891f2c" class="image"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2021.png"><img style="width:720px" src="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/Untitled%2021.png"/></a></figure><p id="006c99e6-a23c-4c59-bbc1-54e094f6c403" class="">5회 평균 정확도 : 0.8667</p><p id="94099b08-b79b-4fbc-88d7-08b911a83b5a" class="">
</p><figure id="94bb328a-0cb9-4367-9fd0-258e32046cd1"><div class="source"><a href="CIFAR-100%20Classification%20d3382ef4b9e04ad683dab70348718c90/7%25EC%25A1%25B0_2%25EC%25A3%25BC%25EC%25B0%25A8_%25EC%25BD%2594%25EB%2593%259C.py">7조_2주차_코드.py</a></div></figure></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>




</style></head><body><article id="f61d0b25-9f4c-474a-888a-e5203f199d58" class="page sans"><header><div class="page-header-icon undefined"><span class="icon">3️⃣</span></div><h1 class="page-title">Office-Home dataset Classification</h1><p class="page-description"></p></header><div class="page-body"><ul id="75695eb4-6588-401a-afeb-a33110cdb8b8" class="toggle"><li><details open=""><summary>240520 최종발표</summary><h3 id="dbc24f49-7a95-4a34-9d1d-21a6c8387a05" class="block-color-blue_background"><strong>모델 Hyperparameter</strong></h3><ol type="1" id="35441a40-f033-444a-ac87-4d3de104b1e0" class="numbered-list" start="1"><li>backbone : <code>Resnet 50</code></li></ol><ol type="1" id="a3e0db8c-f93c-4125-b67d-85a6272eb56a" class="numbered-list" start="2"><li>epoch = <code>20</code></li></ol><ol type="1" id="6ee6734e-1a10-4f53-a7cc-e5bcdfb35ac0" class="numbered-list" start="3"><li>batch_size = <code>64</code></li></ol><ol type="1" id="af5d8a7f-fe08-4675-b331-0b29872a6cbc" class="numbered-list" start="4"><li>learning_rate = <code>0.0004</code></li></ol><ol type="1" id="fa6d7ff4-749f-464b-aacd-13db0ee7fdce" class="numbered-list" start="5"><li>scheduler : <code>StepLR</code> , step_size = <code>7</code> (loss 추이를 보고 결정), gamma = <code>0.3</code></li></ol><ol type="1" id="30431e50-01a1-44f2-9639-82e596e6094c" class="numbered-list" start="6"><li>criterion : <code>CrossEntropyLoss</code></li></ol><ol type="1" id="90a85f50-3da5-4b68-8d21-b1a9362c3249" class="numbered-list" start="7"><li>optimizer : <code>AdamW</code></li></ol><ol type="1" id="91884586-959f-454d-886a-f5ea6e7ffa99" class="numbered-list" start="8"><li>dropout 비율 : <code>0.5</code></li></ol><ol type="1" id="36b687ef-3afa-4ba2-abbf-1b1081767b12" class="numbered-list" start="9"><li>MixStyle 적용 확률 : <code>0.5</code> </li></ol><ol type="1" id="724c6d3f-9c71-40ad-8d5b-b55695934802" class="numbered-list" start="10"><li>MixStyle 섞는 비율(lambda) Sampling 함수 : <code>beta(0.1, 0.1)</code> <figure id="9a4bfd05-617b-49fd-9ade-699741cc9dd3" class="image"><a href="Office-Home%20dataset%20Classification%20f61d0b259f4c474a888ae5203f199d58/Untitled.png"><img style="width:432px" src="Office-Home%20dataset%20Classification%20f61d0b259f4c474a888ae5203f199d58/Untitled.png"/></a></figure></li></ol><p id="dba0ea9b-fee5-487b-a7e7-e20455796fbc" class="">
</p><h3 id="405d443e-886d-4f6f-9c99-0dc1042cb896" class="block-color-blue_background">성능 향상을 위해 활용한 기법 or 설정</h3><h3 id="1c3568d5-e8fc-4759-9b3e-709863d0a2ac" class="block-color-blue"><strong>1. MixStyle + Layer freeze 적용</strong></h3><p id="2b672d3d-aa00-482a-8178-4d015898403d" class="">1️⃣ <strong>MixStyle</strong></p><ul id="2f65b8e8-0cb2-4000-8440-51bbc1d781fd" class="bulleted-list"><li style="list-style-type:disc"><span style="border-bottom:0.05em solid">서로 다른 도메인의 스타일 정보를 혼합하는 방법</span></li></ul><ul id="46176870-4a13-43ab-a115-8c4bfd049770" class="bulleted-list"><li style="list-style-type:disc">batch normalization처럼 층마다 적용하여 normalization을 하는 것</li></ul><figure id="cdae6c43-3352-4299-972e-bf387db9d4ab" class="image" style="text-align:center"><a href="Office-Home%20dataset%20Classification%20f61d0b259f4c474a888ae5203f199d58/Untitled%201.png"><img style="width:384px" src="Office-Home%20dataset%20Classification%20f61d0b259f4c474a888ae5203f199d58/Untitled%201.png"/></a></figure><figure id="de1b2e24-9176-40cb-a520-5f70e3f8aeaa" class="image" style="text-align:center"><a href="Office-Home%20dataset%20Classification%20f61d0b259f4c474a888ae5203f199d58/Untitled%202.png"><img style="width:480px" src="Office-Home%20dataset%20Classification%20f61d0b259f4c474a888ae5203f199d58/Untitled%202.png"/></a></figure><ul id="1b9b7ff1-80ee-48f5-93bd-67a6cc29a566" class="bulleted-list"><li style="list-style-type:disc">lambda : 도메인 섞는 비율 → 이를 정하는 Sampling 함수로 <code>beta(0.1, 0.1)</code>  사용.<br/>이 때, Sampling 함수를 그대로 적용한다면 domain 정답 레이블과 다르게 domain 정보가 뒤섞이므로 원본 도메인의 비율이 더 높게 섞이게끔 진행함.<br/></li></ul><ul id="143f6e41-c887-4616-add2-a2cc9e20db67" class="bulleted-list"><li style="list-style-type:disc">Stage마다 MixStyle이 적용되어 domain label의 정보가 섞이므로, domain loss 계산 시 이를 적용하여 섞는 비율에 맞춰 loss 계산을 수행한 후 역전파 적용.</li></ul><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="ce6e5577-fa46-4b98-9498-e647fd0e5842"><div style="font-size:1.5em"><span class="icon">⚠️</span></div><div style="width:100%">MixStyle 기법은 모델 내에서 domain을 섞는 것과 같은 역할을 하므로, <br/>역전파 적용시 domain을 섞지 않는 부분에도 영향을 주면서 문제가 발생한다.<br/><br/>→ 이를 막기 위해 초기 layer를 freeze 함<br/></div></figure><p id="9ad9bbf5-3ed1-480d-bbfd-6051b019b25a" class="">
</p><p id="4493593b-e087-47aa-ac99-e09c7ecfda1d" class="">2️⃣ <strong>Layer freeze</strong></p><ol type="1" id="aa20755b-ca56-4ad5-a29c-89e39894296a" class="numbered-list" start="1"><li>stage1에 대해서만 freeze</li></ol><ol type="1" id="18bdf638-2936-45f4-a2b5-a2849715656b" class="numbered-list" start="2"><li>stage1 &amp; stage2에 대해 freeze</li></ol><figure id="d7a86c1b-2cd4-438a-b09e-a57ee9c4dc9d" class="image" style="text-align:center"><a href="Office-Home%20dataset%20Classification%20f61d0b259f4c474a888ae5203f199d58/Untitled%203.png"><img style="width:288px" src="Office-Home%20dataset%20Classification%20f61d0b259f4c474a888ae5203f199d58/Untitled%203.png"/></a><figcaption>Resnet 50 + Mixstyle 적용 + stage freeze</figcaption></figure><p id="c8d8cc3c-0cb4-451b-8421-0d7d80aa144d" class="">
</p><p id="3fcd79ae-565b-446d-b18c-d7f565d2d465" class="">3️⃣ R<strong>esnet 50 모델에서 MixStyle과 Layer freeze 적용 유무 성능 비교</strong></p><figure id="41140ee3-56e1-45e9-b95f-9bf4d37e726d" class="image"><a href="Office-Home%20dataset%20Classification%20f61d0b259f4c474a888ae5203f199d58/Untitled%204.png"><img style="width:672px" src="Office-Home%20dataset%20Classification%20f61d0b259f4c474a888ae5203f199d58/Untitled%204.png"/></a></figure><ol type="1" id="42c5606d-53c3-4cfe-bcb0-59ccecb1426b" class="numbered-list" start="1"><li>기법 미적용</li></ol><ol type="1" id="4fc5e67e-b358-46d0-8da8-86efa1d7fb9e" class="numbered-list" start="2"><li>MixStyle + domain label 적용</li></ol><ol type="1" id="86ba6f71-742c-4fe1-aa03-b06ce6fca421" class="numbered-list" start="3"><li>MixStyle + domain label 적용 + MixStyle 적용 전 stage1을 freeze</li></ol><ol type="1" id="b6873113-c0f6-4d0a-a888-d19032c8a3f3" class="numbered-list" start="4"><li>MixStyle + domain label 적용 + MixStyle 적용 전 stage1&amp;2을 freeze</li></ol><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="dec11f35-0a00-456b-8c13-cc27806e0d2a"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"> 3번 모델 MixStyle + domain label 적용 + MixStyle 적용 전 stage1만 freeze한 것이 성능이 좋았음<br/><br/>→ stage 1만 freeze 하는 것으로 결정<br/></div></figure><p id="5508aa1a-be84-4352-9d93-a5b5decb8758" class="">
</p><h3 id="78824540-457c-4f98-8aeb-3ef38391d40e" class="block-color-blue">2. dropout + scheduler</h3><p id="8f8703f3-6865-4260-a9c1-b28ac45730ef" class=""><strong>Resnet 50 모델에서 dropout과 scheduler 적용한 후, MixStyle과 Layer freeze 적용 유무 성능 비교</strong></p><figure id="262d3a31-4e20-4e17-b94c-02aa02f37d1f" class="image"><a href="Office-Home%20dataset%20Classification%20f61d0b259f4c474a888ae5203f199d58/Untitled%205.png"><img style="width:672px" src="Office-Home%20dataset%20Classification%20f61d0b259f4c474a888ae5203f199d58/Untitled%205.png"/></a></figure><ul id="db7a119f-6ba3-42ac-b3c8-cf33726d8ebd" class="bulleted-list"><li style="list-style-type:disc">scheduler : <code>StepLR</code> , step_size = <code>7</code>, gamma = <code>0.3</code></li></ul><ul id="c53d8a95-bce5-4c23-a646-c07dbd1a74aa" class="bulleted-list"><li style="list-style-type:disc">dropout 비율 : <code>0.5</code></li></ul><p id="9a340fec-c7ed-49aa-bf73-1ebd6176de74" class="">
</p><h3 id="7ea6bdf7-e004-4282-9bcf-cc5ddfef38a2" class="block-color-blue">3. Noisy Student Learning</h3><p id="f8789b64-a1f5-462b-95b9-232800660fb0" class="">: <span style="border-bottom:0.05em solid">레이블이 없는 데이터를 활용하여 noise를 추가</span>하며 모델의 일반화를 돕는 기법</p><figure id="ed746c89-a98a-4e77-bdbf-a831aa7617c7" class="image" style="text-align:center"><a href="Office-Home%20dataset%20Classification%20f61d0b259f4c474a888ae5203f199d58/Untitled%206.png"><img style="width:576px" src="Office-Home%20dataset%20Classification%20f61d0b259f4c474a888ae5203f199d58/Untitled%206.png"/></a></figure><p id="ad917c98-0401-42d3-9e26-88e1f1e8ae5f" class=""><strong>적용하지 못한 이유</strong></p><ul id="b14a63ce-0d27-4b10-92ac-9cfa91171cc8" class="bulleted-list"><li style="list-style-type:disc">student의 학습을 위해서는 student model의 크기가 teacher model 이상이어야 함, </li></ul><ul id="486a2833-8b46-42b6-ae5b-77198a967c6d" class="bulleted-list"><li style="list-style-type:disc">student train시에 batch size가 teacher의 훈련시 batch size보다 14~28배 이상 커야 함<p id="2bb01ec5-ead4-4db2-bf2a-045c50d8efc6" class="">→ 적은 batch size로 진행시 student의 학습이 원활히 이루어 지지 않았음</p><p id="0608316e-8b4f-411c-b4af-1b41acbf4b14" class="">→ COLAB 할당량 문제로 적용 어려움</p></li></ul><p id="18db03fb-86e6-45db-b1a3-9ce906c61582" class="">
</p><h3 id="979449c6-dc4b-4324-b13f-a591c75cf86f" class="block-color-blue_background">모델 최종 성능</h3><figure id="08a47489-ab14-4437-ba4b-0a51e161cba5" class="image"><a href="Office-Home%20dataset%20Classification%20f61d0b259f4c474a888ae5203f199d58/Untitled%207.png"><img style="width:674px" src="Office-Home%20dataset%20Classification%20f61d0b259f4c474a888ae5203f199d58/Untitled%207.png"/></a></figure><p id="7c8eefa0-0a5f-4f01-8d43-eccf4349d81a" class="">최종 모델 : domain과 class의 평균 mAP가 가장 높았던 MixStyle + Resnet50 + dropout + scheduler 모델</p><p id="d5930462-daab-45ee-b08e-86391fe123b4" class=""><mark class="highlight-red"><strong>최종 mAP(domain) : 0.924 </strong></mark></p><p id="5e32a018-b0f2-4e48-a710-f2b8764b1a98" class=""><mark class="highlight-red"><strong>최종 mAP(class) : 0.921</strong></mark></p><p id="5d5790db-d58c-4a42-9056-cb135ccc3cab" class="">
</p><h3 id="233b142e-bdf1-4e56-a7f1-2698bb70f316" class="block-color-blue_background">이번 task를 통해 얻은 점</h3><p id="6cce090a-1e94-4d98-97a1-f9b6d698c36c" class=""><strong>실험의 의의</strong> : Domain Generalization의 기법 중 하나인 MixStyle을 활용하여 현 PBL 과제에 맞게 적용 및 변형하여 기법을 적절히 활용함. (적용 전 대비 class 정확도가 높아짐 + domain 정확도가 오차 범위로 근접함.)</p><p id="1dcc0139-c937-431d-a430-a854a888f610" class=""><strong>실험의 한계</strong> : 앞서 제기했던 문제점을 완전히 해결했다면 domain, class 정확도 향상을 모두 이뤄낼 것을 기대하며 문제를 접근하였지만 이를 완전히 해결하지 못하여 큰 성능 향상을 이뤄내지는 못함. </p><p id="3d3a6528-ab87-4d0d-a0d4-4e57a2432c47" class=""><strong>추후 개선해야 할 사항</strong> : 앞서 제기한 문제를 개선하여 domain label을 학습 시 정확히 반영한다면 각각의 정확도를 높일 수 있을 것으로 기대함.</p><p id="cd51c2b2-d505-40d6-b536-69b6421deca8" class="">
</p><h3 id="0812441f-f66d-42ce-bdad-41b26dad9780" class="block-color-blue_background">코드 첨부</h3><figure id="231e4190-2a25-49cb-9e8d-9c730d143e01"><div class="source"><a href="Office-Home%20dataset%20Classification%20f61d0b259f4c474a888ae5203f199d58/7%25EC%25A1%25B0-3%25EC%25A3%25BC%25EC%25B0%25A8-%25EC%25BD%2594%25EB%2593%259C.py">7조-3주차-코드.py</a></div></figure><p id="18935f32-78d3-4513-932d-1651f6724949" class="">
</p></details></li></ul></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>




</style></head><body><article id="45287e85-33e4-4554-9c3f-329d6b9eb2d1" class="page sans"><header><div class="page-header-icon undefined"><span class="icon">4️⃣</span></div><h1 class="page-title">Semantic Segmentation</h1><p class="page-description"></p></header><div class="page-body"><p id="20a28c76-453b-4d91-8d75-23c0b46d27ee" class=""><strong>Semantic Segmentation</strong></p><ul id="ac4f9a42-c2f4-48cc-bda6-4f7dca943ba3" class="toggle"><li><details open=""><summary>240612 최종발표</summary><h3 id="122c66f3-e265-41c9-a1e2-ddb412eccc91" class="block-color-blue_background">이번 Task</h3><figure class="block-color-default callout" style="white-space:pre-wrap;display:flex" id="5016f208-0a5d-4946-a745-3b042543e202"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">본 과제에서는 <span style="border-bottom:0.05em solid">PASCAL VOC 2012 데이터셋</span>을 활용하여 자율주행 자동차의 안전성을 높일 수 있는 딥러닝 기반 <span style="border-bottom:0.05em solid">이미지 분할 모델(Semantic Segmentation)</span>을 설계하고 학습합니다.</div></figure><p id="5785eb78-b8b3-4e37-9dc5-8c9e3cdabbbc" class="">
</p><h3 id="8c1d7587-9c1c-44b8-83b2-22fdd82743f4" class="block-color-blue_background">최종 모델</h3><ol type="1" id="a547f981-d87d-40cd-9ce9-e866f3c78c9d" class="numbered-list" start="1"><li>pretrained :  <code>Deeplab v3</code> </li></ol><ol type="1" id="6d067743-e2ec-4f84-9014-7e3dd6d2cde0" class="numbered-list" start="2"><li>backbone : <code>Resnet 101</code> </li></ol><p id="96823273-0306-430c-8f35-ea9ed0fe7fc7" class="">
</p><h3 id="d0d0d347-41e1-4e1e-a57f-2abb06be521c" class="block-color-blue_background"><strong>모델 Hyperparameter</strong></h3><ol type="1" id="8befe114-86d8-4e9b-bd7a-14f2aeaf5907" class="numbered-list" start="1"><li>epoch = <code>30</code></li></ol><ol type="1" id="154a98dd-9b62-49ca-bd19-644a8ece3e10" class="numbered-list" start="2"><li>batch_size = <code>16</code></li></ol><ol type="1" id="6bc8ad63-b3d4-4222-9b6a-49a6d3969dc3" class="numbered-list" start="3"><li>learning_rate = <code>0.000009</code></li></ol><ol type="1" id="a1b16d0f-9344-423d-9612-e8241451ea41" class="numbered-list" start="4"><li>scheduler : <code>StepLR</code> , step_size = <code>5</code> (loss 추이를 보고 결정), gamma = <code>0.8</code></li></ol><ol type="1" id="3b48eedd-5070-4c9c-952f-43a5730f51a8" class="numbered-list" start="5"><li>criterion : <code>CrossEntropyLoss</code></li></ol><ol type="1" id="f0b0a306-e789-473b-a47b-7fc839d15216" class="numbered-list" start="6"><li>optimizer : <code>AdamW</code> , weight_decay = <code>0.0001</code></li></ol><ol type="1" id="3222c908-2013-438c-ab97-8e808eb23f66" class="numbered-list" start="7"><li>transform : <code>normalize</code>, <code>horizontalflip</code></li></ol><p id="6e57da6e-d571-4aa0-b59d-267b7662d50a" class="">
</p><h3 id="a08644cb-5965-43cb-b708-1a6d9db91891" class="block-color-blue_background">성능 향상을 위해 활용한 기법 or 설정</h3><ol type="1" id="ed68455a-514e-4026-bf24-89f10b8b8dda" class="numbered-list" start="1"><li><mark class="highlight-blue"><strong> </strong></mark><mark class="highlight-default"><strong>Panoptic Segmentation 모델 이용하여 Multi Task Learning 시도</strong></mark><figure id="c339bf44-4e96-4e00-b044-c27b308b6ccf" class="image"><a href="Semantic%20Segmentation%2045287e8533e445549c3f329d6b9eb2d1/Untitled.png"><img style="width:1647px" src="Semantic%20Segmentation%2045287e8533e445549c3f329d6b9eb2d1/Untitled.png"/></a></figure><p id="07294239-b7fd-4141-bffd-54fc387d942e" class="">위와 같이 과제 소개 발표 자료를 통해 Panoptic Segmentation이 Semantic Segmentation과 Instance Segmentation이 합쳐진 형태임을 알 수 있음<br/><br/><br/>아래와 같이 Semantic Segmentation, Instance Segmentation의 label을 Dataset에서 확인할 수 있음.<br/></p><div id="b09e3493-545f-46e5-8388-20089d88b7a4" class="column-list"><div id="6ee02ad2-322f-4b47-9c41-1b14162af989" style="width:50%" class="column"><figure id="3978024b-c1be-4169-b9eb-4cfabe7bf622" class="image" style="text-align:center"><a href="Semantic%20Segmentation%2045287e8533e445549c3f329d6b9eb2d1/Untitled%201.png"><img style="width:336px" src="Semantic%20Segmentation%2045287e8533e445549c3f329d6b9eb2d1/Untitled%201.png"/></a></figure><figure id="5c8219a3-b382-47e1-ba2e-ac2ee2d0ade7" class="image"><a href="Semantic%20Segmentation%2045287e8533e445549c3f329d6b9eb2d1/Untitled%202.png"><img style="width:192px" src="Semantic%20Segmentation%2045287e8533e445549c3f329d6b9eb2d1/Untitled%202.png"/></a></figure><p id="38a17dbc-ba38-4eed-9465-e902c76da1ca" class=""><em><mark class="highlight-purple"><strong>semantic segmentation</strong></mark></em></p></div><div id="57cbe4d3-1fa1-4488-8799-b9b21014b755" style="width:50%" class="column"><figure id="750a3a02-bc74-472e-9259-e491941629f0" class="image" style="text-align:center"><a href="Semantic%20Segmentation%2045287e8533e445549c3f329d6b9eb2d1/Untitled%203.png"><img style="width:288px" src="Semantic%20Segmentation%2045287e8533e445549c3f329d6b9eb2d1/Untitled%203.png"/></a></figure><figure id="dfc30cc2-05cd-4cfa-9696-a0cf90280e6f" class="image"><a href="Semantic%20Segmentation%2045287e8533e445549c3f329d6b9eb2d1/Untitled%204.png"><img style="width:192px" src="Semantic%20Segmentation%2045287e8533e445549c3f329d6b9eb2d1/Untitled%204.png"/></a></figure><p id="fec65ea4-534b-4384-b642-651b68084a73" class=""><em><mark class="highlight-purple"><strong>instance segmentation</strong></mark></em></p></div></div><p id="5154abef-fec7-445a-9c1f-8fbffadd0555" class="">위의 semantic segmentation의 데이터셋을 통해, 같은 instance에 대해서는 각각의 객체 인식에 대해 정확하게 labelling이 되지 않는 것을 확인할 수 있음</p><p id="6942d378-f2e5-4534-8ce0-f80268a84eca" class=""><br/><br/><mark class="highlight-purple"><strong>→ Semantic Segmentation 모델을 기준으로 Instance Segmentation Head를 합치면 어떨까란 고민을 하게 되어, Panoptic SegmentationMTL을 시도함.</strong></mark></p><p id="f2f43873-7ca7-44e1-a312-c2d0cb41f4fd" class="">
</p></li></ol><ol type="1" id="9f4bb0ec-fba6-4254-b32f-b0dce30ed1f4" class="numbered-list" start="2"><li><mark class="highlight-default"><strong>Panoptic Model 이용</strong></mark><p id="a238bf4b-78b3-49e9-a962-2d7daa9cede9" class="">실제 Panoptic Segmentation 모델을 확인하면,<br/>Instance Segmentation 모델에 Semantic Segmentation Head를 추가하거나 <br/>Semantic Segmentation 모델에 Instance Segmentation Head를 추가한 모델들이 있는 것을 확인할 수 있었음.<br/></p><p id="47e7ff5f-9b73-477d-b460-1fd25f163673" class="">
</p><p id="14f24359-f787-4dd3-969e-3a57541c81ac" class="">대표적으로 Panoptic FPN, Panoptic Deeplab이 있음.</p><figure id="294af1ea-c035-4174-9207-4b42f1d4aeec" class="image" style="text-align:center"><a href="Semantic%20Segmentation%2045287e8533e445549c3f329d6b9eb2d1/Untitled%205.png"><img style="width:432px" src="Semantic%20Segmentation%2045287e8533e445549c3f329d6b9eb2d1/Untitled%205.png"/></a><figcaption>Panoptic FPN </figcaption></figure><figure id="5b4537e1-dbcc-45b3-95fe-12ff8ef9a76e" class="image"><a href="Semantic%20Segmentation%2045287e8533e445549c3f329d6b9eb2d1/Untitled%206.png"><img style="width:672px" src="Semantic%20Segmentation%2045287e8533e445549c3f329d6b9eb2d1/Untitled%206.png"/></a><figcaption>Panoptic Deeplab </figcaption></figure><p id="9cad7ead-b1d6-40ac-a65d-0a3787292930" class="">
</p><p id="270b1918-ccf8-45e4-af68-eea6432eeceb" class="">원래 취지는 Semantic Segmentation 모델에 Instance Segmentation Head를 추가하는 것이었기에, <mark class="highlight-default"><strong>Panoptic Deeplab을 모델로 선정하게 됨.</strong></mark><br/><br/>이를 통해 Panoptic Deeplab 모델을 사용하였고, Dataset 내의 Label을 각 헤드에 맞게 변형하여 학습을 시도함.<br/></p><figure id="f4510d00-05ed-4311-a53b-84244c38a749" class="image"><a href="Semantic%20Segmentation%2045287e8533e445549c3f329d6b9eb2d1/Untitled%207.png"><img style="width:674px" src="Semantic%20Segmentation%2045287e8533e445549c3f329d6b9eb2d1/Untitled%207.png"/></a><figcaption>코드 debugging을 통해 MTL을 위한 label 생성 완료</figcaption></figure><figure id="77eded5f-b915-408d-83bb-7353195b634a" class="image" style="text-align:center"><a href="Semantic%20Segmentation%2045287e8533e445549c3f329d6b9eb2d1/Untitled%208.png"><img style="width:480px" src="Semantic%20Segmentation%2045287e8533e445549c3f329d6b9eb2d1/Untitled%208.png"/></a></figure><table id="2ecfe788-b945-45f1-8488-f80a525143fb" class="simple-table"><tbody><tr id="d5fa7c2e-8b42-4755-b264-0bd4f811de7f"><td id="{Ox&lt;" class="" style="width:110.33333333333333px">heatmap, offset</td><td id="jfD[" class="" style="width:110.33333333333333px">(200, 0.01)</td><td id="oa_`" class="" style="width:110.33333333333333px">(10, 0.01)</td><td id="rFW=" class="" style="width:110.33333333333333px">(400, 0.01)</td><td id="xD^T" class="" style="width:110.33333333333333px">(200, 0.001)</td><td id="OEVh" class="" style="width:110.33333333333333px">(200, 0.1)</td></tr><tr id="3e096b41-5cac-4185-9574-aa84adac61f3"><td id="{Ox&lt;" class="" style="width:110.33333333333333px">가장 높은 mIoU</td><td id="jfD[" class="" style="width:110.33333333333333px">0.58<br/>(9 epoch)<br/></td><td id="oa_`" class="" style="width:110.33333333333333px">0.57<br/>(10 epoch)<br/></td><td id="rFW=" class="" style="width:110.33333333333333px">0.57<br/>(12 epoch)<br/></td><td id="xD^T" class="" style="width:110.33333333333333px">0.55<br/>(10 epoch)<br/></td><td id="OEVh" class="" style="width:110.33333333333333px">0.56<br/>(7 epoch)<br/></td></tr></tbody></table><p id="fb5f3624-7ca1-4cfc-bf19-58285570ccd1" class="">loss 계수를 다양하게 하여 시도해보았으나 단일 Task에 비해 성능이 낮아지는 것을 확인할 수 있었음.</p><p id="194ffd62-6908-45c3-879f-71abfbcb5e1a" class="">
</p></li></ol><ol type="1" id="7826abc6-7d27-4c2e-a9a1-5021fcfe075e" class="numbered-list" start="3"><li><strong>Backbone 변경</strong><br/>성능이 예상 외로 잘 안 나오는 것을 인지하여 다른 Backbone을 이용해보면 성능 향상을 기대할 수 있을까 하여 다른 Backbone(Xception)을 이용함. <br/><p id="aa2f3d43-2595-4bad-bd30-8090a77212c6" class="">그러나, 같은 epoch에서 성능이 안 나오는 것을 확인함. </p><p id="a033b9a4-48b0-468e-a4c7-0210e49ff0b5" class="">이후, 다른 pretrained 모델 이용 시도( Detectron2)를 진행하였지만 시간 관계상 힘들 수 있다고 판단함.</p><p id="cb8766b2-cba3-4bcd-aeaf-5fcd43a81fb4" class=""><br/><br/></p></li></ol><ol type="1" id="1418406c-633f-4e8f-9580-21e39883c7c1" class="numbered-list" start="4"><li><strong>Pretrained DeepLab으로 모델 변경</strong><p id="44e6f561-9e0b-43ee-b64f-9f420fc34347" class="">사전 학습된 Semantic Segmentation 모델 이용하여 학습 진행하는 것으로 노선 변경.</p><p id="92f1fdad-22dd-48ee-8d21-141c0bc9dcc8" class="">이 중 torch에서 제공하는 DeepLab V3이 사용하기 가장 용이하여 사용하게 됨.</p><p id="9c8edad8-8157-4829-b434-c1c4093f491c" class="">이후 Learning Rate 조절 및 이전에 사용했던 Method를 추가하여 성능을 올리기 위한 시도 진행함.</p><p id="1279cdd7-eedf-40c3-8fc8-1c1aa9be7c32" class="">
</p><p id="8e1e4a64-3f16-4ef2-9152-6f356f45d91d" class=""><em><mark class="highlight-purple"><strong>Learning Rate 조절</strong></mark></em></p><table id="897b830a-24bd-4ded-b3ee-938f2adf54d0" class="simple-table"><tbody><tr id="90405ab7-5e6c-414f-ae32-e38543f4e77a"><td id="{Ox&lt;" class="" style="width:158.5px">learning_rate</td><td id="jfD[" class="" style="width:158.5px">0.00004</td><td id="oa_`" class="" style="width:158.5px">0.00001</td><td id="rFW=" class="" style="width:158.5px">0.000009</td></tr><tr id="db28db09-4ed1-4713-a061-27738169e7f9"><td id="{Ox&lt;" class="" style="width:158.5px">가장 높은 mIoU</td><td id="jfD[" class="" style="width:158.5px">75.32</td><td id="oa_`" class="" style="width:158.5px">75.97</td><td id="rFW=" class="" style="width:158.5px">76.12</td></tr></tbody></table><p id="c0db0a83-9cfc-4ee5-bc50-3fb19b161ce5" class="">
</p><p id="068a0f89-a0d5-467e-b03c-eecb1a6b05c3" class=""><em><mark class="highlight-purple"><strong>Method 추가(동일 Learning Rate)</strong></mark></em></p><table id="ebe2fe6a-f0c7-4dcf-85fe-7a36322e25f3" class="simple-table"><tbody><tr id="ca3ca1f7-d87e-47a3-9065-6e7897864873"><td id="{Ox&lt;" class="" style="width:158.5px">learning_rate</td><td id="jfD[" class="" style="width:158.5px">일반</td><td id="oa_`" class="" style="width:158.5px">+ scheduler<br/>+ weight_decay<br/></td><td id="rFW=" class="" style="width:158.5px">+ scheduler<br/>+ weight_decay<br/>+ dropout<br/></td></tr><tr id="3405b312-6817-4490-ae05-9d57062e5330"><td id="{Ox&lt;" class="" style="width:158.5px">최종 mIoU</td><td id="jfD[" class="" style="width:158.5px">76.12</td><td id="oa_`" class="" style="width:158.5px">75.97</td><td id="rFW=" class="" style="width:158.5px">76.12</td></tr></tbody></table><p id="63c207ec-8233-40c6-9175-e987a6c62f0a" class="">
</p></li></ol><h3 id="4a934961-69f0-4779-b021-4d626039fb94" class="block-color-blue_background">모델 최종 성능</h3><p id="4795be3d-c46e-4e53-9465-d5cb3927691a" class="">
</p><p id="d1bf72f3-9527-4e8e-9d62-622e90d7c847" class="">
</p><h3 id="8a6adf43-82a0-4be8-bdc8-c6b7e76b5641" class="block-color-blue_background">본 과제를 통해 얻은 점</h3><p id="3adbbc1a-14a3-448a-bc8d-8d58c3b2059f" class="">1️⃣ <mark class="highlight-blue"><strong>실험의 의의</strong></mark></p><ul id="95ea82e2-5bc1-4543-97aa-aa82943a81e6" class="bulleted-list"><li style="list-style-type:disc">Dataset내의 다른 Label을 활용하여 Multi Task Learning을 시도해봄.</li></ul><ul id="ff0301ae-cca2-413d-8c4c-e6e6f27b31d4" class="bulleted-list"><li style="list-style-type:disc">Semantic Segmentation 뿐 아니라 Instance Segmentation 및 Panoptic Segmentation 모델에 대해 공부해봄.</li></ul><ul id="a0417359-8279-4def-ba5b-2f2d5b26e161" class="bulleted-list"><li style="list-style-type:disc">Semantic Segmentation과 Instance Segmentation의 성격이 다른 것을 확인할 수 있었음.</li></ul><ul id="70a0e5e2-e8e9-4171-b9c0-d6a7b70d6aa0" class="bulleted-list"><li style="list-style-type:disc">대표적인 Semi Supervised Learning 기법을 여러 개 공부해보고 해당 Task에 사용할 수 있는 Consistency Regularization을 시도해봄.</li></ul><ul id="7c127ac8-1e85-405f-911c-c44302e14b70" class="bulleted-list"><li style="list-style-type:disc">취지는 좋았으나, Multi Task Learning보다는 Transfer Learning이 매우 효과적임을 알 수 있었음.<br/><br/></li></ul><p id="b2e417da-5c42-4617-8af7-87d5df72ca93" class="">2️⃣ <mark class="highlight-blue"><strong>Panoptic Segmentation, MTL 결과 분석 </strong></mark></p><ul id="fe81f9f2-c768-4be3-9d24-44848a5fe01c" class="bulleted-list"><li style="list-style-type:disc">실제 Instance Segmentation을 MTL로 진행할 시에, 일반 Resnet 대비 pixel을 wide하게 바라보게끔 Backbone을 구성하여 Feature를 추출 할 수 있다고 생각.</li></ul><ul id="34eb6c22-22c5-4e26-87dc-371590e13f43" class="bulleted-list"><li style="list-style-type:disc">DeepLabV3를 기준으로 하는 weight가 해당 모델에 최적화된 파라미터를 가지지 않아 성능이 생각보다 나오지 않을 수 있다고 생각.</li></ul><ul id="7e39d6e1-6874-46f6-92ea-bcbcd3981969" class="bulleted-list"><li style="list-style-type:disc">Semantic Segmentation에 초점을 맞춘 Loss Scaling이 까다로워서 제대로 적용하지 못했을 가능성 존재.</li></ul><p id="82bf8b35-0295-49a8-99e9-37d07c4883ed" class="">
</p><p id="d7fec193-21bb-4913-86eb-d712b14a344f" class="">3️⃣ <mark class="highlight-blue"><strong>아쉬운 점</strong></mark> </p><ul id="5e5883b6-520b-4f70-a41c-b0bdc9db6dfd" class="bulleted-list"><li style="list-style-type:disc">동일한 환경(from scratch 등)에서 DeepLabV3와 Panoptic DeepLab을 test한 뒤 결과 분석을 하지 못한 것이 아쉬움.</li></ul><ul id="4e94dac2-785f-449f-b967-1271072fbdb3" class="bulleted-list"><li style="list-style-type:disc">최신 모델 등에서 이를 극복한 사례를 찾아 추가 적용 및 학습을 진행했다면 개선 여지가 있었을 것으로 생각했으나 하지 못해 아쉬움.</li></ul><ul id="ebb6b0c2-043c-4e2d-8539-27c7fcf15ced" class="bulleted-list"><li style="list-style-type:disc">Consistency Regularization을 진행하였지만 뚜렷한 성과가 없어 원인 분석을 하지 못함.</li></ul><p id="fa80275e-f91a-4edb-aded-3054efa56c0c" class="">
</p><h3 id="aff1ce4b-c108-4932-b1bb-cef132f658fa" class="block-color-blue_background">코드 첨부</h3><figure id="c912a632-14aa-4589-9cbf-de145b151675"><div class="source"><a href="Semantic%20Segmentation%2045287e8533e445549c3f329d6b9eb2d1/Semantic_Segmentation_final.py">Semantic_Segmentation_final.py</a></div></figure><figure id="4865e70a-4da2-4abd-bf2c-c1a19ae7ecec"><div class="source"><a href="Semantic%20Segmentation%2045287e8533e445549c3f329d6b9eb2d1/appendix_file.py">appendix_file.py</a></div></figure><p id="e073dd4d-8549-458c-8e92-194bd21ad235" class="">
</p></details></li></ul></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>
